{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "from sklearn.metrics import *\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files\n",
    "import pickle\n",
    "path = 'C:/Users/User/Desktop/Assignatures/Synthesis project/GraphAnomaly/dades_arnau/'\n",
    "\n",
    "with open(path + 'sequential_train.pkl', 'rb') as f:\n",
    "    df_train_init = pickle.load(f)\n",
    "with open(path + 'sequential_test.pkl', 'rb') as f:\n",
    "    df_test_init = pickle.load(f)\n",
    "with open(path + 'edges.pkl', 'rb') as f:\n",
    "    df_edges_init = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comptador de valors per classe: \n",
      " class\n",
      "2    110537\n",
      "1     27591\n",
      "0      3644\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Comptador de valors per classe: \n",
      " class\n",
      "2    46668\n",
      "1    14428\n",
      "0      901\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prep_df(df_feats: pd.DataFrame, edges: pd.DataFrame):\n",
    "    #ens quedem només amb els edges que apareixen en el nodes d'entrenament\n",
    "    df_edges = edges.loc[((edges['node1'].isin(df_feats['node'])) & (df_edges_init['node2'].isin(df_feats['node'])))]\n",
    "    df_edges = df_edges.reset_index(drop=True)\n",
    "    print(f\"Comptador de valors per classe: \\n {df_feats['class'].value_counts()}\\n\")\n",
    "    return  df_feats, df_edges\n",
    "\n",
    "df_train, df_edges_train = prep_df(df_train_init, df_edges_init)\n",
    "df_test, df_edges_test = prep_df(df_test_init, df_edges_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_idx(feats: pd.DataFrame, edges: pd.DataFrame, save = True, loading_dir = \"a\"):\n",
    "    mapping_txid = dict(zip(feats['node'], list(feats.index)))\n",
    "    dir = 'dades_guillem/' + str(loading_dir) + '.pkl'\n",
    "    if save:\n",
    "        df_edges_mapped = edges.replace({'txId1': mapping_txid, 'txId2': mapping_txid})\n",
    "        \n",
    "        df_edges_mapped.to_pickle(loading_dir)\n",
    "    else:\n",
    "        df_edges_mapped = pd.read_pickle(loading_dir)\n",
    "    return df_edges_mapped\n",
    "\n",
    "df_edges_mapped_train = map_idx(feats = df_train, edges = df_edges_train, save = True, loading_dir='train')\n",
    "df_edges_mapped_test = map_idx(feats = df_test, edges = df_edges_test, save = True, loading_dir='test') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(feats: pd.DataFrame, edges:pd.DataFrame):\n",
    "    x = torch.tensor(feats.drop(columns=['class', 'time step', 'node']).values, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges.values, dtype=torch.long).T\n",
    "    y = torch.tensor(list(feats['class'].values))\n",
    "    time = torch.tensor(feats['time step'].values)\n",
    "    data = Data(x=x, edge_index=edge_index, y=y, time=time)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = get_data(df_train, df_edges_mapped_train)\n",
    "test_data = get_data(df_test, df_edges_mapped_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining, training and saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, n_layers=2):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        # Define the first layer\n",
    "        self.layers.append(SAGEConv(in_channels, hidden_channels if n_layers > 1 else out_channels, normalize=False))\n",
    "        if n_layers > 1:\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # Define the middle layers\n",
    "        for _ in range(n_layers - 2):\n",
    "            self.layers.append(SAGEConv(hidden_channels, hidden_channels, normalize=False))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # Define the last layer if more than one layer\n",
    "        if n_layers > 1:\n",
    "            self.layers.append(SAGEConv(hidden_channels, out_channels, normalize=False))\n",
    "\n",
    "        # Reset parameters\n",
    "        for layer in self.layers:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = layer(x, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=0.5, training=self.training)  # Dropout for regularization\n",
    "\n",
    "        x = self.layers[-1](x, edge_index)\n",
    "        return F.log_softmax(x, dim=-1), torch.var(x)\n",
    "\n",
    "    def inference(self, total_loader, device):\n",
    "        xs, var_ = [], []\n",
    "        for batch in total_loader:\n",
    "            out, var = self.forward(batch.x.to(device), batch.edge_index.to(device))\n",
    "            out = out[:batch.batch_size]\n",
    "            xs.append(out.cpu())\n",
    "            var_.append(var.item())\n",
    "        \n",
    "        out_all = torch.cat(xs, dim=0)\n",
    "        return out_all, var_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ken63thw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249d3381177f4fbc8c51bdc0947c52d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.019 MB uploaded\\r'), FloatProgress(value=0.0686186634797331, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▂▁▂▄▆▆▆▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>test_f1_score_class_0</td><td>▁▁▂▄▅▇▇▇██████████████▇▇████████████████</td></tr><tr><td>test_f1_score_class_1</td><td>▂▁▃▅▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>test_loss</td><td>▅█▆▅▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_precision_class_0</td><td>▁▁▁▂▃▄▄▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>test_precision_class_1</td><td>▄█▆▆▄▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_recall_class_0</td><td>▇█▇▆▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_recall_class_1</td><td>▂▁▂▄▆▆▆▆▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>train_accuracy</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>test_accuracy</td><td>0.82924</td></tr><tr><td>test_f1_score_class_0</td><td>0.56283</td></tr><tr><td>test_f1_score_class_1</td><td>0.8939</td></tr><tr><td>test_loss</td><td>0.44619</td></tr><tr><td>test_precision_class_0</td><td>0.70493</td></tr><tr><td>test_precision_class_1</td><td>0.8522</td></tr><tr><td>test_recall_class_0</td><td>0.4684</td></tr><tr><td>test_recall_class_1</td><td>0.93988</td></tr><tr><td>train_accuracy</td><td>0.90354</td></tr><tr><td>train_loss</td><td>0.267</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2_SAGE - n_layers: 1</strong> at: <a href='https://wandb.ai/uabai/GraphAnomaly/runs/ken63thw' target=\"_blank\">https://wandb.ai/uabai/GraphAnomaly/runs/ken63thw</a><br/> View project at: <a href='https://wandb.ai/uabai/GraphAnomaly' target=\"_blank\">https://wandb.ai/uabai/GraphAnomaly</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240531_024116-ken63thw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ken63thw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62abadc97d1d4d03921ee2c46be90dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\Assignatures\\Synthesis project\\GraphAnomaly\\GNN_models\\wandb\\run-20240531_024326-sv8ojt0m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uabai/GraphAnomaly/runs/sv8ojt0m' target=\"_blank\">2_SAGE - n_layers: 2</a></strong> to <a href='https://wandb.ai/uabai/GraphAnomaly' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uabai/GraphAnomaly' target=\"_blank\">https://wandb.ai/uabai/GraphAnomaly</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uabai/GraphAnomaly/runs/sv8ojt0m' target=\"_blank\">https://wandb.ai/uabai/GraphAnomaly/runs/sv8ojt0m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes classified as fraud: 85504, \n",
      "                nodes classified as licit: 47299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, \n",
      "            Training Loss: 0.8545, Training Accuracy: 0.3794\n",
      "            Test Loss: 46.0407, Test Accuracy: 0.7653\n",
      "            \n",
      "Precision: [0.         0.76533024], Recall: [0. 1.], F1 Score: [0.        0.8670675] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 20694, \n",
      "                nodes classified as licit: 112109\n",
      "Epoch: 5, \n",
      "            Training Loss: 2.9865, Training Accuracy: 0.8464\n",
      "            Test Loss: 16.0341, Test Accuracy: 0.7961\n",
      "            \n",
      "Precision: [0.60491416 0.82885314], Recall: [0.37749407 0.92440099], F1 Score: [0.46488109 0.87402351] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 19968, \n",
      "                nodes classified as licit: 112835\n",
      "Epoch: 10, \n",
      "            Training Loss: 3.1078, Training Accuracy: 0.8592\n",
      "            Test Loss: 34.7099, Test Accuracy: 0.5374\n",
      "            \n",
      "Precision: [0.29785018 0.84697681], Recall: [0.71553217 0.48278711], F1 Score: [0.42061397 0.61501065] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 17401, \n",
      "                nodes classified as licit: 115402\n",
      "Epoch: 15, \n",
      "            Training Loss: 1.9149, Training Accuracy: 0.8643\n",
      "            Test Loss: 12.0874, Test Accuracy: 0.7830\n",
      "            \n",
      "Precision: [0.56643278 0.81623687], Recall: [0.32110092 0.92463705], F1 Score: [0.40985956 0.86706205] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 26821, \n",
      "                nodes classified as licit: 105982\n",
      "Epoch: 20, \n",
      "            Training Loss: 1.0129, Training Accuracy: 0.8584\n",
      "            Test Loss: 7.6518, Test Accuracy: 0.7379\n",
      "            \n",
      "Precision: [0.45669441 0.86816019], Recall: [0.61602618 0.77528819], F1 Score: [0.52452748 0.81910007] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 24312, \n",
      "                nodes classified as licit: 108491\n",
      "Epoch: 25, \n",
      "            Training Loss: 0.7519, Training Accuracy: 0.8774\n",
      "            Test Loss: 6.1480, Test Accuracy: 0.7123\n",
      "            \n",
      "Precision: [0.40839139 0.83614131], Recall: [0.5039456  0.77615376], F1 Score: [0.45116453 0.80503157] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 19483, \n",
      "                nodes classified as licit: 113320\n",
      "Epoch: 30, \n",
      "            Training Loss: 0.5154, Training Accuracy: 0.8981\n",
      "            Test Loss: 6.5483, Test Accuracy: 0.6097\n",
      "            \n",
      "Precision: [0.32612497 0.83945267], Recall: [0.622121   0.60583468], F1 Score: [0.42792524 0.70376253] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 20736, \n",
      "                nodes classified as licit: 112067\n",
      "Epoch: 35, \n",
      "            Training Loss: 0.3976, Training Accuracy: 0.8954\n",
      "            Test Loss: 3.6888, Test Accuracy: 0.6990\n",
      "            \n",
      "Precision: [0.39657357 0.84178562], Recall: [0.54205428 0.7470984 ], F1 Score: [0.45803968 0.79162064] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 23371, \n",
      "                nodes classified as licit: 109432\n",
      "Epoch: 40, \n",
      "            Training Loss: 0.3487, Training Accuracy: 0.8931\n",
      "            Test Loss: 3.1430, Test Accuracy: 0.8040\n",
      "            \n",
      "Precision: [0.63586324 0.83185313], Recall: [0.38538526 0.93232876], F1 Score: [0.47990733 0.87922974] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 19353, \n",
      "                nodes classified as licit: 113450\n",
      "Epoch: 45, \n",
      "            Training Loss: 0.3421, Training Accuracy: 0.9136\n",
      "            Test Loss: 3.2409, Test Accuracy: 0.6697\n",
      "            \n",
      "Precision: [0.37588403 0.8538503 ], Recall: [0.61718098 0.68578117], F1 Score: [0.4672171  0.76064236] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 19228, \n",
      "                nodes classified as licit: 113575\n",
      "Epoch: 50, \n",
      "            Training Loss: 0.3674, Training Accuracy: 0.9166\n",
      "            Test Loss: 4.5677, Test Accuracy: 0.6227\n",
      "            \n",
      "Precision: [0.33495958 0.84161696], Recall: [0.61666774 0.62458197], F1 Score: [0.43411693 0.7170361 ] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 25155, \n",
      "                nodes classified as licit: 107648\n",
      "Epoch: 55, \n",
      "            Training Loss: 0.4366, Training Accuracy: 0.8961\n",
      "            Test Loss: 2.6295, Test Accuracy: 0.7923\n",
      "            \n",
      "Precision: [0.55668669 0.86596451], Recall: [0.56482967 0.8620805 ], F1 Score: [0.56072862 0.86401814] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 15423, \n",
      "                nodes classified as licit: 117380\n",
      "Epoch: 60, \n",
      "            Training Loss: 0.7163, Training Accuracy: 0.9035\n",
      "            Test Loss: 14.5039, Test Accuracy: 0.7801\n",
      "            \n",
      "Precision: [0.8476258  0.77863406], Recall: [0.07673061 0.99577055], F1 Score: [0.14072244 0.87391665] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 25714, \n",
      "                nodes classified as licit: 107089\n",
      "Epoch: 65, \n",
      "            Training Loss: 0.7927, Training Accuracy: 0.9049\n",
      "            Test Loss: 5.4575, Test Accuracy: 0.8141\n",
      "            \n",
      "Precision: [0.63710974 0.8524413 ], Recall: [0.48309489 0.91562734], F1 Score: [0.5495147  0.88290527] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 18623, \n",
      "                nodes classified as licit: 114180\n",
      "Epoch: 70, \n",
      "            Training Loss: 0.4131, Training Accuracy: 0.9000\n",
      "            Test Loss: 2.4167, Test Accuracy: 0.8230\n",
      "            \n",
      "Precision: [0.68426113 0.848804  ], Recall: [0.45659845 0.93539757], F1 Score: [0.54771433 0.88999944] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 19972, \n",
      "                nodes classified as licit: 112831\n",
      "Epoch: 75, \n",
      "            Training Loss: 0.3468, Training Accuracy: 0.9127\n",
      "            Test Loss: 2.1185, Test Accuracy: 0.7781\n",
      "            \n",
      "Precision: [0.52761892 0.85366332], Recall: [0.52088279 0.85700515], F1 Score: [0.52422922 0.85533097] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 20697, \n",
      "                nodes classified as licit: 112106\n",
      "Epoch: 80, \n",
      "            Training Loss: 0.2837, Training Accuracy: 0.9176\n",
      "            Test Loss: 2.2240, Test Accuracy: 0.6994\n",
      "            \n",
      "Precision: [0.4090588  0.86441118], Recall: [0.63155193 0.72024629], F1 Score: [0.49651972 0.78577101] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 21960, \n",
      "                nodes classified as licit: 110843\n",
      "Epoch: 85, \n",
      "            Training Loss: 0.2541, Training Accuracy: 0.9196\n",
      "            Test Loss: 2.0244, Test Accuracy: 0.7062\n",
      "            \n",
      "Precision: [0.41828163 0.86952212], Recall: [0.64528132 0.72482984], F1 Score: [0.50755683 0.79061035] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 20635, \n",
      "                nodes classified as licit: 112168\n",
      "Epoch: 90, \n",
      "            Training Loss: 0.2272, Training Accuracy: 0.9225\n",
      "            Test Loss: 1.9621, Test Accuracy: 0.6933\n",
      "            \n",
      "Precision: [0.39932632 0.85697546], Recall: [0.60845576 0.71936106], F1 Score: [0.48219234 0.78216138] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 21598, \n",
      "                nodes classified as licit: 111205\n",
      "Epoch: 95, \n",
      "            Training Loss: 0.2144, Training Accuracy: 0.9243\n",
      "            Test Loss: 1.5985, Test Accuracy: 0.7243\n",
      "            \n",
      "Precision: [0.43704114 0.86323832], Recall: [0.6072368  0.76016052], F1 Score: [0.50826979 0.80842695] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 20451, \n",
      "                nodes classified as licit: 112352\n",
      "Epoch: 100, \n",
      "            Training Loss: 0.2001, Training Accuracy: 0.9276\n",
      "            Test Loss: 1.6664, Test Accuracy: 0.6930\n",
      "            \n",
      "Precision: [0.39838362 0.85575395], Recall: [0.604029   0.72030531], F1 Score: [0.48011219 0.78220931] \n",
      "\n",
      "---------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 101\n",
    "layers_list = [2]\n",
    "wb = True\n",
    "for LAYERS in layers_list:\n",
    "    model = SAGE(train_data.x.shape[1], 1024, torch.unique(train_data.y).size(0), n_layers=LAYERS)    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.05, weight_decay=5e-4)  # Using AdamW optimizer\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=7, min_lr=1e-5)  # Learning rate scheduler\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'max', patience=7)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if wb:\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"GraphAnomaly\",\n",
    "            name = f\"2_SAGE - n_layers: {LAYERS}\",\n",
    "            # track hyperparameters and run metadata\n",
    "            config={\n",
    "            \"architecture\": \"SAGE_3\",\n",
    "            \"dataset\": \"Time Steps elliptic\",\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"layers\": LAYERS\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def train(data, epoch):\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        out, h = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "        pred = out.argmax(dim=1)\n",
    "        loss = criterion(out, data.y)  # Compute the loss solely based on the training nodes.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct = pred.eq(data.y).sum().item()\n",
    "        total = len(data.y)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        if wb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": loss,\n",
    "                \"train_accuracy\": accuracy,\n",
    "            })\n",
    "        if epoch%5 == 0:\n",
    "            max_value = torch.max(pred)\n",
    "            counts = torch.bincount(pred, minlength=max_value.item() + 1)\n",
    "            print(f\"\"\"Nodes classified as fraud: {counts[0]}, \n",
    "                nodes classified as licit: {counts[1]}\"\"\")\n",
    "            \n",
    "            \n",
    "        return loss, accuracy\n",
    "\n",
    "    def test(data, epoch):\n",
    "        model.eval()  # Set the model to evaluation mode.\n",
    "        with torch.no_grad():\n",
    "            out, _ = model(data.x, data.edge_index)  # Perform a forward pass.\n",
    "            loss = criterion(out, data.y)\n",
    "            \n",
    "            pred = out.argmax(dim=1)  # Get the predicted labels by selecting the class with the highest probability.\n",
    "            # Calculate accuracy\n",
    "            correct = pred.eq(data.y).sum().item()\n",
    "            total = len(data.y)\n",
    "            accuracy = correct / total\n",
    "            \n",
    "            #precision, recall, f1 score\n",
    "            precision = precision_score(data.y.cpu().numpy(), pred.cpu().numpy(), average=None)\n",
    "            recall = recall_score(data.y.cpu().numpy(), pred.cpu().numpy(), average=None)\n",
    "            f1 = f1_score(data.y.cpu().numpy(), pred.cpu().numpy(), average=None)\n",
    "            \n",
    "            other_metrics = [precision, recall, f1]\n",
    "            if wb:\n",
    "                wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"test_loss\": loss,\n",
    "                \"test_accuracy\": accuracy,\n",
    "                \"test_precision_class_0\": precision[0],\n",
    "                \"test_precision_class_1\": precision[1],\n",
    "                \"test_recall_class_0\": recall[0],\n",
    "                \"test_recall_class_1\": recall[1],\n",
    "                \"test_f1_score_class_0\": f1[0],\n",
    "                \"test_f1_score_class_1\": f1[1]\n",
    "            })\n",
    "            \n",
    "        return loss, accuracy, other_metrics\n",
    "            \n",
    "        \n",
    "        \n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    train_acc_hist = []\n",
    "    test_acc_hist = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = train(train_data, epoch)\n",
    "        train_loss_history.append(train_loss.detach().numpy())\n",
    "        train_acc_hist.append(train_acc)\n",
    "        \n",
    "        test_loss, test_acc, metrics = test(test_data, epoch)\n",
    "        test_loss_history.append(test_loss.detach().numpy())\n",
    "        test_acc_hist.append(test_acc)\n",
    "        \n",
    "        if epoch%5 == 0:\n",
    "            print(f\"\"\"Epoch: {epoch}, \n",
    "            Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f}\n",
    "            Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\n",
    "            \"\"\")\n",
    "            \n",
    "            print(f\"Precision: {metrics[0]}, Recall: {metrics[1]}, F1 Score: {metrics[2]} \\n\")\n",
    "            print(\"---------------------------------------------------------------------------------\")\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "if save_model:\n",
    "    torch.save(model.state_dict(), 'final_model_2_layers.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visionEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
