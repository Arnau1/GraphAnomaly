{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import sys\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.nn import MessagePassing, SAGEConv\n",
    "from ogb.nodeproppred import Evaluator #PygNodePropPredDatase\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import wandb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "path = 'C:/Users/User/Desktop/Assignatures/Synthesis project/GraphAnomaly/dades_arnau/'\n",
    "# Load train and test set\n",
    "with open(path + 'sequential_train.pkl', 'rb') as f:\n",
    "    train_set = pickle.load(f)\n",
    "with open(path + 'sequential_test.pkl', 'rb') as f:\n",
    "    test_set = pickle.load(f)\n",
    "with open(path + 'edges.pkl', 'rb') as f:\n",
    "    edges = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.loc[train_set['class'].isin([0, 1])] # Drop unknown\n",
    "test_set = test_set.loc[test_set['class'].isin([0, 1])] # Drop unknown\n",
    "\n",
    "# Split edges\n",
    "edges_train = edges.loc[((edges['node1'].isin(train_set['node'])))]\n",
    "edges_test = edges.loc[((edges['node1'].isin(test_set['node'])))]\n",
    "edges_train = edges_train.reset_index(drop=True)\n",
    "edges_test= edges_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_idx(feats: pd.DataFrame, edges: pd.DataFrame):\n",
    "    mapping_txid = dict(zip(feats['node'], list(feats.index)))      \n",
    "    df_edges_mapped = edges.replace({'node1': mapping_txid, 'node2': mapping_txid})\n",
    "    return df_edges_mapped\n",
    "\n",
    "edges_train = map_idx(feats = train_set, edges = edges_train)\n",
    "edges_test = map_idx(feats = test_set, edges = edges_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node</th>\n",
       "      <th>class</th>\n",
       "      <th>time step</th>\n",
       "      <th>local_feature_1</th>\n",
       "      <th>local_feature_2</th>\n",
       "      <th>local_feature_3</th>\n",
       "      <th>local_feature_4</th>\n",
       "      <th>local_feature_5</th>\n",
       "      <th>local_feature_6</th>\n",
       "      <th>local_feature_7</th>\n",
       "      <th>...</th>\n",
       "      <th>aggregate_feature_63</th>\n",
       "      <th>aggregate_feature_64</th>\n",
       "      <th>aggregate_feature_65</th>\n",
       "      <th>aggregate_feature_66</th>\n",
       "      <th>aggregate_feature_67</th>\n",
       "      <th>aggregate_feature_68</th>\n",
       "      <th>aggregate_feature_69</th>\n",
       "      <th>aggregate_feature_70</th>\n",
       "      <th>aggregate_feature_71</th>\n",
       "      <th>aggregate_feature_72</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54735200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172980</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.218398</td>\n",
       "      <td>-0.285627</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.068266</td>\n",
       "      <td>-0.084674</td>\n",
       "      <td>-0.054450</td>\n",
       "      <td>1.519700</td>\n",
       "      <td>1.521399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6255484</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.169343</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.024025</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>1.753750</td>\n",
       "      <td>1.017910</td>\n",
       "      <td>-0.818353</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.068266</td>\n",
       "      <td>-0.065421</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>1.519700</td>\n",
       "      <td>1.521399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>214640857</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172619</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>2.628795</td>\n",
       "      <td>2.142736</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.068266</td>\n",
       "      <td>-0.038193</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>1.519700</td>\n",
       "      <td>1.521399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>232074746</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171722</td>\n",
       "      <td>-0.196267</td>\n",
       "      <td>0.463609</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>0.055376</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>1.367131</td>\n",
       "      <td>4.407106</td>\n",
       "      <td>0.919675</td>\n",
       "      <td>1.063214</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.010592</td>\n",
       "      <td>-0.014621</td>\n",
       "      <td>-0.085217</td>\n",
       "      <td>-0.905853</td>\n",
       "      <td>-0.942827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>230459468</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.141372</td>\n",
       "      <td>-0.197201</td>\n",
       "      <td>0.463609</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.435113</td>\n",
       "      <td>-0.493772</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136249</th>\n",
       "      <td>288399572</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>-0.172946</td>\n",
       "      <td>-0.070152</td>\n",
       "      <td>0.463609</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140304</td>\n",
       "      <td>0.029745</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.106715</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.183671</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136250</th>\n",
       "      <td>3181</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>1.305212</td>\n",
       "      <td>-0.210553</td>\n",
       "      <td>-1.756361</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>97.300650</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>1.348450</td>\n",
       "      <td>1.590664</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>0.113967</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>1.969527</td>\n",
       "      <td>0.037532</td>\n",
       "      <td>-0.131010</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>0.017772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136257</th>\n",
       "      <td>288717023</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>-0.168035</td>\n",
       "      <td>7.756341</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>7.531856</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>8.189360</td>\n",
       "      <td>0.242712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>0.382961</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136258</th>\n",
       "      <td>104063954</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>-0.172855</td>\n",
       "      <td>0.692064</td>\n",
       "      <td>1.573595</td>\n",
       "      <td>0.403293</td>\n",
       "      <td>0.035526</td>\n",
       "      <td>0.474034</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>1.949179</td>\n",
       "      <td>2.081764</td>\n",
       "      <td>0.792285</td>\n",
       "      <td>1.112808</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.049041</td>\n",
       "      <td>-0.029320</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>0.218964</td>\n",
       "      <td>0.219967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136263</th>\n",
       "      <td>288734134</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>-0.087114</td>\n",
       "      <td>2.112111</td>\n",
       "      <td>0.463609</td>\n",
       "      <td>2.054118</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>2.235141</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.151141</td>\n",
       "      <td>-0.254090</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.335448</td>\n",
       "      <td>0.891428</td>\n",
       "      <td>0.850093</td>\n",
       "      <td>1.519700</td>\n",
       "      <td>1.521399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29894 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             node class  time step  local_feature_1  local_feature_2  \\\n",
       "3        54735200     1          1        -0.172980        -0.184668   \n",
       "7         6255484     1          1        -0.169343        -0.184668   \n",
       "8       214640857     1          1        -0.172619        -0.184668   \n",
       "11      232074746     1          1        -0.171722        -0.196267   \n",
       "12      230459468     1          1        -0.141372        -0.197201   \n",
       "...           ...   ...        ...              ...              ...   \n",
       "136249  288399572     1         34        -0.172946        -0.070152   \n",
       "136250       3181     1         34         1.305212        -0.210553   \n",
       "136257  288717023     0         34        -0.168035         7.756341   \n",
       "136258  104063954     1         34        -0.172855         0.692064   \n",
       "136263  288734134     1         34        -0.087114         2.112111   \n",
       "\n",
       "        local_feature_3  local_feature_4  local_feature_5  local_feature_6  \\\n",
       "3             -1.201369        -0.046932        -0.043875        -0.029140   \n",
       "7             -1.201369        -0.046932        -0.024025        -0.029140   \n",
       "8             -1.201369        -0.046932        -0.043875        -0.029140   \n",
       "11             0.463609        -0.121970         0.055376        -0.113002   \n",
       "12             0.463609        -0.046932        -0.043875        -0.029140   \n",
       "...                 ...              ...              ...              ...   \n",
       "136249         0.463609        -0.121970        -0.043875        -0.113002   \n",
       "136250        -1.756361        -0.121970        97.300650        -0.113002   \n",
       "136257         1.018602         7.531856        -0.063725         8.189360   \n",
       "136258         1.573595         0.403293         0.035526         0.474034   \n",
       "136263         0.463609         2.054118        -0.043875         2.235141   \n",
       "\n",
       "        local_feature_7  ...  aggregate_feature_63  aggregate_feature_64  \\\n",
       "3             -0.061584  ...             -0.218398             -0.285627   \n",
       "7             -0.061584  ...              1.753750              1.017910   \n",
       "8             -0.061584  ...              2.628795              2.142736   \n",
       "11            -0.061584  ...              1.367131              4.407106   \n",
       "12            -0.061584  ...             -0.435113             -0.493772   \n",
       "...                 ...  ...                   ...                   ...   \n",
       "136249        -0.061584  ...              0.140304              0.029745   \n",
       "136250        -0.061584  ...              1.348450              1.590664   \n",
       "136257         0.242712  ...             -0.577099              0.382961   \n",
       "136258        -0.061584  ...              1.949179              2.081764   \n",
       "136263        -0.061584  ...             -0.151141             -0.254090   \n",
       "\n",
       "        aggregate_feature_65  aggregate_feature_66  aggregate_feature_67  \\\n",
       "3                  -0.979074             -0.978556              0.018279   \n",
       "7                  -0.818353             -0.978556             -0.098889   \n",
       "8                  -0.979074             -0.978556             -0.098889   \n",
       "11                  0.919675              1.063214             -0.098889   \n",
       "12                 -0.979074             -0.978556              0.018279   \n",
       "...                      ...                   ...                   ...   \n",
       "136249             -0.979074             -0.978556             -0.098889   \n",
       "136250              0.059948              0.113967             -0.098889   \n",
       "136257              0.241128              0.241406              0.018279   \n",
       "136258              0.792285              1.112808             -0.098889   \n",
       "136263             -0.979074             -0.978556              0.018279   \n",
       "\n",
       "        aggregate_feature_68  aggregate_feature_69  aggregate_feature_70  \\\n",
       "3                  -0.068266             -0.084674             -0.054450   \n",
       "7                  -0.068266             -0.065421             -0.097524   \n",
       "8                  -0.068266             -0.038193             -0.097524   \n",
       "11                 -0.010592             -0.014621             -0.085217   \n",
       "12                 -0.087490             -0.131155             -0.097524   \n",
       "...                      ...                   ...                   ...   \n",
       "136249             -0.106715             -0.131155             -0.183671   \n",
       "136250              1.969527              0.037532             -0.131010   \n",
       "136257             -0.087490             -0.131155             -0.097524   \n",
       "136258             -0.049041             -0.029320             -0.097524   \n",
       "136263              0.335448              0.891428              0.850093   \n",
       "\n",
       "        aggregate_feature_71  aggregate_feature_72  \n",
       "3                   1.519700              1.521399  \n",
       "7                   1.519700              1.521399  \n",
       "8                   1.519700              1.521399  \n",
       "11                 -0.905853             -0.942827  \n",
       "12                 -0.120613             -0.119792  \n",
       "...                      ...                   ...  \n",
       "136249             -0.120613             -0.119792  \n",
       "136250              0.006994              0.017772  \n",
       "136257             -0.120613             -0.119792  \n",
       "136258              0.218964              0.219967  \n",
       "136263              1.519700              1.521399  \n",
       "\n",
       "[29894 rows x 168 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(feats: pd.DataFrame, edges:pd.DataFrame):\n",
    "    x = torch.tensor(feats.drop(columns=['class', 'time step', 'node']).values, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges.values, dtype=torch.long).T    \n",
    "    y = torch.tensor(np.array(feats['class'].values, np.int64))\n",
    "    time = torch.tensor(feats['time step'].values)\n",
    "    data = Data(x=x, edge_index=edge_index, y=y, time=time)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = get_data(train_set, edges_train)\n",
    "test_data = get_data(test_set, edges_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels,\n",
    "                 hidden_channels, out_channels,\n",
    "                 n_layers=2):\n",
    "        \n",
    "        super(SAGE, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers_bn = torch.nn.ModuleList()\n",
    "        if n_layers == 1:\n",
    "            self.layers.append(SAGEConv(in_channels, out_channels,   normalize=False))\n",
    "        elif n_layers == 2:\n",
    "            self.layers.append(SAGEConv(in_channels, hidden_channels, normalize=False))\n",
    "            self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "            # self.layers.append(SAGEConv(hidden_channels, out_channels, normalize=False))\n",
    "        else:\n",
    "            self.layers.append(SAGEConv(in_channels, hidden_channels, normalize=False))\n",
    "            # self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(n_layers - 2):\n",
    "            self.layers.append(SAGEConv(hidden_channels,  hidden_channels, normalize=False))\n",
    "            self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "                    \n",
    "                \n",
    "        if n_layers != 1:\n",
    "            self.layers.append(SAGEConv(hidden_channels, out_channels, normalize=False))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer.reset_parameters()\n",
    "            \n",
    "            \n",
    "            \n",
    "    def forward(self, x, edge_index):\n",
    "        if len(self.layers) > 1:\n",
    "            looper = self.layers[:-1]\n",
    "        else:\n",
    "            looper = self.layers\n",
    "        \n",
    "        for i, layer in enumerate(looper):\n",
    "            x = layer(x, edge_index)\n",
    "            # print(f\"SHAPE: {x.shape}, step: {i}\")\n",
    "            # print(f\"Step: {i}\")\n",
    "            try:\n",
    "                x = self.layers_bn[i](x)\n",
    "            except Exception as e:\n",
    "                abs(1)\n",
    "            finally:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        if len(self.layers) > 1:\n",
    "            x = self.layers[-1](x, edge_index)\n",
    "        return F.log_softmax(x, dim=-1), torch.var(x)\n",
    "    \n",
    "    def inference(self, total_loader, device):\n",
    "        xs = []\n",
    "        var_ = []\n",
    "        for batch in total_loader:\n",
    "            out, var = self.forward(batch.x.to(device), batch.edge_index.to(device))\n",
    "            out = out[:batch.batch_size]\n",
    "            xs.append(out.cpu())\n",
    "            var_.append(var.item())\n",
    "        \n",
    "        out_all = torch.cat(xs, dim=0)\n",
    "        \n",
    "        return out_all, var_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:d8b3k9vz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abd5f842a0d447b982beddba49e0086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.030 MB uploaded\\r'), FloatProgress(value=0.04431723162024828, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2_SAGE - n_layers: 2</strong> at: <a href='https://wandb.ai/uabai/GraphAnomaly/runs/d8b3k9vz' target=\"_blank\">https://wandb.ai/uabai/GraphAnomaly/runs/d8b3k9vz</a><br/> View project at: <a href='https://wandb.ai/uabai/GraphAnomaly' target=\"_blank\">https://wandb.ai/uabai/GraphAnomaly</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240529_211656-d8b3k9vz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:d8b3k9vz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\Assignatures\\Synthesis project\\GraphAnomaly\\Pipeline\\wandb\\run-20240529_212035-dzjewk51</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uabai/GraphAnomaly/runs/dzjewk51' target=\"_blank\">2_SAGE - n_layers: 2</a></strong> to <a href='https://wandb.ai/uabai/GraphAnomaly' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uabai/GraphAnomaly' target=\"_blank\">https://wandb.ai/uabai/GraphAnomaly</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uabai/GraphAnomaly/runs/dzjewk51' target=\"_blank\">https://wandb.ai/uabai/GraphAnomaly/runs/dzjewk51</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "Found indices in 'edge_index' that are larger than 29893 (got 136258). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 29894) in your node feature matrix and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:317\u001b[0m, in \u001b[0;36mMessagePassing._index_select_safe\u001b[1;34m(self, src, index)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim, index)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 94\u001b[0m\n\u001b[0;32m     91\u001b[0m test_acc_hist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m---> 94\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train(train_data, epoch)\n\u001b[0;32m     95\u001b[0m     train_loss_history\u001b[38;5;241m.\u001b[39mappend(train_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     96\u001b[0m     train_acc_hist\u001b[38;5;241m.\u001b[39mappend(train_acc)\n",
      "Cell \u001b[1;32mIn[11], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data, epoch)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(data, epoch):\n\u001b[0;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients.\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     out, h \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)  \u001b[38;5;66;03m# Perform a single forward pass.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     pred \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     29\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)  \u001b[38;5;66;03m# Compute the loss solely based on the training nodes.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 39\u001b[0m, in \u001b[0;36mSAGE.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     36\u001b[0m     looper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(looper):\n\u001b[1;32m---> 39\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x, edge_index)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# print(f\"SHAPE: {x.shape}, step: {i}\")\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# print(f\"Step: {i}\")\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\sage_conv.py:134\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[1;34m(self, x, edge_index, size)\u001b[0m\n\u001b[0;32m    131\u001b[0m     x \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mrelu(), x[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, size\u001b[38;5;241m=\u001b[39msize)\n\u001b[0;32m    135\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_l(out)\n\u001b[0;32m    137\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\torch_geometric.nn.conv.sage_conv_SAGEConv_propagate_5s3qyzks.py:158\u001b[0m, in \u001b[0;36mpropagate\u001b[1;34m(self, edge_index, x, size)\u001b[0m\n\u001b[0;32m    152\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    153\u001b[0m         out,\n\u001b[0;32m    154\u001b[0m     )\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect(\n\u001b[0;32m    159\u001b[0m         edge_index,\n\u001b[0;32m    160\u001b[0m         x,\n\u001b[0;32m    161\u001b[0m         mutable_size,\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Begin Message Forward Pre Hook #######################################\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\torch_geometric.nn.conv.sage_conv_SAGEConv_propagate_5s3qyzks.py:76\u001b[0m, in \u001b[0;36mcollect\u001b[1;34m(self, edge_index, x, size)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_x_0, Tensor):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_size(size, \u001b[38;5;241m0\u001b[39m, _x_0)\n\u001b[1;32m---> 76\u001b[0m     x_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_select(_x_0, edge_index_j)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     x_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:313\u001b[0m, in \u001b[0;36mMessagePassing._index_select\u001b[1;34m(self, src, index)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim, index)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_select_safe(src, index)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:328\u001b[0m, in \u001b[0;36mMessagePassing._index_select_safe\u001b[1;34m(self, src, index)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound negative indices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Please ensure that all \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m point to valid indices \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour node feature matrix and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)):\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound indices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m that are larger \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthan \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Please ensure that all \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m point to valid indices \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour node feature matrix and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mIndexError\u001b[0m: Found indices in 'edge_index' that are larger than 29893 (got 136258). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 29894) in your node feature matrix and try again."
     ]
    }
   ],
   "source": [
    "EPOCHS = 101\n",
    "layers_list = [2]\n",
    "# layers_list = [4]\n",
    "wb = True\n",
    "for LAYERS in layers_list:\n",
    "    model = SAGE(train_data.x.shape[1], 256, torch.unique(train_data.y).size(0), n_layers=LAYERS)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.03)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'max', patience=7)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if wb:\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"GraphAnomaly\",\n",
    "            name = f\"2_SAGE - n_layers: {LAYERS}\",\n",
    "            # track hyperparameters and run metadata\n",
    "            config={\n",
    "            \"architecture\": \"SAGE_3\",\n",
    "            \"dataset\": \"Time Steps elliptic\",\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"layers\": LAYERS\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def train(data, epoch):\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        out, h = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "        pred = out.argmax(dim=1)\n",
    "        loss = criterion(out, data.y)  # Compute the loss solely based on the training nodes.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct = pred.eq(data.y).sum().item()\n",
    "        total = len(data.y)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        if wb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": loss,\n",
    "                \"train_accuracy\": accuracy,\n",
    "            })\n",
    "        if epoch%5 == 0:\n",
    "            max_value = torch.max(pred)\n",
    "            counts = torch.bincount(pred, minlength=max_value.item() + 1)\n",
    "            print(f\"\"\"Nodes classified as fraud: {counts[0]}, \n",
    "                nodes classified as licit: {counts[1]}\"\"\")\n",
    "            \n",
    "            \n",
    "        return loss, accuracy\n",
    "\n",
    "    def test(data, epoch):\n",
    "        model.eval()  # Set the model to evaluation mode.\n",
    "        with torch.no_grad():\n",
    "            out, _ = model(data.x, data.edge_index)  # Perform a forward pass.\n",
    "            loss = criterion(out, data.y)\n",
    "            \n",
    "            pred = out.argmax(dim=1)  # Get the predicted labels by selecting the class with the highest probability.\n",
    "            # Calculate accuracy\n",
    "            correct = pred.eq(data.y).sum().item()\n",
    "            total = len(data.y)\n",
    "            accuracy = correct / total\n",
    "            \n",
    "            #precision, recall, f1 score\n",
    "            precision = precision_score(data.y.cpu().numpy(), pred.cpu().numpy(), average=None)\n",
    "            recall = recall_score(data.y.cpu().numpy(), pred.cpu().numpy(), average=None)\n",
    "            f1 = f1_score(data.y.cpu().numpy(), pred.cpu().numpy(), average=None)\n",
    "            \n",
    "            other_metrics = [precision, recall, f1]\n",
    "            if wb:\n",
    "                wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"test_loss\": loss,\n",
    "                \"test_accuracy\": accuracy,\n",
    "                \"test_precision_class_0\": precision[0],\n",
    "                \"test_precision_class_1\": precision[1],\n",
    "                \"test_recall_class_0\": recall[0],\n",
    "                \"test_recall_class_1\": recall[1],\n",
    "                \"test_f1_score_class_0\": f1[0],\n",
    "                \"test_f1_score_class_1\": f1[1]\n",
    "            })\n",
    "            \n",
    "        return loss, accuracy, other_metrics\n",
    "            \n",
    "        \n",
    "        \n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    train_acc_hist = []\n",
    "    test_acc_hist = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = train(train_data, epoch)\n",
    "        train_loss_history.append(train_loss.detach().numpy())\n",
    "        train_acc_hist.append(train_acc)\n",
    "        \n",
    "        test_loss, test_acc, metrics = test(test_data, epoch)\n",
    "        test_loss_history.append(test_loss.detach().numpy())\n",
    "        test_acc_hist.append(test_acc)\n",
    "        \n",
    "        if epoch%5 == 0:\n",
    "            print(f\"\"\"Epoch: {epoch}, \n",
    "            Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f}\n",
    "            Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\n",
    "            \"\"\")\n",
    "            \n",
    "            print(f\"Precision: {metrics[0]}, Recall: {metrics[1]}, F1 Score: {metrics[2]} \\n\")\n",
    "            print(\"---------------------------------------------------------------------------------\")\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "if save_model:\n",
    "    torch.save(model.state_dict(), 'final_model_2_layers.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_data.x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value 0: 7341 occurrences\n",
      "Value 1: 115940 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = SAGE(unk_data.x.shape[1], 256, 2, n_layers=2)\n",
    "\n",
    "# Load the model's state dict\n",
    "model.load_state_dict(torch.load('C:/Users/gsamp/OneDrive/Documents/AI-3/2n Semestre/Projecte de Síntesi 2/GraphAnomaly/GNN_models/trained_models/final_model_2_layers.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Forward pass through the model\n",
    "    output = model(unk_data.x, unk_data.edge_index)\n",
    "\n",
    "# 'output' now contains the model's predictions for the unseen data\n",
    "output[0]\n",
    "predicted_classes = torch.argmax(output[0], dim=1)\n",
    "\n",
    "# Count the occurrences of each value\n",
    "value_counts = torch.bincount(predicted_classes)\n",
    "\n",
    "# Print the distribution\n",
    "for value, count in enumerate(value_counts):\n",
    "    print(f\"Value {value}: {count.item()} occurrences\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visionEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
