{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import sys\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import torch.utils.data as data \n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.nn import MessagePassing, SAGEConv\n",
    "from ogb.nodeproppred import Evaluator #PygNodePropPredDatase\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files\n",
    "path = \"C:/Users/gsamp/OneDrive/Documents/AI-3/2n Semestre/Projecte de Síntesi 2/GraphAnomaly/dades_guillem/\"\n",
    "df_train_init = pd.read_csv(path + \"train_set.csv\") \n",
    "df_test_init = pd.read_csv(path + \"test_set.csv\")\n",
    "# df_train_edges = pd.read_csv(path + \"train_edges.csv\")\n",
    "# df_test_edges = pd.read_csv(path + \"test_edges.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIXO JA NO HAURIA DE CALDRE, ESTA AL DATA SPLIT\n",
    "path = \"C:/Users/gsamp/OneDrive/Documents/AI-3/2n Semestre/Projecte de Síntesi 2/GraphAnomaly/dades_guillem/\"\n",
    "df_classes = pd.read_csv(path + \"elliptic_txs_classes.csv\") # Nodes' labels\n",
    "df_edges_init = pd.read_csv(path + \"elliptic_txs_edgelist.csv\") # Edges\n",
    "df_features = pd.read_csv(path + \"elliptic_txs_features.csv\", header=None) # Nodes' features\n",
    "\n",
    "# Change column names of df_features\n",
    "colNames1 = {'0': 'txId', 1: \"Time step\"}\n",
    "colNames2 = {str(ii+2): \"Local_feature_\" + str(ii+1) for ii in range(93)}\n",
    "colNames3 = {str(ii+95): \"Aggregate_feature_\" + str(ii+1) for ii in range(72)}\n",
    "\n",
    "colNames = dict(colNames1, **colNames2, **colNames3 )\n",
    "colNames = {int(jj): item_kk for jj,item_kk in colNames.items()}\n",
    "\n",
    "df_features = df_features.rename(columns=colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contador de valors per classe: \n",
      " class\n",
      "1    34654\n",
      "0     2672\n",
      "Name: count, dtype: int64\n",
      "\n",
      "contador de valors per classe: \n",
      " class\n",
      "1    7365\n",
      "0    1873\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsamp\\AppData\\Local\\Temp\\ipykernel_12680\\1529718817.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_feats['class'] = df_feats['class'].replace({1: 0, 2: 1})\n"
     ]
    }
   ],
   "source": [
    "# AIXO JA NO HAURIA DE CALDRE, ESTA AL DATA SPLIT\n",
    "def prep_df(feats: pd.DataFrame, edges: pd.DataFrame):\n",
    "    #1 és la classe illicit, 2 la  licit\n",
    "    df_feats = feats.loc[feats['class'].isin([1, 2])]\n",
    "    df_feats['class'] = df_feats['class'].replace({1: 0, 2: 1})\n",
    "    df_feats = df_feats.reset_index(drop=True)\n",
    "\n",
    "    #ens quedem només amb els edges que apareixen en el nodes d'entrenament\n",
    "    df_edges = edges.loc[((edges['txId1'].isin(df_feats['txId'])) & (df_edges_init['txId2'].isin(df_feats['txId'])))]\n",
    "    df_edges = df_edges.reset_index(drop=True)\n",
    "    print(f\"contador de valors per classe: \\n {df_feats['class'].value_counts()}\\n\")\n",
    "    return  df_feats, df_edges\n",
    "\n",
    "df_train, df_edges_train = prep_df(df_train_init, df_edges_init)\n",
    "df_test, df_edges_test = prep_df(df_test_init, df_edges_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_idx(feats: pd.DataFrame, edges: pd.DataFrame, save = True, loading_dir = \"a\"):\n",
    "    mapping_txid = dict(zip(feats['txId'], list(feats.index)))\n",
    "    dir = 'dades_guillem/' + str(loading_dir) + '.pkl'\n",
    "    if save:\n",
    "        df_edges_mapped = edges.replace({'txId1': mapping_txid, 'txId2': mapping_txid})\n",
    "        \n",
    "        df_edges_mapped.to_pickle(loading_dir)\n",
    "    else:\n",
    "        df_edges_mapped = pd.read_pickle(loading_dir)\n",
    "    return df_edges_mapped\n",
    "\n",
    "df_edges_mapped_train = map_idx(feats = df_train, edges = df_edges_train, save = True, loading_dir='train')\n",
    "df_edges_mapped_test = map_idx(feats = df_test, edges = df_edges_test, save = True, loading_dir='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(feats: pd.DataFrame, edges:pd.DataFrame):\n",
    "    x = torch.tensor(feats.drop(columns=['class', 'Time Step', 'txId']).values, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges.values, dtype=torch.long).T\n",
    "    y = torch.tensor(feats['class'].values)\n",
    "    time = torch.tensor(feats['Time Step'].values)\n",
    "    data = Data(x=x, edge_index=edge_index, y=y, time=time)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = get_data(df_train, df_edges_mapped_train)\n",
    "test_data = get_data(df_test, df_edges_mapped_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return torch.tensor(sample, dtype=torch.float32), torch.tensor(sample, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim=50):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, encoding_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(encoding_dim, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, encoding_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(encoding_dim, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the saved model\n",
    "model_path = 'C:/Users/gsamp/OneDrive/Documents/AI-3/2n Semestre/Projecte de Síntesi 2/GraphAnomaly/GNN_models/trained_models/ae_second_train.pth'\n",
    "input_dim = test_data.x.shape[1]\n",
    "# Create an instance of your model\n",
    "model = Autoencoder(input_dim)\n",
    "\n",
    "# Load the state_dict into the model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Convert input_data to a torch tensor if it's not already\n",
    "train_input_data = train_data.x.clone().detach()\n",
    "test_input_data = test_data.x.clone().detach()\n",
    "\n",
    "# Pass the input_data through the encoder\n",
    "train_latent_representation = model.encoder(train_input_data)\n",
    "test_latent_representation = model.encoder(test_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.9853721266677383\n",
      "Accuracy on test set: 0.9204373240961247\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have already obtained latent representations for your training data\n",
    "# X_latent_train is the matrix of latent representations, y_train is the corresponding labels\n",
    "\n",
    "# Initialize base classifiers\n",
    "logistic_clf = LogisticRegression()\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "random_forest_clf = RandomForestClassifier()\n",
    "gradient_boosting_clf = GradientBoostingClassifier()\n",
    "\n",
    "# Ensemble classifier combining base classifiers\n",
    "ensemble_clf = VotingClassifier(estimators=[\n",
    "    ('logistic', logistic_clf), \n",
    "    ('tree', tree_clf),\n",
    "    ('random_forest', random_forest_clf),\n",
    "    ('gradient_boosting', gradient_boosting_clf)\n",
    "], voting='soft')\n",
    "\n",
    "# Assuming train_latent_representation and test_latent_representation are tensors\n",
    "train_input_latent = train_latent_representation.detach().numpy()\n",
    "test_input_latent = test_latent_representation.detach().numpy()\n",
    "\n",
    "# Train ensemble classifier\n",
    "ensemble_clf.fit(train_input_latent, train_data.y)\n",
    "\n",
    "# Predictions on training data\n",
    "y_pred_train = ensemble_clf.predict(train_input_latent)\n",
    "\n",
    "# Predictions on test data\n",
    "y_pred_test = ensemble_clf.predict(test_input_latent)\n",
    "\n",
    "# Evaluate performance\n",
    "train_accuracy = accuracy_score(train_data.y, y_pred_train)\n",
    "print(\"Accuracy on training set:\", train_accuracy)\n",
    "\n",
    "test_accuracy = accuracy_score(test_data.y, y_pred_test)\n",
    "print(\"Accuracy on test set:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Precision=0.9537480063795853, Recall=0.6385477843032568, F1-score=0.7649504317236968\n",
      "Class 1: Precision=0.9152054108216433, Recall=0.9921249151391718, F1-score=0.9521141442439247\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Assuming you have already obtained predictions for your test data\n",
    "# y_true is the true labels, y_pred is the predicted labels by the ensemble classifier\n",
    "\n",
    "# Compute precision, recall, and F1 score for each class\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(test_data.y, y_pred_test)\n",
    "\n",
    "# Print precision, recall, and F1 score for each class\n",
    "for i in range(len(precision)):\n",
    "    print(f\"Class {i}: Precision={precision[i]}, Recall={recall[i]}, F1-score={f1_score[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visionEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
