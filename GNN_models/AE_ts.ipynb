{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import sys\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import torch.utils.data as data \n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.nn import MessagePassing, SAGEConv\n",
    "from ogb.nodeproppred import Evaluator #PygNodePropPredDatase\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files\n",
    "path = \"C:/Users/gsamp/OneDrive/Documents/AI-3/2n Semestre/Projecte de Síntesi 2/GraphAnomaly/dades_guillem/\"\n",
    "df_train_init = pd.read_csv(path + \"train_set.csv\") \n",
    "df_test_init = pd.read_csv(path + \"test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/gsamp/OneDrive/Documents/AI-3/2n Semestre/Projecte de Síntesi 2/GraphAnomaly/dades_guillem/\"\n",
    "df_classes = pd.read_csv(path + \"elliptic_txs_classes.csv\") # Nodes' labels\n",
    "df_edges_init = pd.read_csv(path + \"elliptic_txs_edgelist.csv\") # Edges\n",
    "df_features = pd.read_csv(path + \"elliptic_txs_features.csv\", header=None) # Nodes' features\n",
    "\n",
    "# Change column names of df_features\n",
    "colNames1 = {'0': 'txId', 1: \"Time step\"}\n",
    "colNames2 = {str(ii+2): \"Local_feature_\" + str(ii+1) for ii in range(93)}\n",
    "colNames3 = {str(ii+95): \"Aggregate_feature_\" + str(ii+1) for ii in range(72)}\n",
    "\n",
    "colNames = dict(colNames1, **colNames2, **colNames3 )\n",
    "colNames = {int(jj): item_kk for jj,item_kk in colNames.items()}\n",
    "\n",
    "df_features = df_features.rename(columns=colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contador de valors per classe: \n",
      " class\n",
      "1    34654\n",
      "0     2672\n",
      "Name: count, dtype: int64\n",
      "\n",
      "contador de valors per classe: \n",
      " class\n",
      "1    7365\n",
      "0    1873\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsamp\\AppData\\Local\\Temp\\ipykernel_876\\1761167425.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_feats['class'] = df_feats['class'].replace({1: 0, 2: 1})\n"
     ]
    }
   ],
   "source": [
    "def prep_df(feats: pd.DataFrame, edges: pd.DataFrame):\n",
    "    #1 és la classe illicit, 2 la  licit\n",
    "    df_feats = feats.loc[feats['class'].isin([1, 2])]\n",
    "    df_feats['class'] = df_feats['class'].replace({1: 0, 2: 1})\n",
    "    df_feats = df_feats.reset_index(drop=True)\n",
    "\n",
    "    #ens quedem només amb els edges que apareixen en el nodes que estan en el dataset\n",
    "    df_edges = edges.loc[((edges['txId1'].isin(df_feats['txId'])) & (df_edges_init['txId2'].isin(df_feats['txId'])))]\n",
    "    df_edges = df_edges.reset_index(drop=True)\n",
    "    print(f\"contador de valors per classe: \\n {df_feats['class'].value_counts()}\\n\")\n",
    "    return  df_feats, df_edges\n",
    "\n",
    "df_train, df_edges_train = prep_df(df_train_init, df_edges_init)\n",
    "df_test, df_edges_test = prep_df(df_test_init, df_edges_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_idx(feats: pd.DataFrame, edges: pd.DataFrame, save = True, loading_dir = \"a\"):\n",
    "    mapping_txid = dict(zip(feats['txId'], list(feats.index)))\n",
    "    dir = 'dades_guillem/' + str(loading_dir) + '.pkl'\n",
    "    if save:\n",
    "        df_edges_mapped = edges.replace({'txId1': mapping_txid, 'txId2': mapping_txid})\n",
    "        \n",
    "        df_edges_mapped.to_pickle(loading_dir)\n",
    "    else:\n",
    "        df_edges_mapped = pd.read_pickle(loading_dir)\n",
    "    return df_edges_mapped\n",
    "\n",
    "df_edges_mapped_train = map_idx(feats = df_train, edges = df_edges_train, save = True, loading_dir='train')\n",
    "df_edges_mapped_test = map_idx(feats = df_test, edges = df_edges_test, save = True, loading_dir='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(feats: pd.DataFrame, edges:pd.DataFrame):\n",
    "    x = torch.tensor(feats.drop(columns=['class', 'Time Step', 'txId']).values, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges.values, dtype=torch.long).T\n",
    "    y = torch.tensor(feats['class'].values)\n",
    "    time = torch.tensor(feats['Time Step'].values)\n",
    "    data = Data(x=x, edge_index=edge_index, y=y, time=time)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = get_data(df_train, df_edges_mapped_train)\n",
    "test_data = get_data(df_test, df_edges_mapped_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for the AE case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate illicit and licit data\n",
    "def separate_data(feats):\n",
    "    licit_x = torch.tensor(feats.loc[feats['class'] == 1].drop(columns=['class', 'Time Step']).values, dtype=torch.float)\n",
    "    licit_y = torch.tensor(feats.loc[feats['class'] == 1]['class'].values)\n",
    "    licit_data = Data(x=licit_x, y=licit_y)\n",
    "\n",
    "    illicit_x = torch.tensor(feats.loc[feats['class'] == 0].drop(columns=['class', 'Time Step']).values, dtype=torch.float)\n",
    "    illicit_y = torch.tensor(feats.loc[feats['class'] == 0]['class'].values)\n",
    "    illicit_data = Data(x=illicit_x, y=illicit_x)\n",
    "    return licit_data, illicit_data\n",
    "\n",
    "train_licit, train_illicit = separate_data(df_train)\n",
    "test_licit, test_illicit = separate_data(df_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return torch.tensor(sample, dtype=torch.float32), torch.tensor(sample, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim=50):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(50, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(autoencoder, data_loader, criterion, optimizer, val_loader, num_epochs=10, learning_rate=0.001):   \n",
    "    total_train_loss = [] \n",
    "    total_validation_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = autoencoder(inputs)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss = running_loss / len(data_loader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}\")\n",
    "        total_train_loss.append(train_loss)\n",
    "        \n",
    "        #validació dins del mateix train loop\n",
    "        validation_loss = val_ae(autoencoder=autoencoder, data_loader=val_loader, criterion=criterion)\n",
    "        print(f\"Validation Loss: {validation_loss:.4f}\")\n",
    "        total_validation_loss.append(validation_loss)\n",
    "    return total_train_loss, total_validation_loss\n",
    "\n",
    "def val_ae(autoencoder, data_loader, criterion):\n",
    "    autoencoder.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = val_loss / len(data_loader.dataset)\n",
    "    return epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34654, 165])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "#samplejar només n elements de les lícites per entrenar el AE\n",
    "x_train = torch.tensor(df_train.loc[df_train[\"class\"] == 1].drop(columns=['class', 'txId', 'Time Step']).values)\n",
    "\n",
    "# n = int(x.shape[0] * 0.2)\n",
    "# x_train = x[n:]\n",
    "x_val = torch.tensor(df_test.loc[df_test[\"class\"] == 1].drop(columns=['class', 'txId', 'Time Step']).values)\n",
    "print(x_train.shape)\n",
    "#amb el scaler la loss passe de 4 a 0.1\n",
    "train_data = preprocessing.MinMaxScaler().fit_transform(x_train)\n",
    "val_data = preprocessing.MinMaxScaler().fit_transform(x_val)\n",
    "\n",
    "# train_data, val_data = train_test_split(x, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(val_data)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/250], Training Loss: 0.1505\n",
      "Validation Loss: 0.1473\n",
      "Epoch [2/250], Training Loss: 0.1499\n",
      "Validation Loss: 0.1467\n",
      "Epoch [3/250], Training Loss: 0.1492\n",
      "Validation Loss: 0.1460\n",
      "Epoch [4/250], Training Loss: 0.1485\n",
      "Validation Loss: 0.1453\n",
      "Epoch [5/250], Training Loss: 0.1478\n",
      "Validation Loss: 0.1446\n",
      "Epoch [6/250], Training Loss: 0.1471\n",
      "Validation Loss: 0.1439\n",
      "Epoch [7/250], Training Loss: 0.1464\n",
      "Validation Loss: 0.1432\n",
      "Epoch [8/250], Training Loss: 0.1457\n",
      "Validation Loss: 0.1425\n",
      "Epoch [9/250], Training Loss: 0.1450\n",
      "Validation Loss: 0.1417\n",
      "Epoch [10/250], Training Loss: 0.1443\n",
      "Validation Loss: 0.1410\n",
      "Epoch [11/250], Training Loss: 0.1436\n",
      "Validation Loss: 0.1404\n",
      "Epoch [12/250], Training Loss: 0.1429\n",
      "Validation Loss: 0.1397\n",
      "Epoch [13/250], Training Loss: 0.1422\n",
      "Validation Loss: 0.1390\n",
      "Epoch [14/250], Training Loss: 0.1415\n",
      "Validation Loss: 0.1383\n",
      "Epoch [15/250], Training Loss: 0.1408\n",
      "Validation Loss: 0.1376\n",
      "Epoch [16/250], Training Loss: 0.1402\n",
      "Validation Loss: 0.1370\n",
      "Epoch [17/250], Training Loss: 0.1395\n",
      "Validation Loss: 0.1363\n",
      "Epoch [18/250], Training Loss: 0.1388\n",
      "Validation Loss: 0.1356\n",
      "Epoch [19/250], Training Loss: 0.1381\n",
      "Validation Loss: 0.1349\n",
      "Epoch [20/250], Training Loss: 0.1374\n",
      "Validation Loss: 0.1342\n",
      "Epoch [21/250], Training Loss: 0.1368\n",
      "Validation Loss: 0.1335\n",
      "Epoch [22/250], Training Loss: 0.1361\n",
      "Validation Loss: 0.1329\n",
      "Epoch [23/250], Training Loss: 0.1354\n",
      "Validation Loss: 0.1322\n",
      "Epoch [24/250], Training Loss: 0.1348\n",
      "Validation Loss: 0.1315\n",
      "Epoch [25/250], Training Loss: 0.1341\n",
      "Validation Loss: 0.1309\n",
      "Epoch [26/250], Training Loss: 0.1334\n",
      "Validation Loss: 0.1302\n",
      "Epoch [27/250], Training Loss: 0.1328\n",
      "Validation Loss: 0.1295\n",
      "Epoch [28/250], Training Loss: 0.1321\n",
      "Validation Loss: 0.1288\n",
      "Epoch [29/250], Training Loss: 0.1314\n",
      "Validation Loss: 0.1282\n",
      "Epoch [30/250], Training Loss: 0.1307\n",
      "Validation Loss: 0.1275\n",
      "Epoch [31/250], Training Loss: 0.1300\n",
      "Validation Loss: 0.1268\n",
      "Epoch [32/250], Training Loss: 0.1293\n",
      "Validation Loss: 0.1261\n",
      "Epoch [33/250], Training Loss: 0.1286\n",
      "Validation Loss: 0.1254\n",
      "Epoch [34/250], Training Loss: 0.1279\n",
      "Validation Loss: 0.1246\n",
      "Epoch [35/250], Training Loss: 0.1272\n",
      "Validation Loss: 0.1239\n",
      "Epoch [36/250], Training Loss: 0.1265\n",
      "Validation Loss: 0.1232\n",
      "Epoch [37/250], Training Loss: 0.1257\n",
      "Validation Loss: 0.1224\n",
      "Epoch [38/250], Training Loss: 0.1250\n",
      "Validation Loss: 0.1217\n",
      "Epoch [39/250], Training Loss: 0.1243\n",
      "Validation Loss: 0.1210\n",
      "Epoch [40/250], Training Loss: 0.1236\n",
      "Validation Loss: 0.1203\n",
      "Epoch [41/250], Training Loss: 0.1229\n",
      "Validation Loss: 0.1195\n",
      "Epoch [42/250], Training Loss: 0.1221\n",
      "Validation Loss: 0.1188\n",
      "Epoch [43/250], Training Loss: 0.1214\n",
      "Validation Loss: 0.1180\n",
      "Epoch [44/250], Training Loss: 0.1207\n",
      "Validation Loss: 0.1173\n",
      "Epoch [45/250], Training Loss: 0.1199\n",
      "Validation Loss: 0.1166\n",
      "Epoch [46/250], Training Loss: 0.1192\n",
      "Validation Loss: 0.1158\n",
      "Epoch [47/250], Training Loss: 0.1184\n",
      "Validation Loss: 0.1150\n",
      "Epoch [48/250], Training Loss: 0.1176\n",
      "Validation Loss: 0.1142\n",
      "Epoch [49/250], Training Loss: 0.1168\n",
      "Validation Loss: 0.1134\n",
      "Epoch [50/250], Training Loss: 0.1160\n",
      "Validation Loss: 0.1126\n",
      "Epoch [51/250], Training Loss: 0.1152\n",
      "Validation Loss: 0.1118\n",
      "Epoch [52/250], Training Loss: 0.1144\n",
      "Validation Loss: 0.1110\n",
      "Epoch [53/250], Training Loss: 0.1137\n",
      "Validation Loss: 0.1102\n",
      "Epoch [54/250], Training Loss: 0.1129\n",
      "Validation Loss: 0.1095\n",
      "Epoch [55/250], Training Loss: 0.1122\n",
      "Validation Loss: 0.1087\n",
      "Epoch [56/250], Training Loss: 0.1114\n",
      "Validation Loss: 0.1080\n",
      "Epoch [57/250], Training Loss: 0.1107\n",
      "Validation Loss: 0.1072\n",
      "Epoch [58/250], Training Loss: 0.1100\n",
      "Validation Loss: 0.1065\n",
      "Epoch [59/250], Training Loss: 0.1093\n",
      "Validation Loss: 0.1058\n",
      "Epoch [60/250], Training Loss: 0.1086\n",
      "Validation Loss: 0.1051\n",
      "Epoch [61/250], Training Loss: 0.1079\n",
      "Validation Loss: 0.1045\n",
      "Epoch [62/250], Training Loss: 0.1072\n",
      "Validation Loss: 0.1038\n",
      "Epoch [63/250], Training Loss: 0.1066\n",
      "Validation Loss: 0.1032\n",
      "Epoch [64/250], Training Loss: 0.1060\n",
      "Validation Loss: 0.1025\n",
      "Epoch [65/250], Training Loss: 0.1054\n",
      "Validation Loss: 0.1019\n",
      "Epoch [66/250], Training Loss: 0.1048\n",
      "Validation Loss: 0.1013\n",
      "Epoch [67/250], Training Loss: 0.1042\n",
      "Validation Loss: 0.1007\n",
      "Epoch [68/250], Training Loss: 0.1036\n",
      "Validation Loss: 0.1002\n",
      "Epoch [69/250], Training Loss: 0.1031\n",
      "Validation Loss: 0.0996\n",
      "Epoch [70/250], Training Loss: 0.1025\n",
      "Validation Loss: 0.0991\n",
      "Epoch [71/250], Training Loss: 0.1020\n",
      "Validation Loss: 0.0986\n",
      "Epoch [72/250], Training Loss: 0.1016\n",
      "Validation Loss: 0.0981\n",
      "Epoch [73/250], Training Loss: 0.1011\n",
      "Validation Loss: 0.0977\n",
      "Epoch [74/250], Training Loss: 0.1006\n",
      "Validation Loss: 0.0972\n",
      "Epoch [75/250], Training Loss: 0.1002\n",
      "Validation Loss: 0.0968\n",
      "Epoch [76/250], Training Loss: 0.0998\n",
      "Validation Loss: 0.0964\n",
      "Epoch [77/250], Training Loss: 0.0994\n",
      "Validation Loss: 0.0960\n",
      "Epoch [78/250], Training Loss: 0.0990\n",
      "Validation Loss: 0.0956\n",
      "Epoch [79/250], Training Loss: 0.0986\n",
      "Validation Loss: 0.0952\n",
      "Epoch [80/250], Training Loss: 0.0983\n",
      "Validation Loss: 0.0949\n",
      "Epoch [81/250], Training Loss: 0.0979\n",
      "Validation Loss: 0.0945\n",
      "Epoch [82/250], Training Loss: 0.0976\n",
      "Validation Loss: 0.0941\n",
      "Epoch [83/250], Training Loss: 0.0972\n",
      "Validation Loss: 0.0937\n",
      "Epoch [84/250], Training Loss: 0.0968\n",
      "Validation Loss: 0.0933\n",
      "Epoch [85/250], Training Loss: 0.0964\n",
      "Validation Loss: 0.0929\n",
      "Epoch [86/250], Training Loss: 0.0960\n",
      "Validation Loss: 0.0926\n",
      "Epoch [87/250], Training Loss: 0.0957\n",
      "Validation Loss: 0.0922\n",
      "Epoch [88/250], Training Loss: 0.0953\n",
      "Validation Loss: 0.0919\n",
      "Epoch [89/250], Training Loss: 0.0950\n",
      "Validation Loss: 0.0915\n",
      "Epoch [90/250], Training Loss: 0.0947\n",
      "Validation Loss: 0.0912\n",
      "Epoch [91/250], Training Loss: 0.0944\n",
      "Validation Loss: 0.0909\n",
      "Epoch [92/250], Training Loss: 0.0941\n",
      "Validation Loss: 0.0906\n",
      "Epoch [93/250], Training Loss: 0.0938\n",
      "Validation Loss: 0.0904\n",
      "Epoch [94/250], Training Loss: 0.0936\n",
      "Validation Loss: 0.0901\n",
      "Epoch [95/250], Training Loss: 0.0933\n",
      "Validation Loss: 0.0899\n",
      "Epoch [96/250], Training Loss: 0.0931\n",
      "Validation Loss: 0.0897\n",
      "Epoch [97/250], Training Loss: 0.0929\n",
      "Validation Loss: 0.0894\n",
      "Epoch [98/250], Training Loss: 0.0927\n",
      "Validation Loss: 0.0892\n",
      "Epoch [99/250], Training Loss: 0.0925\n",
      "Validation Loss: 0.0890\n",
      "Epoch [100/250], Training Loss: 0.0923\n",
      "Validation Loss: 0.0889\n",
      "Epoch [101/250], Training Loss: 0.0922\n",
      "Validation Loss: 0.0887\n",
      "Epoch [102/250], Training Loss: 0.0920\n",
      "Validation Loss: 0.0885\n",
      "Epoch [103/250], Training Loss: 0.0918\n",
      "Validation Loss: 0.0884\n",
      "Epoch [104/250], Training Loss: 0.0917\n",
      "Validation Loss: 0.0882\n",
      "Epoch [105/250], Training Loss: 0.0916\n",
      "Validation Loss: 0.0881\n",
      "Epoch [106/250], Training Loss: 0.0914\n",
      "Validation Loss: 0.0880\n",
      "Epoch [107/250], Training Loss: 0.0913\n",
      "Validation Loss: 0.0879\n",
      "Epoch [108/250], Training Loss: 0.0912\n",
      "Validation Loss: 0.0877\n",
      "Epoch [109/250], Training Loss: 0.0911\n",
      "Validation Loss: 0.0876\n",
      "Epoch [110/250], Training Loss: 0.0910\n",
      "Validation Loss: 0.0875\n",
      "Epoch [111/250], Training Loss: 0.0909\n",
      "Validation Loss: 0.0874\n",
      "Epoch [112/250], Training Loss: 0.0908\n",
      "Validation Loss: 0.0873\n",
      "Epoch [113/250], Training Loss: 0.0907\n",
      "Validation Loss: 0.0873\n",
      "Epoch [114/250], Training Loss: 0.0907\n",
      "Validation Loss: 0.0872\n",
      "Epoch [115/250], Training Loss: 0.0906\n",
      "Validation Loss: 0.0871\n",
      "Epoch [116/250], Training Loss: 0.0905\n",
      "Validation Loss: 0.0870\n",
      "Epoch [117/250], Training Loss: 0.0904\n",
      "Validation Loss: 0.0870\n",
      "Epoch [118/250], Training Loss: 0.0904\n",
      "Validation Loss: 0.0869\n",
      "Epoch [119/250], Training Loss: 0.0903\n",
      "Validation Loss: 0.0868\n",
      "Epoch [120/250], Training Loss: 0.0903\n",
      "Validation Loss: 0.0868\n",
      "Epoch [121/250], Training Loss: 0.0902\n",
      "Validation Loss: 0.0867\n",
      "Epoch [122/250], Training Loss: 0.0902\n",
      "Validation Loss: 0.0867\n",
      "Epoch [123/250], Training Loss: 0.0901\n",
      "Validation Loss: 0.0866\n",
      "Epoch [124/250], Training Loss: 0.0901\n",
      "Validation Loss: 0.0866\n",
      "Epoch [125/250], Training Loss: 0.0900\n",
      "Validation Loss: 0.0865\n",
      "Epoch [126/250], Training Loss: 0.0900\n",
      "Validation Loss: 0.0865\n",
      "Epoch [127/250], Training Loss: 0.0899\n",
      "Validation Loss: 0.0864\n",
      "Epoch [128/250], Training Loss: 0.0899\n",
      "Validation Loss: 0.0864\n",
      "Epoch [129/250], Training Loss: 0.0899\n",
      "Validation Loss: 0.0864\n",
      "Epoch [130/250], Training Loss: 0.0898\n",
      "Validation Loss: 0.0863\n",
      "Epoch [131/250], Training Loss: 0.0898\n",
      "Validation Loss: 0.0863\n",
      "Epoch [132/250], Training Loss: 0.0898\n",
      "Validation Loss: 0.0863\n",
      "Epoch [133/250], Training Loss: 0.0898\n",
      "Validation Loss: 0.0862\n",
      "Epoch [134/250], Training Loss: 0.0897\n",
      "Validation Loss: 0.0862\n",
      "Epoch [135/250], Training Loss: 0.0897\n",
      "Validation Loss: 0.0862\n",
      "Epoch [136/250], Training Loss: 0.0897\n",
      "Validation Loss: 0.0861\n",
      "Epoch [137/250], Training Loss: 0.0897\n",
      "Validation Loss: 0.0861\n",
      "Epoch [138/250], Training Loss: 0.0896\n",
      "Validation Loss: 0.0861\n",
      "Epoch [139/250], Training Loss: 0.0896\n",
      "Validation Loss: 0.0861\n",
      "Epoch [140/250], Training Loss: 0.0896\n",
      "Validation Loss: 0.0861\n",
      "Epoch [141/250], Training Loss: 0.0896\n",
      "Validation Loss: 0.0860\n",
      "Epoch [142/250], Training Loss: 0.0896\n",
      "Validation Loss: 0.0860\n",
      "Epoch [143/250], Training Loss: 0.0895\n",
      "Validation Loss: 0.0860\n",
      "Epoch [144/250], Training Loss: 0.0895\n",
      "Validation Loss: 0.0860\n",
      "Epoch [145/250], Training Loss: 0.0895\n",
      "Validation Loss: 0.0860\n",
      "Epoch [146/250], Training Loss: 0.0895\n",
      "Validation Loss: 0.0859\n",
      "Epoch [147/250], Training Loss: 0.0895\n",
      "Validation Loss: 0.0859\n",
      "Epoch [148/250], Training Loss: 0.0895\n",
      "Validation Loss: 0.0859\n",
      "Epoch [149/250], Training Loss: 0.0894\n",
      "Validation Loss: 0.0859\n",
      "Epoch [150/250], Training Loss: 0.0894\n",
      "Validation Loss: 0.0859\n",
      "Epoch [151/250], Training Loss: 0.0894\n",
      "Validation Loss: 0.0859\n",
      "Epoch [152/250], Training Loss: 0.0894\n",
      "Validation Loss: 0.0859\n",
      "Epoch [153/250], Training Loss: 0.0894\n",
      "Validation Loss: 0.0858\n",
      "Epoch [154/250], Training Loss: 0.0894\n",
      "Validation Loss: 0.0858\n",
      "Epoch [155/250], Training Loss: 0.0894\n",
      "Validation Loss: 0.0858\n",
      "Epoch [156/250], Training Loss: 0.0894\n",
      "Validation Loss: 0.0858\n",
      "Epoch [157/250], Training Loss: 0.0894\n",
      "Validation Loss: 0.0858\n",
      "Epoch [158/250], Training Loss: 0.0893\n",
      "Validation Loss: 0.0858\n",
      "Epoch [159/250], Training Loss: 0.0893\n",
      "Validation Loss: 0.0858\n",
      "Epoch [160/250], Training Loss: 0.0893\n",
      "Validation Loss: 0.0858\n",
      "Epoch [161/250], Training Loss: 0.0893\n",
      "Validation Loss: 0.0858\n",
      "Epoch [162/250], Training Loss: 0.0893\n",
      "Validation Loss: 0.0858\n",
      "Epoch [163/250], Training Loss: 0.0893\n",
      "Validation Loss: 0.0857\n",
      "Epoch [164/250], Training Loss: 0.0893\n",
      "Validation Loss: 0.0857\n",
      "Epoch [165/250], Training Loss: 0.0893\n",
      "Validation Loss: 0.0857\n",
      "Epoch [166/250], Training Loss: 0.0893\n",
      "Validation Loss: 0.0857\n",
      "Epoch [167/250], Training Loss: 0.0893\n",
      "Validation Loss: 0.0857\n",
      "Epoch [168/250], Training Loss: 0.0893\n",
      "Validation Loss: 0.0857\n",
      "Epoch [169/250], Training Loss: 0.0893\n",
      "Validation Loss: 0.0857\n",
      "Epoch [170/250], Training Loss: 0.0893\n",
      "Validation Loss: 0.0857\n",
      "Epoch [171/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0857\n",
      "Epoch [172/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0857\n",
      "Epoch [173/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0857\n",
      "Epoch [174/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0857\n",
      "Epoch [175/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0857\n",
      "Epoch [176/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0857\n",
      "Epoch [177/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0856\n",
      "Epoch [178/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0856\n",
      "Epoch [179/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0856\n",
      "Epoch [180/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0856\n",
      "Epoch [181/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0856\n",
      "Epoch [182/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0856\n",
      "Epoch [183/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0856\n",
      "Epoch [184/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0856\n",
      "Epoch [185/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0856\n",
      "Epoch [186/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0856\n",
      "Epoch [187/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0856\n",
      "Epoch [188/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0856\n",
      "Epoch [189/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0856\n",
      "Epoch [190/250], Training Loss: 0.0892\n",
      "Validation Loss: 0.0856\n",
      "Epoch [191/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0856\n",
      "Epoch [192/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0856\n",
      "Epoch [193/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0856\n",
      "Epoch [194/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0856\n",
      "Epoch [195/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0856\n",
      "Epoch [196/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0856\n",
      "Epoch [197/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0856\n",
      "Epoch [198/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0856\n",
      "Epoch [199/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [200/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [201/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [202/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [203/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [204/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [205/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [206/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [207/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [208/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [209/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [210/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [211/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [212/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [213/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [214/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [215/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [216/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [217/250], Training Loss: 0.0891\n",
      "Validation Loss: 0.0855\n",
      "Epoch [218/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0855\n",
      "Epoch [219/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0855\n",
      "Epoch [220/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0855\n",
      "Epoch [221/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0855\n",
      "Epoch [222/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0855\n",
      "Epoch [223/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0855\n",
      "Epoch [224/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0855\n",
      "Epoch [225/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0855\n",
      "Epoch [226/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0855\n",
      "Epoch [227/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0855\n",
      "Epoch [228/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0855\n",
      "Epoch [229/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [230/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [231/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [232/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [233/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [234/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [235/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [236/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [237/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [238/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [239/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [240/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [241/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [242/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [243/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [244/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [245/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [246/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [247/250], Training Loss: 0.0890\n",
      "Validation Loss: 0.0854\n",
      "Epoch [248/250], Training Loss: 0.0889\n",
      "Validation Loss: 0.0854\n",
      "Epoch [249/250], Training Loss: 0.0889\n",
      "Validation Loss: 0.0854\n",
      "Epoch [250/250], Training Loss: 0.0889\n",
      "Validation Loss: 0.0854\n"
     ]
    }
   ],
   "source": [
    "#hiperparàmetres\n",
    "lr = 0.001\n",
    "EPOCHS = 250\n",
    "\n",
    "input_dim = x_train.shape[1]\n",
    "model = Autoencoder(input_dim)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "# training_loss, validation_loss = train_ae(autoencoder=model, data_loader=train_loader, criterion=criterion, \n",
    "#                                           optimizer=optimizer, val_loader=val_loader, num_epochs=EPOCHS, learning_rate=lr)\n",
    "# torch.save(model.state_dict(), 'C:/Users/gsamp/OneDrive/Documents/AI-3/2n Semestre/Projecte de Síntesi 2/GraphAnomaly/GNN_models/trained_models/ae_new_data.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error de recunstrucció"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reconstruction Error for Licit Class: 0.08537691583236058\n",
      "Average Reconstruction Error for Illicit Class: 0.06231612861156464\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the saved model\n",
    "model_path = \"C:/Users/gsamp/OneDrive/Documents/AI-3/2n Semestre/Projecte de Síntesi 2/GraphAnomaly/GNN_models/trained_models/ae_new_data.pth\"\n",
    "\n",
    "# Create an instance of your model\n",
    "model = Autoencoder(input_dim)\n",
    "\n",
    "# Load the state_dict into the model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize variables for storing reconstruction error\n",
    "\n",
    "\n",
    "ilicit_data = torch.tensor(df_test.loc[df_test[\"class\"] == 0].drop(columns=['class', 'txId', 'Time Step']).values)\n",
    "\n",
    "ilicit_data = preprocessing.MinMaxScaler().fit_transform(ilicit_data)\n",
    "\n",
    "\n",
    "ilicit_dataset = CustomDataset(ilicit_data)\n",
    "ilicit_loader = DataLoader(ilicit_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Assuming you have already defined the necessary modules and datasets\n",
    "\n",
    "# Initialize variables for storing reconstruction error\n",
    "total_licit_loss = 0\n",
    "total_ilicit_loss = 0\n",
    "num_batches = 0\n",
    "\n",
    "licit_loss_history = []\n",
    "ilicit_loss_history = []\n",
    "\n",
    "# Iterate through the validation dataset\n",
    "with torch.no_grad():\n",
    "    for seen, unseen in zip(val_loader, ilicit_loader):\n",
    "        \n",
    "        seen_inputs, _ = seen\n",
    "        unseen_inputs, _ = unseen\n",
    "        \n",
    "        seen_outputs = model(seen_inputs)\n",
    "        unseen_outputs = model(unseen_inputs)\n",
    "        \n",
    "        seen_loss = criterion(seen_outputs, seen_inputs)  # Compute reconstruction loss\n",
    "        unseen_loss = criterion(unseen_outputs, unseen_inputs)\n",
    "        \n",
    "        licit_loss_history.append(float(seen_loss))\n",
    "        ilicit_loss_history.append(float(unseen_loss))\n",
    "        \n",
    "        total_licit_loss += seen_loss.item()\n",
    "        total_ilicit_loss += unseen_loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "# Compute average reconstruction error for both seen and unseen classes\n",
    "average_licit_loss = total_licit_loss / num_batches\n",
    "average_ilicit_loss = total_ilicit_loss / num_batches\n",
    "\n",
    "print(\"Average Reconstruction Error for Licit Class:\", average_licit_loss)\n",
    "print(\"Average Reconstruction Error for Illicit Class:\", average_ilicit_loss)\n",
    "\n",
    "# You can return ilicit_loss_history if you want to analyze it further\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAHHCAYAAACRAnNyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABBQklEQVR4nO3deVRV9f7/8deRGUEQFXFAQJzHnHMKK8v5OlSaaYKSWlmmZgO3wakblmk2qnVL7GuDmqXeBuchc9bSMs2BHAu1MkE0UOHz+6PF+XkCTY/A3uDzsdZZy7335+z93h+OmxefPRyHMcYIAADAhkpYXQAAAMClEFQAAIBtEVQAAIBtEVQAAIBtEVQAAIBtEVQAAIBtEVQAAIBtEVQAAIBtEVQAAIBtEVQA2Mrq1avlcDi0evVqq0spdsaOHSuHw2F1GcBVIagAkpKSkuRwOJwvT09PVapUSXFxcfr555+tLi/fvfnmm0pKSrrua/i7du3auXwOLn7VqlXL6vIsZcefF64PnlYXANjJ+PHjFRUVpYyMDG3cuFFJSUn6+uuvtXPnTvn6+lpdXr558803VbZsWcXFxdmuhptuukl//vmnvL29LamrcuXKSkxMzDU/KCjIgmrsww6fGVyfCCrARTp16qSmTZtKku677z6VLVtWL7zwghYtWqTevXtbXJ01zpw5o5IlSxba9kqUKGFpKAwKClL//v2v+n2X6idjjDIyMuTn5+d2TRkZGfL29laJEgyC4/rDpx64jLZt20qSkpOTXeb/+OOPuvPOOxUSEiJfX181bdpUixYtyvX+U6dOaeTIkYqMjJSPj48qV66sAQMG6LfffnO2OXHihOLj41W+fHn5+vqqYcOGmjVrlst6Dh48KIfDoZdeeklvvfWWoqOj5ePjo2bNmmnLli0ubY8dO6aBAweqcuXK8vHxUYUKFdS9e3cdPHhQkhQZGakffvhBa9ascZ7WaNeunaT/fwpszZo1evDBBxUaGqrKlStLkuLi4hQZGZlrHy913cPs2bPVvHlz+fv7q3Tp0rrpppu0dOnSf6zhUteozJs3T02aNJGfn5/Kli2r/v375zotFxcXp4CAAP3888/q0aOHAgICVK5cOY0ePVpZWVm5anRXzj7v2rVL99xzj0qXLq02bdo4961r165asmSJmjZtKj8/P82YMUOS9NNPP+muu+5SSEiI/P39deONN+rzzz93WXfO/n/00Ud6+umnValSJfn7+ystLS3PWi7+bLz88suKiIiQn5+fYmJitHPnzn/clwsXLmjChAnOz1RkZKT+/e9/KzMz09nmcj8voKAxogJcRs4v99KlSzvn/fDDD2rdurUqVaqkJ598UiVLltTcuXPVo0cPzZ8/Xz179pQkpaenq23bttq9e7cGDRqkxo0b67ffftOiRYt09OhRlS1bVn/++afatWun/fv366GHHlJUVJTmzZunuLg4nTp1So888ohLPR988IFOnz6toUOHyuFw6MUXX1SvXr30008/ycvLS5J0xx136IcfftDDDz+syMhInThxQsuWLdPhw4cVGRmpqVOn6uGHH1ZAQICeeuopSVL58uVdtvPggw+qXLlyevbZZ3XmzJmr7rdx48Zp7NixatWqlcaPHy9vb29t2rRJK1eu1O23335FNVwsKSlJAwcOVLNmzZSYmKjjx4/rlVde0bp16/Ttt98qODjY2TYrK0sdOnRQixYt9NJLL2n58uWaPHmyoqOj9cADD/xj7VlZWS5BMoefn1+uEZO77rpL1atX1/PPPy9jjHP+nj171LdvXw0dOlSDBw9WzZo1dfz4cbVq1Upnz57V8OHDVaZMGc2aNUv/+te/9PHHHzs/NzkmTJggb29vjR49WpmZmf94Kuy9997T6dOnNWzYMGVkZOiVV17RLbfcou+///6yfXvfffdp1qxZuvPOO/Xoo49q06ZNSkxM1O7du/Xpp59K0lX/vIB8ZQCYmTNnGklm+fLl5tdffzVHjhwxH3/8sSlXrpzx8fExR44ccba99dZbTf369U1GRoZzXnZ2tmnVqpWpXr26c96zzz5rJJlPPvkk1/ays7ONMcZMnTrVSDKzZ892Ljt37pxp2bKlCQgIMGlpacYYYw4cOGAkmTJlypiTJ0862y5cuNBIMv/73/+MMcb88ccfRpKZNGnSZfe3bt26JiYm5pL90KZNG3PhwgWXZbGxsSYiIiLXe8aMGWMuPpTs27fPlChRwvTs2dNkZWXlud+Xq2HVqlVGklm1apWzP0JDQ029evXMn3/+6Wz32WefGUnm2WefdalRkhk/frzLOhs1amSaNGmSa1t/FxMTYyTl+Ro6dGiufe7bt2+udURERBhJZvHixS7zR4wYYSSZtWvXOuedPn3aREVFmcjISGdf5ex/1apVzdmzZ/+x5pzPhp+fnzl69Khz/qZNm4wkM3LkyFx159i+fbuRZO677z6XdY4ePdpIMitXrnTOu9TPCyhonPoBLtK+fXuVK1dO4eHhuvPOO1WyZEktWrTIefrj5MmTWrlypXr37q3Tp0/rt99+02+//abff/9dHTp00L59+5ynI+bPn6+GDRvm+ktZkvNUyRdffKGwsDD17dvXuczLy0vDhw9Xenq61qxZ4/K+Pn36uIzu5Jya+umnnyT99Ve/t7e3Vq9erT/++MPtfhg8eLA8PDzceu+CBQuUnZ2tZ599Ntc1Fe7cGrt161adOHFCDz74oMu1K126dFGtWrVynTqRpPvvv99lum3bts4++ieRkZFatmxZrteIESP+cTs5oqKi1KFDB5d5X3zxhZo3b+48RSRJAQEBGjJkiA4ePKhdu3a5tI+Njb2q61p69OihSpUqOaebN2+uFi1a6Isvvrjke3KWjRo1ymX+o48+Kkl59i1Q2Dj1A1zkjTfeUI0aNZSamqp3331XX331lXx8fJzL9+/fL2OMnnnmGT3zzDN5ruPEiROqVKmSkpOTdccdd1x2e4cOHVL16tVz/UKvXbu2c/nFqlSp4jKdE1pyQomPj49eeOEFPfrooypfvrxuvPFGde3aVQMGDFBYWNgV9MBfoqKirrjt3yUnJ6tEiRKqU6eO2+u4WE4f1KxZM9eyWrVq6euvv3aZ5+vrq3LlyrnMK1269BUHt5IlS6p9+/ZX1PZS/ZTX/EOHDqlFixa55l/8s65Xr94/rvtSqlevnmtejRo1NHfu3Eu+59ChQypRooSqVavmMj8sLEzBwcG5Pn+AFQgqwEWaN2/uvOunR48eatOmje655x7t2bNHAQEBys7OliSNHj0611/MOf5+0M9PlxrlMBddHzFixAh169ZNCxYs0JIlS/TMM88oMTFRK1euVKNGja5oO3n9JX+p0ZD8vEg1P7g7EuSOS414XMsdPvm5jivFQ+BgZ5z6AS7Bw8NDiYmJ+uWXX/T6669LkqpWrSrpr9Mz7du3z/MVGBgoSYqOjv7Huy4iIiK0b98+ZwDK8eOPPzqXuyM6OlqPPvqoli5dqp07d+rcuXOaPHmyc7k7v5hKly6tU6dO5Zr/97+6o6OjlZ2dnetUxt9daQ05fbBnz55cy/bs2eN2HxW2iIiIPPfhWn/WOfbt25dr3t69e/O8U+vimrKzs3O99/jx4zp16pRLTYQZWIWgAlxGu3bt1Lx5c02dOlUZGRkKDQ1Vu3btNGPGDKWkpORq/+uvvzr/fccdd2jHjh3OOyculjMC0rlzZx07dkxz5sxxLrtw4YJee+01BQQEKCYm5qrqPXv2rDIyMlzmRUdHKzAw0OV205IlS+YZOi4nOjpaqamp+u6775zzUlJScu1fjx49VKJECY0fPz5XALt45OdKa2jatKlCQ0M1ffp0l3348ssvtXv3bnXp0uWq9sMqnTt31ubNm7VhwwbnvDNnzuitt95SZGTkNZ8qW7Bggcvt2ps3b9amTZvUqVOny9Yk/XVXz8WmTJkiSS59685nBsgPnPoB/sFjjz2mu+66S0lJSbr//vv1xhtvqE2bNqpfv74GDx6sqlWr6vjx49qwYYOOHj2qHTt2ON/38ccf66677tKgQYPUpEkTnTx5UosWLdL06dPVsGFDDRkyRDNmzFBcXJy2bdumyMhIffzxx1q3bp2mTp3qHJ25Unv37tWtt96q3r17q06dOvL09NSnn36q48eP6+6773a2a9KkiaZNm6bnnntO1apVU2hoqG655ZbLrvvuu+/WE088oZ49e2r48OE6e/aspk2bpho1auibb75xtqtWrZqeeuopTZgwQW3btlWvXr3k4+OjLVu2qGLFis6nvl5pDV5eXnrhhRc0cOBAxcTEqG/fvs7bkyMjIzVy5Mir6qN/kpqaqtmzZ+e5zJ0HweV48skn9eGHH6pTp04aPny4QkJCNGvWLB04cEDz58+/5oe5VatWTW3atNEDDzygzMxMTZ06VWXKlNHjjz9+yfc0bNhQsbGxeuutt3Tq1CnFxMRo8+bNmjVrlnr06KGbb77Z2dadzwyQL6y96Qiwh5zbcrds2ZJrWVZWlomOjjbR0dHOW3aTk5PNgAEDTFhYmPHy8jKVKlUyXbt2NR9//LHLe3///Xfz0EMPmUqVKhlvb29TuXJlExsba3777Tdnm+PHj5uBAweasmXLGm9vb1O/fn0zc+ZMl/Xk3IKa123HksyYMWOMMcb89ttvZtiwYaZWrVqmZMmSJigoyLRo0cLMnTvX5T3Hjh0zXbp0MYGBgUaS87bTy/WDMcYsXbrU1KtXz3h7e5uaNWua2bNn57rlNce7775rGjVqZHx8fEzp0qVNTEyMWbZs2T/W8Pfbk3PMmTPHub6QkBDTr18/l9txjfnr9uSSJUvmquVSNf7d5W5Pvvj9Oev79ddfc60jIiLCdOnSJc/1JycnmzvvvNMEBwcbX19f07x5c/PZZ5+5tMnZ/3nz5v1jvca4fjYmT55swsPDjY+Pj2nbtq3ZsWOHS9u8+uH8+fNm3LhxJioqynh5eZnw8HCTkJDgcvu9MZf+eQEFzWHMRWOxAIAi5eDBg4qKitKkSZM0evRoq8sB8h3XqAAAANsiqAAAANsiqAAAANviGhUAAGBbjKgAAADbIqgAAADbKtIPfMvOztYvv/yiwMBAHu8MAEARYYzR6dOnVbFixX982GGRDiq//PKLwsPDrS4DAAC44ciRI6pcufJl2xTpoJLzePEjR46oVKlSFlcDAACuRFpamsLDw6/oa0KKdFDJOd1TqlQpggoAAEXMlVy2wcW0AADAtggqAADAtggqAADAtor0NSpXKisrS+fPn7e6DBQgLy8veXh4WF0GACCfFeugYozRsWPHdOrUKatLQSEIDg5WWFgYz9QBgGKkWAeVnJASGhoqf39/foEVU8YYnT17VidOnJAkVahQweKKAAD5pdgGlaysLGdIKVOmjNXloID5+flJkk6cOKHQ0FBOAwFAMVFsL6bNuSbF39/f4kpQWHJ+1lyPBADFR7ENKjk43XP94GcNAMVPsQ8qAACg6LI0qGRlZemZZ55RVFSU/Pz8FB0drQkTJsgYY2VZtudwOLRgwYIrajt27FjdcMMNBVrP1bBbPQAAe7P0YtoXXnhB06ZN06xZs1S3bl1t3bpVAwcOVFBQkIYPH15g23152d4CW/ffjbytxlW/Jy4uTqdOnbpkGElJSVHp0qWvaF2jR4/Www8/fMXrBgDATiwNKuvXr1f37t3VpUsXSVJkZKQ+/PBDbd682cqybC8sLOyK2wYEBCggIKAAqwEAoOBYeuqnVatWWrFihfbu/WuEY8eOHfr666/VqVMnK8uyvb+f+jl69Kj69u2rkJAQlSxZUk2bNtWmTZskuZ5qGTt2rGbNmqWFCxfK4XDI4XBo9erVeW6jXbt2Gj58uB5//HGFhIQoLCxMY8eOdWlz+PBhde/eXQEBASpVqpR69+6t48ePu7SZOHGiypcvr8DAQMXHxysjIyPXtv773/+qdu3a8vX1Va1atfTmm286l507d04PPfSQKlSoIF9fX0VERCgxMfHqOw0AUCRZOqLy5JNPKi0tTbVq1ZKHh4eysrL0n//8R/369cuzfWZmpjIzM53TaWlphVWqbaWnpysmJkaVKlXSokWLFBYWpm+++UbZ2dm52o4ePVq7d+9WWlqaZs6cKUkKCQm55LpnzZqlUaNGadOmTdqwYYPi4uLUunVr3XbbbcrOznaGlDVr1ujChQsaNmyY+vTp4ww/c+fO1dixY/XGG2+oTZs2+r//+z+9+uqrqlq1qnMb77//vp599lm9/vrratSokb799lsNHjxYJUuWVGxsrF599VUtWrRIc+fOVZUqVXTkyBEdOXIkfzsRAGBblgaVuXPn6v3339cHH3ygunXravv27RoxYoQqVqyo2NjYXO0TExM1btw4Cyq1rw8++EC//vqrtmzZ4gwd1apVy7NtQECA/Pz8lJmZeUWnjxo0aKAxY8ZIkqpXr67XX39dK1as0G233aYVK1bo+++/14EDBxQeHi5Jeu+991S3bl1t2bJFzZo109SpUxUfH6/4+HhJ0nPPPafly5e7jKqMGTNGkydPVq9evSRJUVFR2rVrl2bMmKHY2FgdPnxY1atXV5s2beRwOBQREeF+ZwFAEVaY11dezJ1rLfOTpad+HnvsMT355JO6++67Vb9+fd17770aOXLkJYf2ExISlJqa6nzxl7W0fft2NWrU6LIjI+5q0KCBy3SFChWcj6nfvXu3wsPDnSFFkurUqaPg4GDt3r3b2aZFixYu62jZsqXz32fOnFFycrLi4+Od19IEBAToueeeU3JysqS/Lv7dvn27atasqeHDh2vp0qX5vp8AAPuydETl7NmzKlHCNSt5eHjkedpCknx8fOTj41MYpRUZOY+OLwheXl4u0w6H45I/G3ekp6dLkt5+++1cgSbnEfiNGzfWgQMH9OWXX2r58uXq3bu32rdvr48//jjf6gAA2JelIyrdunXTf/7zH33++ec6ePCgPv30U02ZMkU9e/a0sqwipUGDBtq+fbtOnjx5Re29vb2VlZV1zdutXbt2rutFdu3apVOnTqlOnTrONjkX9ebYuHGj89/ly5dXxYoV9dNPP6latWour6ioKGe7UqVKqU+fPnr77bc1Z84czZ8//4r3FwBQtFk6ovLaa6/pmWee0YMPPqgTJ06oYsWKGjp0qJ599lkry7KF1NRUbd++3WVemTJlXE61SFLfvn31/PPPq0ePHkpMTFSFChX07bffqmLFii6nWXJERkZqyZIl2rNnj8qUKaOgoKBcIydXon379qpfv7769eunqVOn6sKFC3rwwQcVExOjpk2bSpIeeeQRxcXFqWnTpmrdurXef/99/fDDDy4X044bN07Dhw9XUFCQOnbsqMzMTG3dulV//PGHRo0apSlTpqhChQpq1KiRSpQooXnz5iksLEzBwcFXXTMAoOixdEQlMDBQU6dO1aFDh/Tnn38qOTlZzz33nLy9va0syxZWr16tRo0aubzyupDY29tbS5cuVWhoqDp37qz69etr4sSJl/z24MGDB6tmzZpq2rSpypUrp3Xr1rlVn8Ph0MKFC1W6dGnddNNNat++vapWrao5c+Y42/Tp00fPPPOMHn/8cTVp0kSHDh3SAw884LKe++67T//97381c+ZM1a9fXzExMUpKSnKOqAQGBurFF19U06ZN1axZMx08eFBffPFFrlOGAIDiyWGK8PPq09LSFBQUpNTUVJUqVcplWUZGhg4cOKCoqCj5+vpaVCEKEz9zAMVZcbrr53K/v/+OP0sBAIBtEVQAAIBtEVQAAIBtEVQAAIBtEVQAAIBtEVQAAIBtEVQAAIBtEVQAAIBtEVQAAIBtEVSKgHbt2mnEiBHO6cjISE2dOvWK3puUlGSr78WxWz0AAHuz9EsJLbMqsfC2dXPCVb8lLi5Op06d0oIFC/JcvmXLFpUsWfKK1tWnTx917tzZOT127FgtWLAg1xceAgBgR9dnUCniypUrd8Vt/fz85OfnV4DVAABQcDj1UwT9/dTPqVOnNHToUJUvX16+vr6qV6+ePvvsM0mup1qSkpI0btw47dixQw6HQw6HQ0lJSXluIy4uTj169NBLL72kChUqqEyZMho2bJjOnz/vbPPHH39owIABKl26tPz9/dWpUyft27fPZT1JSUmqUqWK/P391bNnT/3++++5trVw4UI1btxYvr6+qlq1qsaNG6cLFy5IkowxGjt2rKpUqSIfHx9VrFhRw4cPv4beAwAUJYyoFHHZ2dnq1KmTTp8+rdmzZys6Olq7du2Sh4dHrrZ9+vTRzp07tXjxYi1fvlySFBQUdMl1r1q1ShUqVNCqVau0f/9+9enTRzfccIMGDx4s6a8ws2/fPi1atEilSpXSE088oc6dO2vXrl3y8vLSpk2bFB8fr8TERPXo0UOLFy/WmDFjXLaxdu1aDRgwQK+++qratm2r5ORkDRkyRJI0ZswYzZ8/Xy+//LI++ugj1a1bV8eOHdOOHTvyq/sAADZHUCnili9frs2bN2v37t2qUeOvr+KuWrVqnm39/PwUEBAgT09PhYWF/eO6S5curddff10eHh6qVauWunTpohUrVmjw4MHOgLJu3Tq1atVKkvT+++8rPDxcCxYs0F133aVXXnlFHTt21OOPPy5JqlGjhtavX6/Fixc7tzFu3Dg9+eSTio2NddY+YcIEPf744xozZowOHz6ssLAwtW/fXl5eXqpSpYqaN29+TX0GACg6OPVTxG3fvl2VK1d2hpT8VLduXZeRmQoVKujEiROSpN27d8vT01MtWrRwLi9Tpoxq1qyp3bt3O9tcvFySWrZs6TK9Y8cOjR8/XgEBAc7X4MGDlZKSorNnz+quu+7Sn3/+qapVq2rw4MH69NNPnaeFAADFHyMqRVxBXijr5eXlMu1wOJSdnZ2v20hPT9e4cePUq1evXMt8fX0VHh6uPXv2aPny5Vq2bJkefPBBTZo0SWvWrMlVHwCg+CGoFHENGjTQ0aNHtXfv3isaVfH29lZWVtY1b7d27dq6cOGCNm3a5Dz18/vvv2vPnj2qU6eOs82mTZtc3rdx40aX6caNG2vPnj2qVq3aJbfl5+enbt26qVu3bho2bJhq1aql77//Xo0bN77m/QAA2BtBpYiLiYnRTTfdpDvuuENTpkxRtWrV9OOPP8rhcKhjx4652kdGRurAgQPOU0aBgYHy8fG56u1Wr15d3bt31+DBgzVjxgwFBgbqySefVKVKldS9e3dJ0vDhw9W6dWu99NJL6t69u5YsWeJyfYokPfvss+ratauqVKmiO++8UyVKlNCOHTu0c+dOPffcc0pKSlJWVpZatGghf39/zZ49W35+foqIiHCvwwAARQrXqBQD8+fPV7NmzdS3b1/VqVNHjz/++CVHTe644w517NhRN998s8qVK6cPP/zQ7e3OnDlTTZo0UdeuXdWyZUsZY/TFF184T8nceOONevvtt/XKK6+oYcOGWrp0qZ5++mmXdXTo0EGfffaZli5dqmbNmunGG2/Uyy+/7AwiwcHBevvtt9W6dWs1aNBAy5cv1//+9z+VKVPG7boBAEWHwxhjrC7CXWlpaQoKClJqaqpKlSrlsiwjI0MHDhxQVFSUfH19LaoQhYmfOYDi7OVley3Z7sjb8v9mjcv9/v47RlQAAIBtEVQAAIBtEVQAAIBtEVQAAIBtFfugUoSvFcZV4mcNAMVPsQ0qObfInj171uJKUFhyftY8sRYAio9i+8A3Dw8PBQcHO7+bxt/fXw6Hw+KqUBCMMTp79qxOnDih4ODgPL85GgBQNBXboCLJ+Q3BOWEFxVtwcPAVfSs0AKDoKNZBxeFwqEKFCgoNDdX58+etLgcFyMvLi5EUACiGinVQyeHh4cEvMQAAiqBiezEtAAAo+iwNKpGRkXI4HLlew4YNs7IsAABgE5ae+tmyZYvLt/zu3LlTt912m+666y4LqwIAAHZhaVApV66cy/TEiRMVHR2tmJgYiyoCAAB2YptrVM6dO6fZs2dr0KBBPO8EAABIstFdPwsWLNCpU6cUFxd3yTaZmZnKzMx0TqelpRVCZQAAwCq2GVF555131KlTJ1WsWPGSbRITExUUFOR8hYeHF2KFAACgsNkiqBw6dEjLly/Xfffdd9l2CQkJSk1Ndb6OHDlSSBUCAAAr2OLUz8yZMxUaGqouXbpctp2Pj498fHwKqSoAAGA1y0dUsrOzNXPmTMXGxsrT0xa5CQAA2ITlQWX58uU6fPiwBg0aZHUpAADAZiwfwrj99ttljLG6DAAAYEOWj6gAAABcCkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAADYluVB5eeff1b//v1VpkwZ+fn5qX79+tq6davVZQEAABvwtHLjf/zxh1q3bq2bb75ZX375pcqVK6d9+/apdOnSVpYFAABswtKg8sILLyg8PFwzZ850zouKirKwIgAAYCeWnvpZtGiRmjZtqrvuukuhoaFq1KiR3n77bStLAgAANmJpUPnpp580bdo0Va9eXUuWLNEDDzyg4cOHa9asWXm2z8zMVFpamssLAAAUX5ae+snOzlbTpk31/PPPS5IaNWqknTt3avr06YqNjc3VPjExUePGjSvsMgEAkCS9vGyv1SVcdywdUalQoYLq1KnjMq927do6fPhwnu0TEhKUmprqfB05cqQwygQAABaxdESldevW2rNnj8u8vXv3KiIiIs/2Pj4+8vHxKYzSAACADVg6ojJy5Eht3LhRzz//vPbv368PPvhAb731loYNG2ZlWQAAwCYsDSrNmjXTp59+qg8//FD16tXThAkTNHXqVPXr18/KsgAAgE1YeupHkrp27aquXbtaXQYAALAhyx+hDwAAcCkEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFuWBpWxY8fK4XC4vGrVqmVlSQAAwEY8rS6gbt26Wr58uXPa09PykgAAgE1Yngo8PT0VFhZmdRkAAMCGLL9GZd++fapYsaKqVq2qfv366fDhw1aXBAAAbMLSEZUWLVooKSlJNWvWVEpKisaNG6e2bdtq586dCgwMzNU+MzNTmZmZzum0tLTCLBcAABQyS4NKp06dnP9u0KCBWrRooYiICM2dO1fx8fG52icmJmrcuHGFWSIAALCQ5ad+LhYcHKwaNWpo//79eS5PSEhQamqq83XkyJFCrhAAABQmt4LKTz/9lN91SJLS09OVnJysChUq5Lncx8dHpUqVcnkBAIDiy62gUq1aNd18882aPXu2MjIy3N746NGjtWbNGh08eFDr169Xz5495eHhob59+7q9TgAAUHy4FVS++eYbNWjQQKNGjVJYWJiGDh2qzZs3X/V6jh49qr59+6pmzZrq3bu3ypQpo40bN6pcuXLulAUAAIoZhzHGuPvmCxcuaNGiRUpKStLixYtVo0YNDRo0SPfee2+hhI20tDQFBQUpNTWV00AAgAL38rK9VpdQ6EbeViPf13k1v7+v6WJaT09P9erVS/PmzdMLL7yg/fv3a/To0QoPD9eAAQOUkpJyLasHAADXuWsKKlu3btWDDz6oChUqaMqUKRo9erSSk5O1bNky/fLLL+revXt+1QkAAK5Dbj1HZcqUKZo5c6b27Nmjzp0767333lPnzp1VosRfuScqKkpJSUmKjIzMz1oBAMB1xq2gMm3aNA0aNEhxcXGXvJU4NDRU77zzzjUVBwAArm9uBZV9+/b9Yxtvb2/Fxsa6s3oAAABJbl6jMnPmTM2bNy/X/Hnz5mnWrFnXXBQAAIDkZlBJTExU2bJlc80PDQ3V888/f81FAQAASG4GlcOHDysqKirX/IiICB0+fPiaiwIAAJDcDCqhoaH67rvvcs3fsWOHypQpc81FAQAASG4Glb59+2r48OFatWqVsrKylJWVpZUrV+qRRx7R3Xffnd81AgCA65Rbd/1MmDBBBw8e1K233ipPz79WkZ2drQEDBnCNCgAAyDduBRVvb2/NmTNHEyZM0I4dO+Tn56f69esrIiIiv+sDAADXMbeCSo4aNWqoRo38/7IiAAAAyc2gkpWVpaSkJK1YsUInTpxQdna2y/KVK1fmS3EAAOD65lZQeeSRR5SUlKQuXbqoXr16cjgc+V0XAACAe0Hlo48+0ty5c9W5c+f8rgcAAMDJrduTvb29Va1atfyuBQAAwIVbQeXRRx/VK6+8ImNMftcDAADg5Napn6+//lqrVq3Sl19+qbp168rLy8tl+SeffJIvxQEAgOubW0ElODhYPXv2zO9aAAAAXLgVVGbOnJnfdQAAAOTi1jUqknThwgUtX75cM2bM0OnTpyVJv/zyi9LT0/OtOAAAcH1za0Tl0KFD6tixow4fPqzMzEzddtttCgwM1AsvvKDMzExNnz49v+sEAADXIbdGVB555BE1bdpUf/zxh/z8/Jzze/bsqRUrVuRbcQAA4Prm1ojK2rVrtX79enl7e7vMj4yM1M8//5wvhQEAALg1opKdna2srKxc848eParAwMBrLgoAAEByM6jcfvvtmjp1qnPa4XAoPT1dY8aM4bH6AAAg37h16mfy5Mnq0KGD6tSpo4yMDN1zzz3at2+fypYtqw8//DC/awQAANcpt4JK5cqVtWPHDn300Uf67rvvlJ6ervj4ePXr18/l4loAAIBr4VZQkSRPT0/1798/P2sBAABw4VZQee+99y67fMCAAW4VAwAAcDG3gsojjzziMn3+/HmdPXtW3t7e8vf3J6gAAIB84dZdP3/88YfLKz09XXv27FGbNm24mBYAAOQbt7/r5++qV6+uiRMn5hptuVITJ06Uw+HQiBEj8qskAABQxOVbUJH+usD2l19+uer3bdmyRTNmzFCDBg3ysxwAAFDEuXWNyqJFi1ymjTFKSUnR66+/rtatW1/VutLT09WvXz+9/fbbeu6559wpBwAAFFNuBZUePXq4TDscDpUrV0633HKLJk+efFXrGjZsmLp06aL27dsTVAAAgAu3gkp2dna+bPyjjz7SN998oy1btlxR+8zMTGVmZjqn09LS8qUOAABgT24/8O1aHTlyRI888oiWLVsmX1/fK3pPYmKixo0bV8CV2cCqRGu2e3OCNdsFAOAS3Aoqo0aNuuK2U6ZMyXP+tm3bdOLECTVu3Ng5LysrS1999ZVef/11ZWZmysPDw+U9CQkJLttOS0tTeHj4VVYPAACKCreCyrfffqtvv/1W58+fV82aNSVJe/fulYeHh0vwcDgcl1zHrbfequ+//95l3sCBA1WrVi098cQTuUKKJPn4+MjHx8edkgEAQBHkVlDp1q2bAgMDNWvWLJUuXVrSXw+BGzhwoNq2batHH330H9cRGBioevXqucwrWbKkypQpk2s+AAC4Prn1HJXJkycrMTHRGVIkqXTp0nruueeu+q4fAACAS3FrRCUtLU2//vprrvm//vqrTp8+7XYxq1evdvu9AACg+HFrRKVnz54aOHCgPvnkEx09elRHjx7V/PnzFR8fr169euV3jQAA4Drl1ojK9OnTNXr0aN1zzz06f/78Xyvy9FR8fLwmTZqUrwUCAIDrl1tBxd/fX2+++aYmTZqk5ORkSVJ0dLRKliyZr8UBAIDr2zV9KWFKSopSUlJUvXp1lSxZUsaY/KoLAADAvaDy+++/69Zbb1WNGjXUuXNnpaSkSJLi4+Ov6NZkAACAK+FWUBk5cqS8vLx0+PBh+fv7O+f36dNHixcvzrfiAADA9c2ta1SWLl2qJUuWqHLlyi7zq1evrkOHDuVLYQAAAG6NqJw5c8ZlJCXHyZMnecQ9AADIN24FlbZt2+q9995zTjscDmVnZ+vFF1/UzTffnG/FAQCA65tbp35efPFF3Xrrrdq6davOnTunxx9/XD/88INOnjypdevW5XeNAADgOuXWiEq9evW0d+9etWnTRt27d9eZM2fUq1cvffvtt4qOjs7vGgEAwHXqqkdUzp8/r44dO2r69Ol66qmnCqImAAAASW6MqHh5eem7774riFoAAABcuHXqp3///nrnnXfyuxYAAAAXbl1Me+HCBb377rtavny5mjRpkus7fqZMmZIvxQEAgOvbVQWVn376SZGRkdq5c6caN24sSdq7d69LG4fDkX/VAQCA69pVBZXq1asrJSVFq1atkvTXI/NfffVVlS9fvkCKAwAA17erukbl79+O/OWXX+rMmTP5WhAAAEAOty6mzfH34AIAAJCfriqoOByOXNegcE0KAAAoKFd1jYoxRnFxcc4vHszIyND999+f666fTz75JP8qBAAA162rCiqxsbEu0/3798/XYgAAAC52VUFl5syZBVUHAABALtd0MS0AAEBBIqgAAADbIqgAAADbIqgAAADbIqgAAADbIqgAAADbIqgAAADbIqgAAADbIqgAAADbIqgAAADbsjSoTJs2TQ0aNFCpUqVUqlQptWzZUl9++aWVJQEAABuxNKhUrlxZEydO1LZt27R161bdcsst6t69u3744QcrywIAADZxVV9KmN+6devmMv2f//xH06ZN08aNG1W3bl2LqgIAAHZhaVC5WFZWlubNm6czZ86oZcuWVpcDAABswPKg8v3336tly5bKyMhQQECAPv30U9WpUyfPtpmZmcrMzHROp6WlFVaZAADAApYHlZo1a2r79u1KTU3Vxx9/rNjYWK1ZsybPsJKYmKhx48ZZUCUK1KpEqysoXDcnWF0BABQZlt+e7O3trWrVqqlJkyZKTExUw4YN9corr+TZNiEhQampqc7XkSNHCrlaAABQmCwfUfm77Oxsl9M7F/Px8ZGPj08hVwQAAKxiaVBJSEhQp06dVKVKFZ0+fVoffPCBVq9erSVLllhZFgAAsAlLg8qJEyc0YMAApaSkKCgoSA0aNNCSJUt02223WVkWAACwCUuDyjvvvGPl5gEAgM1ZfjEtAADApRBUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbRFUAACAbVkaVBITE9WsWTMFBgYqNDRUPXr00J49e6wsCQAA2IilQWXNmjUaNmyYNm7cqGXLlun8+fO6/fbbdebMGSvLAgAANuFp5cYXL17sMp2UlKTQ0FBt27ZNN910k0VVAQAAu7DVNSqpqamSpJCQEIsrAQAAdmDpiMrFsrOzNWLECLVu3Vr16tXLs01mZqYyMzOd02lpaYVVHgAAsIBtgsqwYcO0c+dOff3115dsk5iYqHHjxhViVUABWJVozXZvTrBmuwBwDWxx6uehhx7SZ599plWrVqly5cqXbJeQkKDU1FTn68iRI4VYJQAAKGyWjqgYY/Twww/r008/1erVqxUVFXXZ9j4+PvLx8Smk6gAAgNUsDSrDhg3TBx98oIULFyowMFDHjh2TJAUFBcnPz8/K0gAAgA1Yeupn2rRpSk1NVbt27VShQgXna86cOVaWBQAAbMLyUz8AAACXYouLaQEAAPJCUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZFUAEAALZlaVD56quv1K1bN1WsWFEOh0MLFiywshwAAGAzlgaVM2fOqGHDhnrjjTesLAMAANiUp5Ub79Spkzp16mRlCQAAwMa4RgUAANiWpSMqVyszM1OZmZnO6bS0NAurAQAABa1IBZXExESNGzeu8Da4KrHwtmUH19v+Xm+s+vnenGDNdlGsvbxsr9UloJAUqVM/CQkJSk1Ndb6OHDlidUkAAKAAFakRFR8fH/n4+FhdBgAAKCSWBpX09HTt37/fOX3gwAFt375dISEhqlKlioWVAQAAO7A0qGzdulU333yzc3rUqFGSpNjYWCUlJVlUFQAAsAtLg0q7du1kjLGyBAAAYGNF6mJaAABwfSGoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA27JFUHnjjTcUGRkpX19ftWjRQps3b7a6JAAAYAOWB5U5c+Zo1KhRGjNmjL755hs1bNhQHTp00IkTJ6wuDQAAWMzyoDJlyhQNHjxYAwcOVJ06dTR9+nT5+/vr3Xfftbo0AABgMUuDyrlz57Rt2za1b9/eOa9EiRJq3769NmzYYGFlAADADjyt3Phvv/2mrKwslS9f3mV++fLl9eOPP+Zqn5mZqczMTOd0amqqJCktLa1gCjyTUTDrBa4nBfX/E9e1jDPpVpdw3SiI37E56zTG/GNbS4PK1UpMTNS4ceNyzQ8PD7egGgBXZrzVBQC4Bv8uwHWfPn1aQUFBl21jaVApW7asPDw8dPz4cZf5x48fV1hYWK72CQkJGjVqlHM6OztbJ0+eVJkyZeRwOPK1trS0NIWHh+vIkSMqVapUvq67qKJP8ka/5Eaf5I1+yY0+yVtx7xdjjE6fPq2KFSv+Y1tLg4q3t7eaNGmiFStWqEePHpL+Ch8rVqzQQw89lKu9j4+PfHx8XOYFBwcXaI2lSpUqlh+Sa0Gf5I1+yY0+yRv9kht9krfi3C//NJKSw/JTP6NGjVJsbKyaNm2q5s2ba+rUqTpz5owGDhxodWkAAMBilgeVPn366Ndff9Wzzz6rY8eO6YYbbtDixYtzXWALAACuP5YHFUl66KGH8jzVYyUfHx+NGTMm16mm6xl9kjf6JTf6JG/0S270Sd7ol//PYa7k3iAAAAALWP5kWgAAgEshqAAAANsiqAAAANsiqAAAANsqtkHljTfeUGRkpHx9fdWiRQtt3rz5su3nzZunWrVqydfXV/Xr19cXX3zhsjwuLk4Oh8Pl1bFjR5c2kZGRudpMnDgx3/ftWuR3v0jS7t279a9//UtBQUEqWbKkmjVrpsOHDzuXZ2RkaNiwYSpTpowCAgJ0xx135HoasZWs6JN27drl+qzcf//9+b5v7srvPvn7vua8Jk2a5Gxz8uRJ9evXT6VKlVJwcLDi4+OVnm6v73Oxol/sflzJ7z5JT0/XQw89pMqVK8vPz0916tTR9OnTXdrY/ZgiWdMvdj+uuM0UQx999JHx9vY27777rvnhhx/M4MGDTXBwsDl+/Hie7detW2c8PDzMiy++aHbt2mWefvpp4+XlZb7//ntnm9jYWNOxY0eTkpLifJ08edJlPREREWb8+PEubdLT0wt0X69GQfTL/v37TUhIiHnsscfMN998Y/bv328WLlzoss7777/fhIeHmxUrVpitW7eaG2+80bRq1arA9/dKWNUnMTExZvDgwS6fldTU1ALf3ytREH1y8X6mpKSYd9991zgcDpOcnOxs07FjR9OwYUOzceNGs3btWlOtWjXTt2/fAt/fK2VVv9j5uFIQfTJ48GATHR1tVq1aZQ4cOGBmzJhhPDw8zMKFC51t7HxMMca6frHzceVaFMug0rx5czNs2DDndFZWlqlYsaJJTEzMs33v3r1Nly5dXOa1aNHCDB061DkdGxtrunfvftntRkREmJdfftntugtaQfRLnz59TP/+/S+5zVOnThkvLy8zb94857zdu3cbSWbDhg3u7kq+saJPjPnrgPLII4+4X3gBKog++bvu3bubW265xTm9a9cuI8ls2bLFOe/LL780DofD/Pzzz+7uSr6yol+MsfdxpSD6pG7dumb8+PEubRo3bmyeeuopY4z9jynGWNMvxtj7uHItit2pn3Pnzmnbtm1q3769c16JEiXUvn17bdiwIc/3bNiwwaW9JHXo0CFX+9WrVys0NFQ1a9bUAw88oN9//z3XuiZOnKgyZcqoUaNGmjRpki5cuJAPe3XtCqJfsrOz9fnnn6tGjRrq0KGDQkND1aJFCy1YsMDZftu2bTp//rzLemrVqqUqVapccruFxao+yfH++++rbNmyqlevnhISEnT27Nn82zk3FeT/nxzHjx/X559/rvj4eJd1BAcHq2nTps557du3V4kSJbRp06Zr2aV8YVW/5LDjcaWg+qRVq1ZatGiRfv75ZxljtGrVKu3du1e33367JHsfUyTr+iWHHY8r18oWT6bNT7/99puysrJyPYK/fPny+vHHH/N8z7Fjx/Jsf+zYMed0x44d1atXL0VFRSk5OVn//ve/1alTJ23YsEEeHh6SpOHDh6tx48YKCQnR+vXrlZCQoJSUFE2ZMiWf9/LqFUS/nDhxQunp6Zo4caKee+45vfDCC1q8eLF69eqlVatWKSYmRseOHZO3t3euL4/8e/9awao+kaR77rlHERERqlixor777js98cQT2rNnjz755JMC2NMrV1D/fy42a9YsBQYGqlevXi7rCA0NdWnn6empkJAQyz8nknX9Itn3uFJQffLaa69pyJAhqly5sjw9PVWiRAm9/fbbuummm5zrsOsxRbKuXyT7HleuVbELKgXl7rvvdv67fv36atCggaKjo7V69Wrdeuutkv76gsUcDRo0kLe3t4YOHarExMRi+Rjk7OxsSVL37t01cuRISdINN9yg9evXa/r06c5fyteTK+2TIUOGON9Tv359VahQQbfeequSk5MVHR1d+IUXonfffVf9+vWTr6+v1aXYyqX65Xo7rrz22mvauHGjFi1apIiICH311VcaNmyYKlasmGvU4XpyJf1SXI8rxe7UT9myZeXh4ZHrCvDjx48rLCwsz/eEhYVdVXtJqlq1qsqWLav9+/dfsk2LFi104cIFHTx48Mp3oIAURL+ULVtWnp6eqlOnjkub2rVrO+9wCQsL07lz53Tq1Kkr3m5hsapP8tKiRQtJuuznqTAU9P+ftWvXas+ePbrvvvtyrePEiRMu8y5cuKCTJ09a/jmRrOuXvNjluFIQffLnn3/q3//+t6ZMmaJu3bqpQYMGeuihh9SnTx+99NJLznXY9ZgiWdcvebHLceVaFbug4u3trSZNmmjFihXOednZ2VqxYoVatmyZ53tatmzp0l6Sli1bdsn2knT06FH9/vvvqlChwiXbbN++XSVKlMg1pG2FgugXb29vNWvWTHv27HFps3fvXkVEREiSmjRpIi8vL5f17NmzR4cPH75s/xYGq/okL9u3b5eky36eCkNB//9555131KRJEzVs2DDXOk6dOqVt27Y5561cuVLZ2dnOg62VrOqXvNjluFIQfXL+/HmdP39eJUq4/mry8PBwjlba+ZgiWdcvebHLceWaWX01b0H46KOPjI+Pj0lKSjK7du0yQ4YMMcHBwebYsWPGGGPuvfde8+STTzrbr1u3znh6epqXXnrJ7N6924wZM8bl1rDTp0+b0aNHmw0bNpgDBw6Y5cuXm8aNG5vq1aubjIwMY4wx69evNy+//LLZvn27SU5ONrNnzzblypUzAwYMKPwOuIT87hdjjPnkk0+Ml5eXeeutt8y+ffvMa6+9Zjw8PMzatWudbe6//35TpUoVs3LlSrN161bTsmVL07Jly8Lb8cuwok/2799vxo8fb7Zu3WoOHDhgFi5caKpWrWpuuummwt35SyiIPjHGmNTUVOPv72+mTZuW53Y7duxoGjVqZDZt2mS+/vprU716ddvdnlzY/WL340pB9ElMTIypW7euWbVqlfnpp5/MzJkzja+vr3nzzTedbex8TDHGmn6x+3HlWhTLoGKMMa+99pqpUqWK8fb2Ns2bNzcbN250LouJiTGxsbEu7efOnWtq1KhhvL29Td26dc3nn3/uXHb27Flz++23m3LlyhkvLy8TERFhBg8e7PzQGWPMtm3bTIsWLUxQUJDx9fU1tWvXNs8//7wzyNhFfvZLjnfeecdUq1bN+Pr6moYNG5oFCxa4LP/zzz/Ngw8+aEqXLm38/f1Nz549TUpKSoHsnzsKu08OHz5sbrrpJhMSEmJ8fHxMtWrVzGOPPWar5x0URJ/MmDHD+Pn5mVOnTuW5zd9//9307dvXBAQEmFKlSpmBAwea06dP5+t+XavC7peicFzJ7z5JSUkxcXFxpmLFisbX19fUrFnTTJ482WRnZzvb2P2YYkzh90tROK64y2GMMVaP6gAAAOSl2F2jAgAAig+CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgAAsC2CCgBbaNeunUaMGGF1GQBshqAC4Jp169ZNHTt2zHPZ2rVr5XA49N133xVyVQCKA4IKgGsWHx+vZcuW6ejRo7mWzZw5U02bNlWDBg0sqAxAUUdQAXDNunbtqnLlyikpKcllfnp6uubNm6cePXqob9++qlSpkvz9/VW/fn19+OGHl12nw+HQggULXOYFBwe7bOPIkSPq3bu3goODFRISou7du+vgwYPO5atXr1bz5s1VsmRJBQcHq3Xr1jp06NA17i2AwkRQAXDNPD09NWDAACUlJenirw+bN2+esrKy1L9/fzVp0kSff/65du7cqSFDhujee+/V5s2b3d7m+fPn1aFDBwUGBmrt2rVat26dAgIC1LFjR507d04XLlxQjx49FBMTo++++04bNmzQkCFD5HA48mOXARQST6sLAFA8DBo0SJMmTdKaNWvUrl07SX+d9rnjjjsUERGh0aNHO9s+/PDDWrJkiebOnavmzZu7tb05c+YoOztb//3vf53hY+bMmQoODtbq1avVtGlTpaamqmvXroqOjpYk1a5d+9p2EkChY0QFQL6oVauWWrVqpXfffVeStH//fq1du1bx8fHKysrShAkTVL9+fYWEhCggIEBLlizR4cOH3d7ejh07tH//fgUGBiogIEABAQEKCQlRRkaGkpOTFRISori4OHXo0EHdunXTK6+8opSUlPzaXQCFhKACIN/Ex8dr/vz5On36tGbOnKno6GjFxMRo0qRJeuWVV/TEE09o1apV2r59uzp06KBz585dcl0Oh8PlNJL01+meHOnp6WrSpIm2b9/u8tq7d6/uueceSX+NsGzYsEGtWrXSnDlzVKNGDW3cuLFgdh5AgSCoAMg3vXv3VokSJfTBBx/ovffe06BBg+RwOLRu3Tp1795d/fv3V8OGDVW1alXt3bv3susqV66cywjIvn37dPbsWed048aNtW/fPoWGhqpatWour6CgIGe7Ro0aKSEhQevXr1e9evX0wQcf5P+OAygwBBUA+SYgIEB9+vRRQkKCUlJSFBcXJ0mqXr26li1bpvXr12v37t0aOnSojh8/ftl13XLLLXr99df17bffauvWrbr//vvl5eXlXN6vXz+VLVtW3bt319q1a3XgwAGtXr1aw4cP19GjR3XgwAElJCRow4YNOnTokJYuXap9+/ZxnQpQxBBUAOSr+Ph4/fHHH+rQoYMqVqwoSXr66afVuHFjdejQQe3atVNYWJh69Ohx2fVMnjxZ4eHhatu2re655x6NHj1a/v7+zuX+/v766quvVKVKFfXq1Uu1a9dWfHy8MjIyVKpUKfn7++vHH3/UHXfcoRo1amjIkCEaNmyYhg4dWpC7DyCfOczfTwIDAADYBCMqAADAtggqAADAtggqAADAtggqAADAtggqAADAtggqAADAtggqAADAtggqAADAtggqAADAtggqAADAtggqAADAtggqAADAtv4fHgjcsnDDRZIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Determine the range of values in both lists\n",
    "min_value = min(min(licit_loss_history), min(ilicit_loss_history))\n",
    "max_value = max(max(licit_loss_history), max(ilicit_loss_history))\n",
    "\n",
    "# Plot histograms for both lists with 10 bins\n",
    "plt.hist(licit_loss_history, bins=len(licit_loss_history), alpha=0.5, label='Licit nodes', range=(min_value, max_value))\n",
    "plt.hist(ilicit_loss_history, bins=len(ilicit_loss_history), alpha=0.5, label='Ilicit nodes', range=(min_value, max_value))\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Reconstruction Error plot')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visionEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
