{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import sys\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import torch.utils.data as data \n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.nn import MessagePassing, SAGEConv\n",
    "from ogb.nodeproppred import Evaluator #PygNodePropPredDatase\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files\n",
    "path = \"dades_guillem/\"\n",
    "df_train_init = pd.read_csv(path + \"train_set.csv\") \n",
    "df_test_init = pd.read_csv(path + \"test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/gsamp/OneDrive/Documents/AI-3/2n Semestre/Projecte de Síntesi 2/GraphAnomaly/dades_guillem/\"\n",
    "df_classes = pd.read_csv(path + \"elliptic_txs_classes.csv\") # Nodes' labels\n",
    "df_edges_init = pd.read_csv(path + \"elliptic_txs_edgelist.csv\") # Edges\n",
    "df_features = pd.read_csv(path + \"elliptic_txs_features.csv\", header=None) # Nodes' features\n",
    "\n",
    "# Change column names of df_features\n",
    "colNames1 = {'0': 'txId', 1: \"Time step\"}\n",
    "colNames2 = {str(ii+2): \"Local_feature_\" + str(ii+1) for ii in range(93)}\n",
    "colNames3 = {str(ii+95): \"Aggregate_feature_\" + str(ii+1) for ii in range(72)}\n",
    "\n",
    "colNames = dict(colNames1, **colNames2, **colNames3 )\n",
    "colNames = {int(jj): item_kk for jj,item_kk in colNames.items()}\n",
    "\n",
    "df_features = df_features.rename(columns=colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contador de valors per classe: \n",
      " class\n",
      "1    34654\n",
      "0     2672\n",
      "Name: count, dtype: int64\n",
      "\n",
      "contador de valors per classe: \n",
      " class\n",
      "1    7365\n",
      "0    1873\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsamp\\AppData\\Local\\Temp\\ipykernel_16268\\1761167425.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_feats['class'] = df_feats['class'].replace({1: 0, 2: 1})\n"
     ]
    }
   ],
   "source": [
    "def prep_df(feats: pd.DataFrame, edges: pd.DataFrame):\n",
    "    #1 és la classe illicit, 2 la  licit\n",
    "    df_feats = feats.loc[feats['class'].isin([1, 2])]\n",
    "    df_feats['class'] = df_feats['class'].replace({1: 0, 2: 1})\n",
    "    df_feats = df_feats.reset_index(drop=True)\n",
    "\n",
    "    #ens quedem només amb els edges que apareixen en el nodes d'entrenament\n",
    "    df_edges = edges.loc[((edges['txId1'].isin(df_feats['txId'])) & (df_edges_init['txId2'].isin(df_feats['txId'])))]\n",
    "    df_edges = df_edges.reset_index(drop=True)\n",
    "    print(f\"contador de valors per classe: \\n {df_feats['class'].value_counts()}\\n\")\n",
    "    return  df_feats, df_edges\n",
    "\n",
    "df_train, df_edges_train = prep_df(df_train_init, df_edges_init)\n",
    "df_test, df_edges_test = prep_df(df_test_init, df_edges_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_idx(feats: pd.DataFrame, edges: pd.DataFrame, save = True, loading_dir = \"a\"):\n",
    "    mapping_txid = dict(zip(feats['txId'], list(feats.index)))\n",
    "    dir = 'dades_guillem/' + str(loading_dir) + '.pkl'\n",
    "    if save:\n",
    "        df_edges_mapped = edges.replace({'txId1': mapping_txid, 'txId2': mapping_txid})\n",
    "        \n",
    "        df_edges_mapped.to_pickle(loading_dir)\n",
    "    else:\n",
    "        df_edges_mapped = pd.read_pickle(loading_dir)\n",
    "    return df_edges_mapped\n",
    "\n",
    "df_edges_mapped_train = map_idx(feats = df_train, edges = df_edges_train, save = True, loading_dir='train')\n",
    "df_edges_mapped_test = map_idx(feats = df_test, edges = df_edges_test, save = True, loading_dir='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(feats: pd.DataFrame, edges:pd.DataFrame):\n",
    "    x = torch.tensor(feats.drop(columns=['class', 'Time Step', 'txId']).values, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges.values, dtype=torch.long).T\n",
    "    y = torch.tensor(feats['class'].values)\n",
    "    time = torch.tensor(feats['Time Step'].values)\n",
    "    data = Data(x=x, edge_index=edge_index, y=y, time=time)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = get_data(df_train, df_edges_mapped_train)\n",
    "test_data = get_data(df_test, df_edges_mapped_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for the AE case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate illicit and licit data\n",
    "def separate_data(feats):\n",
    "    licit_x = torch.tensor(feats.loc[feats['class'] == 1].drop(columns=['class', 'Time Step']).values, dtype=torch.float)\n",
    "    licit_y = torch.tensor(feats.loc[feats['class'] == 1]['class'].values)\n",
    "    licit_data = Data(x=licit_x, y=licit_y)\n",
    "\n",
    "    illicit_x = torch.tensor(feats.loc[feats['class'] == 0].drop(columns=['class', 'Time Step']).values, dtype=torch.float)\n",
    "    illicit_y = torch.tensor(feats.loc[feats['class'] == 0]['class'].values)\n",
    "    illicit_data = Data(x=illicit_x, y=illicit_x)\n",
    "    return licit_data, illicit_data\n",
    "\n",
    "train_licit, train_illicit = separate_data(df_train)\n",
    "test_licit, test_illicit = separate_data(df_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return torch.tensor(sample, dtype=torch.float32), torch.tensor(sample, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim=50):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(50, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(autoencoder, data_loader, criterion, optimizer, val_loader, num_epochs=10, learning_rate=0.001):   \n",
    "    total_train_loss = [] \n",
    "    total_validation_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = autoencoder(inputs)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss = running_loss / len(data_loader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}\")\n",
    "        validation_loss = val_ae(autoencoder=autoencoder, data_loader=val_loader, criterion=criterion)\n",
    "        print(f\"Validation Loss: {validation_loss:.4f}\")\n",
    "        total_train_loss.append(train_loss)\n",
    "        total_validation_loss.append(validation_loss)\n",
    "    return total_train_loss, total_validation_loss\n",
    "\n",
    "def val_ae(autoencoder, data_loader, criterion):\n",
    "    autoencoder.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = val_loss / len(data_loader.dataset)\n",
    "    return epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 0.1509\n",
      "Validation Loss: 0.1358\n",
      "Epoch [2/50], Training Loss: 0.1509\n",
      "Validation Loss: 0.1357\n",
      "Epoch [3/50], Training Loss: 0.1508\n",
      "Validation Loss: 0.1357\n",
      "Epoch [4/50], Training Loss: 0.1508\n",
      "Validation Loss: 0.1357\n",
      "Epoch [5/50], Training Loss: 0.1508\n",
      "Validation Loss: 0.1357\n",
      "Epoch [6/50], Training Loss: 0.1508\n",
      "Validation Loss: 0.1357\n",
      "Epoch [7/50], Training Loss: 0.1508\n",
      "Validation Loss: 0.1356\n",
      "Epoch [8/50], Training Loss: 0.1507\n",
      "Validation Loss: 0.1356\n",
      "Epoch [9/50], Training Loss: 0.1507\n",
      "Validation Loss: 0.1356\n",
      "Epoch [10/50], Training Loss: 0.1507\n",
      "Validation Loss: 0.1356\n",
      "Epoch [11/50], Training Loss: 0.1507\n",
      "Validation Loss: 0.1355\n",
      "Epoch [12/50], Training Loss: 0.1506\n",
      "Validation Loss: 0.1355\n",
      "Epoch [13/50], Training Loss: 0.1506\n",
      "Validation Loss: 0.1355\n",
      "Epoch [14/50], Training Loss: 0.1506\n",
      "Validation Loss: 0.1355\n",
      "Epoch [15/50], Training Loss: 0.1506\n",
      "Validation Loss: 0.1355\n",
      "Epoch [16/50], Training Loss: 0.1506\n",
      "Validation Loss: 0.1354\n",
      "Epoch [17/50], Training Loss: 0.1505\n",
      "Validation Loss: 0.1354\n",
      "Epoch [18/50], Training Loss: 0.1505\n",
      "Validation Loss: 0.1354\n",
      "Epoch [19/50], Training Loss: 0.1505\n",
      "Validation Loss: 0.1354\n",
      "Epoch [20/50], Training Loss: 0.1505\n",
      "Validation Loss: 0.1354\n",
      "Epoch [21/50], Training Loss: 0.1504\n",
      "Validation Loss: 0.1353\n",
      "Epoch [22/50], Training Loss: 0.1504\n",
      "Validation Loss: 0.1353\n",
      "Epoch [23/50], Training Loss: 0.1504\n",
      "Validation Loss: 0.1353\n",
      "Epoch [24/50], Training Loss: 0.1504\n",
      "Validation Loss: 0.1353\n",
      "Epoch [25/50], Training Loss: 0.1503\n",
      "Validation Loss: 0.1352\n",
      "Epoch [26/50], Training Loss: 0.1503\n",
      "Validation Loss: 0.1352\n",
      "Epoch [27/50], Training Loss: 0.1503\n",
      "Validation Loss: 0.1352\n",
      "Epoch [28/50], Training Loss: 0.1503\n",
      "Validation Loss: 0.1352\n",
      "Epoch [29/50], Training Loss: 0.1503\n",
      "Validation Loss: 0.1352\n",
      "Epoch [30/50], Training Loss: 0.1502\n",
      "Validation Loss: 0.1351\n",
      "Epoch [31/50], Training Loss: 0.1502\n",
      "Validation Loss: 0.1351\n",
      "Epoch [32/50], Training Loss: 0.1502\n",
      "Validation Loss: 0.1351\n",
      "Epoch [33/50], Training Loss: 0.1502\n",
      "Validation Loss: 0.1351\n",
      "Epoch [34/50], Training Loss: 0.1501\n",
      "Validation Loss: 0.1350\n",
      "Epoch [35/50], Training Loss: 0.1501\n",
      "Validation Loss: 0.1350\n",
      "Epoch [36/50], Training Loss: 0.1501\n",
      "Validation Loss: 0.1350\n",
      "Epoch [37/50], Training Loss: 0.1501\n",
      "Validation Loss: 0.1350\n",
      "Epoch [38/50], Training Loss: 0.1500\n",
      "Validation Loss: 0.1349\n",
      "Epoch [39/50], Training Loss: 0.1500\n",
      "Validation Loss: 0.1349\n",
      "Epoch [40/50], Training Loss: 0.1500\n",
      "Validation Loss: 0.1349\n",
      "Epoch [41/50], Training Loss: 0.1500\n",
      "Validation Loss: 0.1349\n",
      "Epoch [42/50], Training Loss: 0.1499\n",
      "Validation Loss: 0.1349\n",
      "Epoch [43/50], Training Loss: 0.1499\n",
      "Validation Loss: 0.1348\n",
      "Epoch [44/50], Training Loss: 0.1499\n",
      "Validation Loss: 0.1348\n",
      "Epoch [45/50], Training Loss: 0.1499\n",
      "Validation Loss: 0.1348\n",
      "Epoch [46/50], Training Loss: 0.1498\n",
      "Validation Loss: 0.1348\n",
      "Epoch [47/50], Training Loss: 0.1498\n",
      "Validation Loss: 0.1347\n",
      "Epoch [48/50], Training Loss: 0.1498\n",
      "Validation Loss: 0.1347\n",
      "Epoch [49/50], Training Loss: 0.1498\n",
      "Validation Loss: 0.1347\n",
      "Epoch [50/50], Training Loss: 0.1497\n",
      "Validation Loss: 0.1347\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "n= 2000\n",
    "#samplejar només n elements de les lícites per entrenar el AE\n",
    "x_train = torch.tensor(df_train.loc[df_train[\"class\"] == 1].drop(columns=['class', 'txId', 'Time Step'])[:n].values)\n",
    "x_val = torch.tensor(df_test.loc[df_train[\"class\"] == 1].drop(columns=['class', 'txId', 'Time Step'])[:n].values)\n",
    "#amb el scaler la loss passe de 4 a 0.1\n",
    "train_data = preprocessing.MinMaxScaler().fit_transform(x_train)\n",
    "val_data = preprocessing.MinMaxScaler().fit_transform(x_val)\n",
    "\n",
    "# train_data, val_data = train_test_split(x, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = CustomDataset(val_data)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#hiperparàmetres\n",
    "lr = 0.001\n",
    "EPOCHS = 50\n",
    "\n",
    "input_dim = x_train.shape[1]\n",
    "model = Autoencoder(input_dim)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=lr)\n",
    "\n",
    "training_loss, validation_loss = train_ae(autoencoder=model, data_loader=train_loader, criterion=criterion, optimizer=optimizer, val_loader=val_loader, num_epochs=EPOCHS, learning_rate=lr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visionEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
