{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import sys\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.nn import MessagePassing, SAGEConv\n",
    "from ogb.nodeproppred import Evaluator #PygNodePropPredDatase\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files\n",
    "path = \"C:/Users/gsamp/OneDrive/Documents/AI-3/2n Semestre/Projecte de Síntesi 2/GraphAnomaly/dades_guillem/\"\n",
    "df_train_init = pd.read_csv(path + \"train_set.csv\") \n",
    "df_test_init = pd.read_csv(path + \"test_set.csv\")\n",
    "\n",
    "\n",
    "path = \"C:/Users/gsamp/OneDrive/Documents/AI-3/2n Semestre/Projecte de Síntesi 2/GraphAnomaly/dades_guillem/\"\n",
    "df_classes = pd.read_csv(path + \"elliptic_txs_classes.csv\") # Nodes' labels\n",
    "df_edges_init = pd.read_csv(path + \"elliptic_txs_edgelist.csv\") # Edges\n",
    "df_features = pd.read_csv(path + \"elliptic_txs_features.csv\", header=None) # Nodes' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIXO JA NO HAURIA DE CALDRE, ESTA AL DATA SPLIT\n",
    "# path = \"C:/Users/gsamp/OneDrive/Documents/AI-3/2n Semestre/Projecte de Síntesi 2/GraphAnomaly/dades_guillem/\"\n",
    "# df_classes = pd.read_csv(path + \"elliptic_txs_classes.csv\") # Nodes' labels\n",
    "# df_edges_init = pd.read_csv(path + \"elliptic_txs_edgelist.csv\") # Edges\n",
    "# df_features = pd.read_csv(path + \"elliptic_txs_features.csv\", header=None) # Nodes' features\n",
    "\n",
    "# Change column names of df_features\n",
    "colNames1 = {'0': 'txId', 1: \"Time step\"}\n",
    "colNames2 = {str(ii+2): \"Local_feature_\" + str(ii+1) for ii in range(93)}\n",
    "colNames3 = {str(ii+95): \"Aggregate_feature_\" + str(ii+1) for ii in range(72)}\n",
    "\n",
    "colNames = dict(colNames1, **colNames2, **colNames3 )\n",
    "colNames = {int(jj): item_kk for jj,item_kk in colNames.items()}\n",
    "\n",
    "df_features = df_features.rename(columns=colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contador de valors per classe: \n",
      " class\n",
      "3    123281\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsamp\\AppData\\Local\\Temp\\ipykernel_12228\\1010652687.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_feats['class'] = df_feats['class'].replace({1: 0, 2: 1})\n"
     ]
    }
   ],
   "source": [
    "# AIXO JA NO HAURIA DE CALDRE, ESTA AL DATA SPLIT\n",
    "def prep_df(feats: pd.DataFrame, edges: pd.DataFrame):\n",
    "    #1 és la classe illicit, 2 la  licit\n",
    "    df_feats = feats.loc[feats['class'].isin([1,2,3])]\n",
    "    df_feats['class'] = df_feats['class'].replace({1: 0, 2: 1})\n",
    "    df_feats = df_feats.reset_index(drop=True)\n",
    "\n",
    "    #ens quedem només amb els edges que apareixen en el nodes d'entrenament\n",
    "    df_edges = edges.loc[((edges['txId1'].isin(df_feats['txId'])) & (df_edges_init['txId2'].isin(df_feats['txId'])))]\n",
    "    df_edges = df_edges.reset_index(drop=True)\n",
    "    print(f\"contador de valors per classe: \\n {df_feats['class'].value_counts()}\\n\")\n",
    "    return  df_feats, df_edges\n",
    "\n",
    "# df_train, df_edges_train = prep_df(df_train_init, df_edges_init)\n",
    "# df_test, df_edges_test = prep_df(df_test_init, df_edges_init)\n",
    "df_unk, df_edges_unk = prep_df(df_train_init, df_edges_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_idx(feats: pd.DataFrame, edges: pd.DataFrame, save = True, loading_dir = \"a\"):\n",
    "    mapping_txid = dict(zip(feats['txId'], list(feats.index)))\n",
    "    dir = 'dades_guillem/' + str(loading_dir) + '.pkl'\n",
    "    if save:\n",
    "        df_edges_mapped = edges.replace({'txId1': mapping_txid, 'txId2': mapping_txid})\n",
    "        \n",
    "        df_edges_mapped.to_pickle(loading_dir)\n",
    "    else:\n",
    "        df_edges_mapped = pd.read_pickle(loading_dir)\n",
    "    return df_edges_mapped\n",
    "\n",
    "# df_edges_mapped_train = map_idx(feats = df_train, edges = df_edges_train, save = True, loading_dir='train')\n",
    "# df_edges_mapped_test = map_idx(feats = df_test, edges = df_edges_test, save = True, loading_dir='test') # Això no hauria de ser test?\n",
    "df_edges_mapped_unk = map_idx(feats = df_unk, edges = df_edges_unk, save = True, loading_dir='unk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(feats: pd.DataFrame, edges:pd.DataFrame):\n",
    "    x = torch.tensor(feats.drop(columns=['class', 'Time Step', 'txId']).values, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges.values, dtype=torch.long).T\n",
    "    y = torch.tensor(feats['class'].values)\n",
    "    time = torch.tensor(feats['Time Step'].values)\n",
    "    data = Data(x=x, edge_index=edge_index, y=y, time=time)\n",
    "    return data\n",
    "\n",
    "\n",
    "# train_data = get_data(df_train, df_edges_mapped_train)\n",
    "# test_data = get_data(df_test, df_edges_mapped_test)\n",
    "unk_data = get_data(df_unk, df_edges_mapped_unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels,\n",
    "                 hidden_channels, out_channels,\n",
    "                 n_layers=2):\n",
    "        \n",
    "        super(SAGE, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers_bn = torch.nn.ModuleList()\n",
    "        if n_layers == 1:\n",
    "            self.layers.append(SAGEConv(in_channels, out_channels,   normalize=False))\n",
    "        elif n_layers == 2:\n",
    "            self.layers.append(SAGEConv(in_channels, hidden_channels, normalize=False))\n",
    "            self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "            # self.layers.append(SAGEConv(hidden_channels, out_channels, normalize=False))\n",
    "        else:\n",
    "            self.layers.append(SAGEConv(in_channels, hidden_channels, normalize=False))\n",
    "            # self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(n_layers - 2):\n",
    "            self.layers.append(SAGEConv(hidden_channels,  hidden_channels, normalize=False))\n",
    "            self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "                    \n",
    "                \n",
    "        if n_layers != 1:\n",
    "            self.layers.append(SAGEConv(hidden_channels, out_channels, normalize=False))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer.reset_parameters()\n",
    "            \n",
    "            \n",
    "            \n",
    "    def forward(self, x, edge_index):\n",
    "        if len(self.layers) > 1:\n",
    "            looper = self.layers[:-1]\n",
    "        else:\n",
    "            looper = self.layers\n",
    "        \n",
    "        for i, layer in enumerate(looper):\n",
    "            x = layer(x, edge_index)\n",
    "            # print(f\"SHAPE: {x.shape}, step: {i}\")\n",
    "            # print(f\"Step: {i}\")\n",
    "            try:\n",
    "                x = self.layers_bn[i](x)\n",
    "            except Exception as e:\n",
    "                abs(1)\n",
    "            finally:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        if len(self.layers) > 1:\n",
    "            x = self.layers[-1](x, edge_index)\n",
    "        return F.log_softmax(x, dim=-1), torch.var(x)\n",
    "    \n",
    "    def inference(self, total_loader, device):\n",
    "        xs = []\n",
    "        var_ = []\n",
    "        for batch in total_loader:\n",
    "            out, var = self.forward(batch.x.to(device), batch.edge_index.to(device))\n",
    "            out = out[:batch.batch_size]\n",
    "            xs.append(out.cpu())\n",
    "            var_.append(var.item())\n",
    "        \n",
    "        out_all = torch.cat(xs, dim=0)\n",
    "        \n",
    "        return out_all, var_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgsamper\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gsamp\\OneDrive\\Documents\\AI-3\\2n Semestre\\Projecte de Síntesi 2\\GraphAnomaly\\GNN_models\\wandb\\run-20240516_125512-avngqg96</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gsamper/GraphAnomaly/runs/avngqg96' target=\"_blank\">2_SAGE - n_layers: 2</a></strong> to <a href='https://wandb.ai/gsamper/GraphAnomaly' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gsamper/GraphAnomaly' target=\"_blank\">https://wandb.ai/gsamper/GraphAnomaly</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gsamper/GraphAnomaly/runs/avngqg96' target=\"_blank\">https://wandb.ai/gsamper/GraphAnomaly/runs/avngqg96</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes classified as fraud: 20593, \n",
      "                nodes classified as licit: 16733\n",
      "Epoch: 0, \n",
      "            Training Loss: 0.9328, Training Accuracy: 0.4679\n",
      "            Test Loss: 1.0730, Test Accuracy: 0.7973\n",
      "            \n",
      "Precision: [0.         0.79725049], Recall: [0. 1.], F1 Score: [0.         0.88718906] \n",
      "\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gsamp\\anaconda3\\envs\\visionEnv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\gsamp\\anaconda3\\envs\\visionEnv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\gsamp\\anaconda3\\envs\\visionEnv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\gsamp\\anaconda3\\envs\\visionEnv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\gsamp\\anaconda3\\envs\\visionEnv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes classified as fraud: 5, \n",
      "                nodes classified as licit: 37321\n",
      "Epoch: 5, \n",
      "            Training Loss: 0.4026, Training Accuracy: 0.9284\n",
      "            Test Loss: 1.0265, Test Accuracy: 0.8538\n",
      "            \n",
      "Precision: [0.79726651 0.859689  ], Recall: [0.37373198 0.97583164], F1 Score: [0.50890585 0.91408585] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2380, \n",
      "                nodes classified as licit: 34946\n",
      "Epoch: 10, \n",
      "            Training Loss: 0.2496, Training Accuracy: 0.9550\n",
      "            Test Loss: 0.7618, Test Accuracy: 0.9132\n",
      "            \n",
      "Precision: [0.85440106 0.92467969], Recall: [0.68926855 0.97012899], F1 Score: [0.76300236 0.94685926] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2491, \n",
      "                nodes classified as licit: 34835\n",
      "Epoch: 15, \n",
      "            Training Loss: 0.1601, Training Accuracy: 0.9646\n",
      "            Test Loss: 0.5524, Test Accuracy: 0.9267\n",
      "            \n",
      "Precision: [0.8405467  0.94693932], Recall: [0.78804058 0.96198235], F1 Score: [0.81344723 0.95440156] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2396, \n",
      "                nodes classified as licit: 34930\n",
      "Epoch: 20, \n",
      "            Training Loss: 0.1110, Training Accuracy: 0.9713\n",
      "            Test Loss: 0.4740, Test Accuracy: 0.9302\n",
      "            \n",
      "Precision: [0.86417556 0.94491525], Recall: [0.77789642 0.96890699], F1 Score: [0.81876932 0.95676074] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2352, \n",
      "                nodes classified as licit: 34974\n",
      "Epoch: 25, \n",
      "            Training Loss: 0.0850, Training Accuracy: 0.9755\n",
      "            Test Loss: 0.3992, Test Accuracy: 0.9363\n",
      "            \n",
      "Precision: [0.90845518 0.94207436], Recall: [0.76294714 0.98044807], F1 Score: [0.82936738 0.96087824] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2410, \n",
      "                nodes classified as licit: 34916\n",
      "Epoch: 30, \n",
      "            Training Loss: 0.0652, Training Accuracy: 0.9784\n",
      "            Test Loss: 0.3237, Test Accuracy: 0.9444\n",
      "            \n",
      "Precision: [0.90567164 0.95292873], Recall: [0.80993059 0.97854718], F1 Score: [0.85512965 0.96556806] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2426, \n",
      "                nodes classified as licit: 34900\n",
      "Epoch: 35, \n",
      "            Training Loss: 0.0549, Training Accuracy: 0.9815\n",
      "            Test Loss: 0.3064, Test Accuracy: 0.9450\n",
      "            \n",
      "Precision: [0.90746269 0.9533254 ], Recall: [0.8115323  0.97895451], F1 Score: [0.85682074 0.96596999] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2450, \n",
      "                nodes classified as licit: 34876\n",
      "Epoch: 40, \n",
      "            Training Loss: 0.0472, Training Accuracy: 0.9841\n",
      "            Test Loss: 0.2730, Test Accuracy: 0.9510\n",
      "            \n",
      "Precision: [0.90993072 0.96043165], Recall: [0.84143086 0.97881874], F1 Score: [0.87434119 0.96953803] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2461, \n",
      "                nodes classified as licit: 34865\n",
      "Epoch: 45, \n",
      "            Training Loss: 0.0415, Training Accuracy: 0.9860\n",
      "            Test Loss: 0.2615, Test Accuracy: 0.9522\n",
      "            \n",
      "Precision: [0.9119171 0.9614718], Recall: [0.84570208 0.97922607], F1 Score: [0.87756233 0.97026773] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2510, \n",
      "                nodes classified as licit: 34816\n",
      "Epoch: 50, \n",
      "            Training Loss: 0.0367, Training Accuracy: 0.9878\n",
      "            Test Loss: 0.2560, Test Accuracy: 0.9544\n",
      "            \n",
      "Precision: [0.91676234 0.96318036], Recall: [0.85264282 0.98031229], F1 Score: [0.8835408  0.97167082] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2522, \n",
      "                nodes classified as licit: 34804\n",
      "Epoch: 55, \n",
      "            Training Loss: 0.0327, Training Accuracy: 0.9893\n",
      "            Test Loss: 0.2499, Test Accuracy: 0.9556\n",
      "            \n",
      "Precision: [0.92064405 0.9637285 ], Recall: [0.85477843 0.98126273], F1 Score: [0.88648948 0.97241658] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2561, \n",
      "                nodes classified as licit: 34765\n",
      "Epoch: 60, \n",
      "            Training Loss: 0.0293, Training Accuracy: 0.9904\n",
      "            Test Loss: 0.2525, Test Accuracy: 0.9562\n",
      "            \n",
      "Precision: [0.92038946 0.96449546], Recall: [0.85798185 0.98112695], F1 Score: [0.88809063 0.97274012] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2582, \n",
      "                nodes classified as licit: 34744\n",
      "Epoch: 65, \n",
      "            Training Loss: 0.0264, Training Accuracy: 0.9920\n",
      "            Test Loss: 0.2568, Test Accuracy: 0.9557\n",
      "            \n",
      "Precision: [0.92117376 0.96373333], Recall: [0.85477843 0.98139851], F1 Score: [0.88673498 0.9724857 ] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2595, \n",
      "                nodes classified as licit: 34731\n",
      "Epoch: 70, \n",
      "            Training Loss: 0.0240, Training Accuracy: 0.9927\n",
      "            Test Loss: 0.2608, Test Accuracy: 0.9564\n",
      "            \n",
      "Precision: [0.9233871 0.9640096], Recall: [0.85584624 0.98194162], F1 Score: [0.88833472 0.97289298] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2603, \n",
      "                nodes classified as licit: 34723\n",
      "Epoch: 75, \n",
      "            Training Loss: 0.0218, Training Accuracy: 0.9934\n",
      "            Test Loss: 0.2648, Test Accuracy: 0.9569\n",
      "            \n",
      "Precision: [0.92409431 0.9645286 ], Recall: [0.85798185 0.98207739], F1 Score: [0.88981174 0.9732239 ] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2621, \n",
      "                nodes classified as licit: 34705\n",
      "Epoch: 80, \n",
      "            Training Loss: 0.0199, Training Accuracy: 0.9941\n",
      "            Test Loss: 0.2691, Test Accuracy: 0.9567\n",
      "            \n",
      "Precision: [0.92303274 0.96451914], Recall: [0.85798185 0.98180584], F1 Score: [0.88931931 0.97308572] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2628, \n",
      "                nodes classified as licit: 34698\n",
      "Epoch: 85, \n",
      "            Training Loss: 0.0182, Training Accuracy: 0.9947\n",
      "            Test Loss: 0.2748, Test Accuracy: 0.9575\n",
      "            \n",
      "Precision: [0.92725173 0.96442846], Recall: [0.85744794 0.98289206], F1 Score: [0.89098474 0.97357273] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2644, \n",
      "                nodes classified as licit: 34682\n",
      "Epoch: 90, \n",
      "            Training Loss: 0.0167, Training Accuracy: 0.9953\n",
      "            Test Loss: 0.2800, Test Accuracy: 0.9569\n",
      "            \n",
      "Precision: [0.92556261 0.96415723], Recall: [0.85638014 0.98248473], F1 Score: [0.8896284 0.9732347] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2654, \n",
      "                nodes classified as licit: 34672\n",
      "Epoch: 95, \n",
      "            Training Loss: 0.0154, Training Accuracy: 0.9959\n",
      "            Test Loss: 0.2854, Test Accuracy: 0.9570\n",
      "            \n",
      "Precision: [0.92413793 0.96465724], Recall: [0.85851575 0.98207739], F1 Score: [0.89011901 0.97328938] \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Nodes classified as fraud: 2656, \n",
      "                nodes classified as licit: 34670\n",
      "Epoch: 100, \n",
      "            Training Loss: 0.0142, Training Accuracy: 0.9964\n",
      "            Test Loss: 0.2910, Test Accuracy: 0.9568\n",
      "            \n",
      "Precision: [0.92307692 0.96464781], Recall: [0.85851575 0.98180584], F1 Score: [0.88962656 0.9731512 ] \n",
      "\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory GNN_models/trained_models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 111\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------------------------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 111\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGNN_models/trained_models/final_model_2_layers.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gsamp\\anaconda3\\envs\\visionEnv\\Lib\\site-packages\\torch\\serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    625\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    629\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gsamp\\anaconda3\\envs\\visionEnv\\Lib\\site-packages\\torch\\serialization.py:502\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    501\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[1;32mc:\\Users\\gsamp\\anaconda3\\envs\\visionEnv\\Lib\\site-packages\\torch\\serialization.py:473\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 473\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory GNN_models/trained_models does not exist."
     ]
    }
   ],
   "source": [
    "EPOCHS = 101\n",
    "layers_list = [2]\n",
    "# layers_list = [4]\n",
    "wb = True\n",
    "for LAYERS in layers_list:\n",
    "    model = SAGE(train_data.x.shape[1], 256, torch.unique(train_data.y).size(0), n_layers=LAYERS)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.03)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'max', patience=7)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if wb:\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"GraphAnomaly\",\n",
    "            name = f\"2_SAGE - n_layers: {LAYERS}\",\n",
    "            # track hyperparameters and run metadata\n",
    "            config={\n",
    "            \"architecture\": \"SAGE_3\",\n",
    "            \"dataset\": \"Time Steps elliptic\",\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"layers\": LAYERS\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def train(data, epoch):\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        out, h = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "        pred = out.argmax(dim=1)\n",
    "        loss = criterion(out, data.y)  # Compute the loss solely based on the training nodes.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct = pred.eq(data.y).sum().item()\n",
    "        total = len(data.y)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        if wb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": loss,\n",
    "                \"train_accuracy\": accuracy,\n",
    "            })\n",
    "        if epoch%5 == 0:\n",
    "            max_value = torch.max(pred)\n",
    "            counts = torch.bincount(pred, minlength=max_value.item() + 1)\n",
    "            print(f\"\"\"Nodes classified as fraud: {counts[0]}, \n",
    "                nodes classified as licit: {counts[1]}\"\"\")\n",
    "            \n",
    "            \n",
    "        return loss, accuracy\n",
    "\n",
    "    def test(data, epoch):\n",
    "        model.eval()  # Set the model to evaluation mode.\n",
    "        with torch.no_grad():\n",
    "            out, _ = model(data.x, data.edge_index)  # Perform a forward pass.\n",
    "            loss = criterion(out, data.y)\n",
    "            \n",
    "            pred = out.argmax(dim=1)  # Get the predicted labels by selecting the class with the highest probability.\n",
    "            # Calculate accuracy\n",
    "            correct = pred.eq(data.y).sum().item()\n",
    "            total = len(data.y)\n",
    "            accuracy = correct / total\n",
    "            \n",
    "            #precision, recall, f1 score\n",
    "            precision = precision_score(data.y.cpu().numpy(), pred.cpu().numpy(), average=None)\n",
    "            recall = recall_score(data.y.cpu().numpy(), pred.cpu().numpy(), average=None)\n",
    "            f1 = f1_score(data.y.cpu().numpy(), pred.cpu().numpy(), average=None)\n",
    "            \n",
    "            other_metrics = [precision, recall, f1]\n",
    "            if wb:\n",
    "                wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"test_loss\": loss,\n",
    "                \"test_accuracy\": accuracy,\n",
    "                \"test_precision_class_0\": precision[0],\n",
    "                \"test_precision_class_1\": precision[1],\n",
    "                \"test_recall_class_0\": recall[0],\n",
    "                \"test_recall_class_1\": recall[1],\n",
    "                \"test_f1_score_class_0\": f1[0],\n",
    "                \"test_f1_score_class_1\": f1[1]\n",
    "            })\n",
    "            \n",
    "        return loss, accuracy, other_metrics\n",
    "            \n",
    "        \n",
    "        \n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    train_acc_hist = []\n",
    "    test_acc_hist = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = train(train_data, epoch)\n",
    "        train_loss_history.append(train_loss.detach().numpy())\n",
    "        train_acc_hist.append(train_acc)\n",
    "        \n",
    "        test_loss, test_acc, metrics = test(test_data, epoch)\n",
    "        test_loss_history.append(test_loss.detach().numpy())\n",
    "        test_acc_hist.append(test_acc)\n",
    "        \n",
    "        if epoch%5 == 0:\n",
    "            print(f\"\"\"Epoch: {epoch}, \n",
    "            Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f}\n",
    "            Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\n",
    "            \"\"\")\n",
    "            \n",
    "            print(f\"Precision: {metrics[0]}, Recall: {metrics[1]}, F1 Score: {metrics[2]} \\n\")\n",
    "            print(\"---------------------------------------------------------------------------------\")\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "if save_model:\n",
    "    torch.save(model.state_dict(), 'final_model_2_layers.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_data.x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value 0: 7341 occurrences\n",
      "Value 1: 115940 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = SAGE(unk_data.x.shape[1], 256, 2, n_layers=2)\n",
    "\n",
    "# Load the model's state dict\n",
    "model.load_state_dict(torch.load('C:/Users/gsamp/OneDrive/Documents/AI-3/2n Semestre/Projecte de Síntesi 2/GraphAnomaly/GNN_models/trained_models/final_model_2_layers.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Forward pass through the model\n",
    "    output = model(unk_data.x, unk_data.edge_index)\n",
    "\n",
    "# 'output' now contains the model's predictions for the unseen data\n",
    "output[0]\n",
    "predicted_classes = torch.argmax(output[0], dim=1)\n",
    "\n",
    "# Count the occurrences of each value\n",
    "value_counts = torch.bincount(predicted_classes)\n",
    "\n",
    "# Print the distribution\n",
    "for value, count in enumerate(value_counts):\n",
    "    print(f\"Value {value}: {count.item()} occurrences\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visionEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
