{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0195ef-52db-415d-8c27-1d73052baa49",
   "metadata": {},
   "source": [
    "# Graph Anomaly Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d260f-cfe2-4bff-8371-f04ece167377",
   "metadata": {},
   "source": [
    "### Processing and analyzing training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6611c5-9d2b-4804-be59-390630832b45",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62935a85-ffc1-477d-9997-c6d4c0e34c90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f0c835-44bb-4cde-8ad1-a8c0eec58f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read files\n",
    "path = \"C:/Users/marti/Desktop/WAP/6e_semestre/SPII/GraphAnomaly/dades_marti/\"\n",
    "df_classes = pd.read_csv(path + \"elliptic_txs_classes.csv\") # Nodes' labels\n",
    "df_edges = pd.read_csv(path + \"elliptic_txs_edgelist.csv\") # Edges\n",
    "df_features = pd.read_csv(path + \"elliptic_txs_features.csv\", header=None) # Nodes' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce855918-d964-42f4-b9f7-c32d9189f415",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change column names of df_features\n",
    "colNames1 = {'0': 'txId', 1: \"Time step\"}\n",
    "colNames2 = {str(ii+2): \"Local_feature_\" + str(ii+1) for ii in range(93)}\n",
    "colNames3 = {str(ii+95): \"Aggregate_feature_\" + str(ii+1) for ii in range(72)}\n",
    "\n",
    "colNames = dict(colNames1, **colNames2, **colNames3 )\n",
    "colNames = {int(jj): item_kk for jj,item_kk in colNames.items()}\n",
    "\n",
    "df_features = df_features.rename(columns=colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "664158be-741a-4a8f-baf1-1b7e9dd5dcf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 1 belongs to illicit transactions, label 2 to licit transactions and label 3 to unknown transactions.\n",
      "\n",
      "Shape of classes (203769, 2)\n",
      "Shape of edges (234355, 2)\n",
      "Shape of features (203769, 167)\n"
     ]
    }
   ],
   "source": [
    "# Pass unknown to number 3\n",
    "df_classes.loc[df_classes['class'] == 'unknown', 'class'] = 3\n",
    "print('Label 1 belongs to illicit transactions, label 2 to licit transactions and label 3 to unknown transactions.\\n')\n",
    "print('Shape of classes', df_classes.shape)\n",
    "print('Shape of edges', df_edges.shape)\n",
    "print('Shape of features', df_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f12c7-602b-4f8c-b02c-5440f50ad329",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89baa3ce-89ae-48c5-8180-0837addd98be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         txId\n",
       "class        \n",
       "3      157205\n",
       "1        4545\n",
       "2       42019"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classes.groupby(['class']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e9c6181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((203769, 167), (203769, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.shape,df_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f378e045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>Time step</th>\n",
       "      <th>Local_feature_1</th>\n",
       "      <th>Local_feature_2</th>\n",
       "      <th>Local_feature_3</th>\n",
       "      <th>Local_feature_4</th>\n",
       "      <th>Local_feature_5</th>\n",
       "      <th>Local_feature_6</th>\n",
       "      <th>Local_feature_7</th>\n",
       "      <th>Local_feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>Aggregate_feature_64</th>\n",
       "      <th>Aggregate_feature_65</th>\n",
       "      <th>Aggregate_feature_66</th>\n",
       "      <th>Aggregate_feature_67</th>\n",
       "      <th>Aggregate_feature_68</th>\n",
       "      <th>Aggregate_feature_69</th>\n",
       "      <th>Aggregate_feature_70</th>\n",
       "      <th>Aggregate_feature_71</th>\n",
       "      <th>Aggregate_feature_72</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230425980</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171469</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600999</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5530458</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171484</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673103</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232022460</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172107</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439728</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.106715</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.183671</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>232438397</td>\n",
       "      <td>1</td>\n",
       "      <td>0.163054</td>\n",
       "      <td>1.963790</td>\n",
       "      <td>-0.646376</td>\n",
       "      <td>12.409294</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>9.782742</td>\n",
       "      <td>12.414558</td>\n",
       "      <td>-0.163645</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>1.072793</td>\n",
       "      <td>0.085530</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>0.677799</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230460314</td>\n",
       "      <td>1</td>\n",
       "      <td>1.011523</td>\n",
       "      <td>-0.081127</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>1.153668</td>\n",
       "      <td>0.333276</td>\n",
       "      <td>1.312656</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.400422</td>\n",
       "      <td>0.517257</td>\n",
       "      <td>0.579382</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.277775</td>\n",
       "      <td>0.326394</td>\n",
       "      <td>1.293750</td>\n",
       "      <td>0.178136</td>\n",
       "      <td>0.179117</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        txId  Time step  Local_feature_1  Local_feature_2  Local_feature_3  \\\n",
       "0  230425980          1        -0.171469        -0.184668        -1.201369   \n",
       "1    5530458          1        -0.171484        -0.184668        -1.201369   \n",
       "2  232022460          1        -0.172107        -0.184668        -1.201369   \n",
       "3  232438397          1         0.163054         1.963790        -0.646376   \n",
       "4  230460314          1         1.011523        -0.081127        -1.201369   \n",
       "\n",
       "   Local_feature_4  Local_feature_5  Local_feature_6  Local_feature_7  \\\n",
       "0        -0.121970        -0.043875        -0.113002        -0.061584   \n",
       "1        -0.121970        -0.043875        -0.113002        -0.061584   \n",
       "2        -0.121970        -0.043875        -0.113002        -0.061584   \n",
       "3        12.409294        -0.063725         9.782742        12.414558   \n",
       "4         1.153668         0.333276         1.312656        -0.061584   \n",
       "\n",
       "   Local_feature_8  ...  Aggregate_feature_64  Aggregate_feature_65  \\\n",
       "0        -0.162097  ...             -0.600999              1.461330   \n",
       "1        -0.162112  ...              0.673103             -0.979074   \n",
       "2        -0.162749  ...              0.439728             -0.979074   \n",
       "3        -0.163645  ...             -0.613614              0.241128   \n",
       "4        -0.163523  ...             -0.400422              0.517257   \n",
       "\n",
       "   Aggregate_feature_66  Aggregate_feature_67  Aggregate_feature_68  \\\n",
       "0              1.461369              0.018279             -0.087490   \n",
       "1             -0.978556              0.018279             -0.087490   \n",
       "2             -0.978556             -0.098889             -0.106715   \n",
       "3              0.241406              1.072793              0.085530   \n",
       "4              0.579382              0.018279              0.277775   \n",
       "\n",
       "   Aggregate_feature_69  Aggregate_feature_70  Aggregate_feature_71  \\\n",
       "0             -0.131155             -0.097524             -0.120613   \n",
       "1             -0.131155             -0.097524             -0.120613   \n",
       "2             -0.131155             -0.183671             -0.120613   \n",
       "3             -0.131155              0.677799             -0.120613   \n",
       "4              0.326394              1.293750              0.178136   \n",
       "\n",
       "   Aggregate_feature_72  class  \n",
       "0             -0.119792      3  \n",
       "1             -0.119792      3  \n",
       "2             -0.119792      3  \n",
       "3             -0.119792      2  \n",
       "4              0.179117      3  \n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the DataFrames on the column 'source', assuming it's the same name in both DataFrames\n",
    "df_merged = pd.merge(df_features, df_classes, on='txId', how='left')\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "716beefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46564"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged['class'] = df_merged['class'].astype(int)\n",
    "df_labeled = df_merged.copy()\n",
    "df_labeled = df_labeled[df_labeled['class'] < 3]\n",
    "df_edges_labeled = df_edges[df_edges['txId1'].isin(df_labeled['txId'])]\n",
    "df_edges_labeled = df_edges_labeled[df_edges_labeled['txId2'].isin(df_labeled['txId'])]\n",
    "len(df_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e6a6b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph = False\n",
    "if generate_graph:\n",
    "    # Create an empty graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for _, row in df_features.iterrows():\n",
    "        # Extract node ID and attributes\n",
    "        node_id = row['txId']\n",
    "        node_attributes = row.drop('txId').to_dict()\n",
    "        # Add node to the graph with its attributes\n",
    "        G.add_node(node_id, **node_attributes)\n",
    "\n",
    "    # Add edges to the graph\n",
    "    for _, row in df_edges.iterrows():\n",
    "        G.add_edge(row['txId1'], row['txId2'])\n",
    "\n",
    "    # Save the graph as a pickle file\n",
    "    with open(\"./dades_marti/elipticData_graph.pkl\", \"wb\") as f:\n",
    "        pkl.dump(G, f)\n",
    "\n",
    "generate_labeled_graph = False\n",
    "if generate_labeled_graph:\n",
    "    # Create an empty graph\n",
    "    Glab = nx.Graph()\n",
    "\n",
    "    for _, row in df_labeled.iterrows():\n",
    "        # Extract node ID and attributes\n",
    "        node_id = row['txId']\n",
    "        node_attributes = row.drop('txId').to_dict()\n",
    "        # Add node to the graph with its attributes\n",
    "        Glab.add_node(node_id, **node_attributes)\n",
    "\n",
    "    # Add edges to the graph\n",
    "    for _, row in df_edges_labeled.iterrows():\n",
    "        Glab.add_edge(row['txId1'], row['txId2'])\n",
    "    \n",
    "    # Save the labeled graph as a pickle file\n",
    "    with open(\"./dades_marti/elipticData_graph_lab.pkl\", \"wb\") as f:\n",
    "        pkl.dump(Glab, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d087d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your pickle file\n",
    "pickle_file_path = path + 'elipticData_graph.pkl'\n",
    "\n",
    "# Open the pickle file in binary mode\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    # Load the data from the pickle file\n",
    "    G = pkl.load(f)\n",
    "\n",
    "\n",
    "# Specify the path to your pickle file\n",
    "pickle_file_path = path + 'elipticData_graph_lab.pkl'\n",
    "\n",
    "# Open the pickle file in binary mode\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    # Load the data from the pickle file\n",
    "    Glab = pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cb18ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 46564\n",
      "Number of edges: 36624\n"
     ]
    }
   ],
   "source": [
    "# Get the number of nodes\n",
    "num_nodes = nx.number_of_nodes(Glab)\n",
    "\n",
    "# Get the number of edges\n",
    "num_edges = nx.number_of_edges(Glab)\n",
    "\n",
    "print(\"Number of nodes:\", num_nodes)\n",
    "print(\"Number of edges:\", num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d41d04",
   "metadata": {},
   "source": [
    "### Adding common metrics as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70aa6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of the betweenness centrality by NetworkX\n",
    "\n",
    "\"\"\"\n",
    "====================\n",
    "Parallel Betweenness\n",
    "====================\n",
    "\n",
    "Example of parallel implementation of betweenness centrality using the\n",
    "multiprocessing module from Python Standard Library.\n",
    "\n",
    "The function betweenness centrality accepts a bunch of nodes and computes\n",
    "the contribution of those nodes to the betweenness centrality of the whole\n",
    "network. Here we divide the network in chunks of nodes and we compute their\n",
    "contribution to the betweenness centrality of the whole network.\n",
    "\n",
    "Note: The example output below shows that the non-parallel implementation is\n",
    "faster. This is a limitation of our CI/CD pipeline running on a single core.\n",
    "\n",
    "Depending on your setup, you will likely observe a speedup.\n",
    "\"\"\"\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Divide a list of nodes `l` in `n` chunks\"\"\"\n",
    "    l_c = iter(l)\n",
    "    while 1:\n",
    "        x = tuple(itertools.islice(l_c, n))\n",
    "        if not x:\n",
    "            return\n",
    "        yield x\n",
    "\n",
    "\n",
    "def betweenness_centrality_parallel(G, processes=None):\n",
    "    \"\"\"Parallel betweenness centrality  function\"\"\"\n",
    "    p = Pool(processes=processes)\n",
    "    node_divisor = len(p._pool) * 4\n",
    "    node_chunks = list(chunks(G.nodes(), G.order() // node_divisor))\n",
    "    num_chunks = len(node_chunks)\n",
    "    bt_sc = p.starmap(\n",
    "        nx.betweenness_centrality_subset,\n",
    "        zip(\n",
    "            [G] * num_chunks,\n",
    "            node_chunks,\n",
    "            [list(G)] * num_chunks,\n",
    "            [True] * num_chunks,\n",
    "            [None] * num_chunks,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Reduce the partial solutions\n",
    "    bt_c = bt_sc[0]\n",
    "    for bt in bt_sc[1:]:\n",
    "        for n in bt:\n",
    "            bt_c[n] += bt[n]\n",
    "    return bt_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20314446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Això triga com 1000 minuts a correr!!!\n",
    "computar = False\n",
    "if computar:\n",
    "    d_grau = dict(Glab.degree())\n",
    "    graus = [grau for grau in d_grau.keys()]\n",
    "    print(\"Getting degrees - done!\")\n",
    "    degree_centralities = [dc for dc in nx.degree_centrality(Glab).keys()]\n",
    "    print(\"Getting degree centrality - done!\")\n",
    "    betweenness_centralities = [bc for bc in betweenness_centrality_parallel(Glab).keys()]\n",
    "    print(\"Getting betweenness centrality - done!\")\n",
    "    eigenvector_centralities = [ec for ec in nx.eigenvector_centrality(Glab).keys()]\n",
    "    print(\"Getting eigenvector centrality - done!\")\n",
    "    closeness_centralities = [cc for cc in nx.closeness_centrality(Glab).keys()]\n",
    "    print(\"Getting closeness centrality - done!\")\n",
    "    clustering_coefficients = [cc for cc in nx.clustering(Glab).keys()]\n",
    "    print(\"Getting the clustering coefficient - done!\")\n",
    "\n",
    "    df_extended = df_labeled.copy()\n",
    "    df_extended['degree'] = graus\n",
    "    df_extended['degree_centrality'] = degree_centralities\n",
    "    df_extended['betweenness_centrality'] = betweenness_centralities\n",
    "    df_extended['eigenvector_centrality'] = eigenvector_centralities\n",
    "    df_extended['closeness_centrality'] = closeness_centralities\n",
    "    df_extended['clustering_coefficient'] = clustering_coefficients\n",
    "    print(\"All done!\")\n",
    "\n",
    "    # Modify this to your \n",
    "    df_extended.to_csv('./dades_marti/labeled_extended.csv')\n",
    "\n",
    "else:\n",
    "    df_extended = pd.read_csv('./dades_marti/labeled_extended.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0b353",
   "metadata": {},
   "source": [
    "### Logistic regression using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e8343f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88f91241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46564"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79108cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: 0.9806481763024362\n",
      "Data reduction, from shape (46564, 173) to (46564, 69)\n"
     ]
    }
   ],
   "source": [
    "# Standardize the features (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# df_pca = df_extended.drop(columns=['txId', 'Time step', 'class'])\n",
    "df_pca = df_extended.drop(columns=['txId',  'class'])\n",
    "scaled_data = scaler.fit_transform(df_pca)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=69)  # You can choose the number of components you want to keep\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a DataFrame for the principal components\n",
    "columns = [f\"PC{i+1}\" for i in range(principal_components.shape[1])]\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=columns)\n",
    "\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance_ratio = explained_variance_ratio.sum()\n",
    "\n",
    "print(f\"Explained variance ratio: {cumulative_variance_ratio}\")\n",
    "print(f\"Data reduction, from shape {df_pca.shape} to {principal_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509fd44e",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "939fd76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.33%\n",
      "Precision: 81.44%\n",
      "Recall: 66.85%\n",
      "F1 Score: 73.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marti\\anaconda3\\envs\\graphanomaly310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#afegir dues columnes per poder aplicar logistic regression\n",
    "principal_df['txId'] = df_pca.index\n",
    "principal_df['class'] = [1 if classe == 1 else 0 for classe in df_extended['class']]\n",
    "\n",
    "X = principal_df.drop(columns=['txId', 'class'])\n",
    "y = principal_df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision: {:.2f}%\".format(precision * 100))\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall: {:.2f}%\".format(recall * 100))\n",
    "f1score = f1_score(y_test, y_pred)\n",
    "print(\"F1 Score: {:.2f}%\".format(f1score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75251e96",
   "metadata": {},
   "source": [
    "### Trying node embeddings with node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a671595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from node2vec import Node2Vec\n",
    "# import tqdm as notebook_tqdm\n",
    "\n",
    "# node2vec = Node2Vec(Glab, dimensions=64, walk_length=20, num_walks=200, p=2, q=1, workers=1)\n",
    "# model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "# model.save('dades_marti/node2vec_labelled.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ad7894",
   "metadata": {},
   "source": [
    "Node Embeddings with DGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2e4b00d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new() received an invalid combination of arguments - got (int, dtype=torch.dtype), but expected one of:\n * (*, torch.device device)\n      didn't match because some of the keywords were incorrect: dtype\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n * (object data, *, torch.device device)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m GlabDGL \u001b[38;5;241m=\u001b[39m dgl\u001b[38;5;241m.\u001b[39mfrom_networkx(Glab)\n\u001b[1;32m----> 6\u001b[0m nodeId \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      8\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (int, dtype=torch.dtype), but expected one of:\n * (*, torch.device device)\n      didn't match because some of the keywords were incorrect: dtype\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n * (object data, *, torch.device device)\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "GlabDGL = dgl.from_networkx(Glab)\n",
    "\n",
    "nodeId = torch.Tensor(0, dtype=torch.int64)\n",
    "p = 1\n",
    "q = 1\n",
    "walk_length = 20\n",
    "traces = dgl.sampling.node2vec_random_walk(GlabDGL, nodeId, p, q, walk_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b8b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2487a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b20e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
