{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0195ef-52db-415d-8c27-1d73052baa49",
   "metadata": {},
   "source": [
    "# Graph Anomaly Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d260f-cfe2-4bff-8371-f04ece167377",
   "metadata": {},
   "source": [
    "### Processing and analyzing training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6611c5-9d2b-4804-be59-390630832b45",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62935a85-ffc1-477d-9997-c6d4c0e34c90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f0c835-44bb-4cde-8ad1-a8c0eec58f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read files\n",
    "path = \"C:/Users/marti/Desktop/WAP/6e_semestre/SPII/GraphAnomaly/dades_marti/\"\n",
    "df_classes = pd.read_csv(path + \"elliptic_txs_classes.csv\") # Nodes' labels\n",
    "df_edges = pd.read_csv(path + \"elliptic_txs_edgelist.csv\") # Edges\n",
    "df_features = pd.read_csv(path + \"elliptic_txs_features.csv\", header=None) # Nodes' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce855918-d964-42f4-b9f7-c32d9189f415",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change column names of df_features\n",
    "colNames1 = {'0': 'txId', 1: \"Time step\"}\n",
    "colNames2 = {str(ii+2): \"Local_feature_\" + str(ii+1) for ii in range(93)}\n",
    "colNames3 = {str(ii+95): \"Aggregate_feature_\" + str(ii+1) for ii in range(72)}\n",
    "\n",
    "colNames = dict(colNames1, **colNames2, **colNames3 )\n",
    "colNames = {int(jj): item_kk for jj,item_kk in colNames.items()}\n",
    "\n",
    "df_features = df_features.rename(columns=colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664158be-741a-4a8f-baf1-1b7e9dd5dcf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 1 belongs to illicit transactions, label 2 to licit transactions and label 3 to unknown transactions.\n",
      "\n",
      "Shape of classes (203769, 2)\n",
      "Shape of edges (234355, 2)\n",
      "Shape of features (203769, 167)\n"
     ]
    }
   ],
   "source": [
    "# Pass unknown to number 3\n",
    "df_classes.loc[df_classes['class'] == 'unknown', 'class'] = 3\n",
    "print('Label 1 belongs to illicit transactions, label 2 to licit transactions and label 3 to unknown transactions.\\n')\n",
    "print('Shape of classes', df_classes.shape)\n",
    "print('Shape of edges', df_edges.shape)\n",
    "print('Shape of features', df_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f12c7-602b-4f8c-b02c-5440f50ad329",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89baa3ce-89ae-48c5-8180-0837addd98be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         txId\n",
       "class        \n",
       "3      157205\n",
       "1        4545\n",
       "2       42019"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classes.groupby(['class']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9c6181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((203769, 167), (203769, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.shape,df_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f378e045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>Time step</th>\n",
       "      <th>Local_feature_1</th>\n",
       "      <th>Local_feature_2</th>\n",
       "      <th>Local_feature_3</th>\n",
       "      <th>Local_feature_4</th>\n",
       "      <th>Local_feature_5</th>\n",
       "      <th>Local_feature_6</th>\n",
       "      <th>Local_feature_7</th>\n",
       "      <th>Local_feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>Aggregate_feature_64</th>\n",
       "      <th>Aggregate_feature_65</th>\n",
       "      <th>Aggregate_feature_66</th>\n",
       "      <th>Aggregate_feature_67</th>\n",
       "      <th>Aggregate_feature_68</th>\n",
       "      <th>Aggregate_feature_69</th>\n",
       "      <th>Aggregate_feature_70</th>\n",
       "      <th>Aggregate_feature_71</th>\n",
       "      <th>Aggregate_feature_72</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230425980</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171469</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600999</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5530458</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171484</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673103</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232022460</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172107</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439728</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.106715</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.183671</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>232438397</td>\n",
       "      <td>1</td>\n",
       "      <td>0.163054</td>\n",
       "      <td>1.963790</td>\n",
       "      <td>-0.646376</td>\n",
       "      <td>12.409294</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>9.782742</td>\n",
       "      <td>12.414558</td>\n",
       "      <td>-0.163645</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>1.072793</td>\n",
       "      <td>0.085530</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>0.677799</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230460314</td>\n",
       "      <td>1</td>\n",
       "      <td>1.011523</td>\n",
       "      <td>-0.081127</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>1.153668</td>\n",
       "      <td>0.333276</td>\n",
       "      <td>1.312656</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.400422</td>\n",
       "      <td>0.517257</td>\n",
       "      <td>0.579382</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.277775</td>\n",
       "      <td>0.326394</td>\n",
       "      <td>1.293750</td>\n",
       "      <td>0.178136</td>\n",
       "      <td>0.179117</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        txId  Time step  Local_feature_1  Local_feature_2  Local_feature_3  \\\n",
       "0  230425980          1        -0.171469        -0.184668        -1.201369   \n",
       "1    5530458          1        -0.171484        -0.184668        -1.201369   \n",
       "2  232022460          1        -0.172107        -0.184668        -1.201369   \n",
       "3  232438397          1         0.163054         1.963790        -0.646376   \n",
       "4  230460314          1         1.011523        -0.081127        -1.201369   \n",
       "\n",
       "   Local_feature_4  Local_feature_5  Local_feature_6  Local_feature_7  \\\n",
       "0        -0.121970        -0.043875        -0.113002        -0.061584   \n",
       "1        -0.121970        -0.043875        -0.113002        -0.061584   \n",
       "2        -0.121970        -0.043875        -0.113002        -0.061584   \n",
       "3        12.409294        -0.063725         9.782742        12.414558   \n",
       "4         1.153668         0.333276         1.312656        -0.061584   \n",
       "\n",
       "   Local_feature_8  ...  Aggregate_feature_64  Aggregate_feature_65  \\\n",
       "0        -0.162097  ...             -0.600999              1.461330   \n",
       "1        -0.162112  ...              0.673103             -0.979074   \n",
       "2        -0.162749  ...              0.439728             -0.979074   \n",
       "3        -0.163645  ...             -0.613614              0.241128   \n",
       "4        -0.163523  ...             -0.400422              0.517257   \n",
       "\n",
       "   Aggregate_feature_66  Aggregate_feature_67  Aggregate_feature_68  \\\n",
       "0              1.461369              0.018279             -0.087490   \n",
       "1             -0.978556              0.018279             -0.087490   \n",
       "2             -0.978556             -0.098889             -0.106715   \n",
       "3              0.241406              1.072793              0.085530   \n",
       "4              0.579382              0.018279              0.277775   \n",
       "\n",
       "   Aggregate_feature_69  Aggregate_feature_70  Aggregate_feature_71  \\\n",
       "0             -0.131155             -0.097524             -0.120613   \n",
       "1             -0.131155             -0.097524             -0.120613   \n",
       "2             -0.131155             -0.183671             -0.120613   \n",
       "3             -0.131155              0.677799             -0.120613   \n",
       "4              0.326394              1.293750              0.178136   \n",
       "\n",
       "   Aggregate_feature_72  class  \n",
       "0             -0.119792      3  \n",
       "1             -0.119792      3  \n",
       "2             -0.119792      3  \n",
       "3             -0.119792      2  \n",
       "4              0.179117      3  \n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the DataFrames on the column 'source', assuming it's the same name in both DataFrames\n",
    "df_merged = pd.merge(df_features, df_classes, on='txId', how='left')\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716beefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46564"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged['class'] = df_merged['class'].astype(int)\n",
    "df_labeled = df_merged.copy()\n",
    "df_labeled = df_labeled[df_labeled['class'] < 3]\n",
    "df_edges_labeled = df_edges[df_edges['txId1'].isin(df_labeled['txId'])]\n",
    "df_edges_labeled = df_edges_labeled[df_edges_labeled['txId2'].isin(df_labeled['txId'])]\n",
    "len(df_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e6a6b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph = False\n",
    "if generate_graph:\n",
    "    # Create an empty graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for _, row in df_features.iterrows():\n",
    "        # Extract node ID and attributes\n",
    "        node_id = row['txId']\n",
    "        node_attributes = row.drop('txId').to_dict()\n",
    "        # Add node to the graph with its attributes\n",
    "        G.add_node(node_id, **node_attributes)\n",
    "\n",
    "    # Add edges to the graph\n",
    "    for _, row in df_edges.iterrows():\n",
    "        G.add_edge(row['txId1'], row['txId2'])\n",
    "\n",
    "generate_labeled_graph = False\n",
    "if generate_labeled_graph:\n",
    "    # Create an empty graph\n",
    "    Glab = nx.Graph()\n",
    "\n",
    "    for _, row in df_labeled.iterrows():\n",
    "        # Extract node ID and attributes\n",
    "        node_id = row['txId']\n",
    "        node_attributes = row.drop('txId').to_dict()\n",
    "        # Add node to the graph with its attributes\n",
    "        Glab.add_node(node_id, **node_attributes)\n",
    "\n",
    "    # Add edges to the graph\n",
    "    for _, row in df_edges_labeled.iterrows():\n",
    "        Glab.add_edge(row['txId1'], row['txId2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d087d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the graph as a pickle file\n",
    "with open(\"./dades_marti/elipticData_graph.pkl\", \"wb\") as f:\n",
    "    pkl.dump(G, f)\n",
    "\n",
    "\n",
    "# Specify the path to your pickle file\n",
    "pickle_file_path = path + 'elipticData_graph.pkl'\n",
    "\n",
    "# Open the pickle file in binary mode\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    # Load the data from the pickle file\n",
    "    G = pkl.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# Save the labeled graph as a pickle file\n",
    "with open(\"./dades_marti/elipticData_graph_lab.pkl\", \"wb\") as f:\n",
    "    pkl.dump(Glab, f)\n",
    "\n",
    "\n",
    "# Specify the path to your pickle file\n",
    "pickle_file_path = path + 'elipticData_graph_lab.pkl'\n",
    "\n",
    "# Open the pickle file in binary mode\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    # Load the data from the pickle file\n",
    "    Glab = pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cb18ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 46564\n",
      "Number of edges: 36624\n"
     ]
    }
   ],
   "source": [
    "# Get the number of nodes\n",
    "num_nodes = nx.number_of_nodes(Glab)\n",
    "\n",
    "# Get the number of edges\n",
    "num_edges = nx.number_of_edges(Glab)\n",
    "\n",
    "print(\"Number of nodes:\", num_nodes)\n",
    "print(\"Number of edges:\", num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d41d04",
   "metadata": {},
   "source": [
    "### Adding common metrics as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70aa6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of the betweenness centrality by NetworkX\n",
    "\n",
    "\"\"\"\n",
    "====================\n",
    "Parallel Betweenness\n",
    "====================\n",
    "\n",
    "Example of parallel implementation of betweenness centrality using the\n",
    "multiprocessing module from Python Standard Library.\n",
    "\n",
    "The function betweenness centrality accepts a bunch of nodes and computes\n",
    "the contribution of those nodes to the betweenness centrality of the whole\n",
    "network. Here we divide the network in chunks of nodes and we compute their\n",
    "contribution to the betweenness centrality of the whole network.\n",
    "\n",
    "Note: The example output below shows that the non-parallel implementation is\n",
    "faster. This is a limitation of our CI/CD pipeline running on a single core.\n",
    "\n",
    "Depending on your setup, you will likely observe a speedup.\n",
    "\"\"\"\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Divide a list of nodes `l` in `n` chunks\"\"\"\n",
    "    l_c = iter(l)\n",
    "    while 1:\n",
    "        x = tuple(itertools.islice(l_c, n))\n",
    "        if not x:\n",
    "            return\n",
    "        yield x\n",
    "\n",
    "\n",
    "def betweenness_centrality_parallel(G, processes=None):\n",
    "    \"\"\"Parallel betweenness centrality  function\"\"\"\n",
    "    p = Pool(processes=processes)\n",
    "    node_divisor = len(p._pool) * 4\n",
    "    node_chunks = list(chunks(G.nodes(), G.order() // node_divisor))\n",
    "    num_chunks = len(node_chunks)\n",
    "    bt_sc = p.starmap(\n",
    "        nx.betweenness_centrality_subset,\n",
    "        zip(\n",
    "            [G] * num_chunks,\n",
    "            node_chunks,\n",
    "            [list(G)] * num_chunks,\n",
    "            [True] * num_chunks,\n",
    "            [None] * num_chunks,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Reduce the partial solutions\n",
    "    bt_c = bt_sc[0]\n",
    "    for bt in bt_sc[1:]:\n",
    "        for n in bt:\n",
    "            bt_c[n] += bt[n]\n",
    "    return bt_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20314446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting degrees - done!\n",
      "Getting degree centrality - done!\n"
     ]
    }
   ],
   "source": [
    "# Això triga com 1000 minuts a correr!!!\n",
    "computar = True\n",
    "if computar:\n",
    "    d_grau = dict(Glab.degree())\n",
    "    graus = [grau for grau in d_grau.keys()]\n",
    "    print(\"Getting degrees - done!\")\n",
    "    degree_centralities = [dc for dc in nx.degree_centrality(Glab).keys()]\n",
    "    print(\"Getting degree centrality - done!\")\n",
    "    betweenness_centralities = [bc for bc in betweenness_centrality_parallel(Glab).keys()]\n",
    "    print(\"Getting betweenness centrality - done!\")\n",
    "    eigenvector_centralities = [ec for ec in nx.eigenvector_centrality(Glab).keys()]\n",
    "    print(\"Getting eigenvector centrality - done!\")\n",
    "    closeness_centralities = [cc for cc in nx.closeness_centrality(Glab).keys()]\n",
    "    print(\"Getting closeness centrality - done!\")\n",
    "    clustering_coefficients = [cc for cc in nx.clustering(Glab).keys()]\n",
    "    print(\"Getting the clustering coefficient - done!\")\n",
    "\n",
    "    df_extended = df_labeled.copy()\n",
    "    df_extended['degree'] = graus\n",
    "    df_extended['degree_centrality'] = degree_centralities\n",
    "    df_extended['betweenness_centrality'] = betweenness_centralities\n",
    "    df_extended['eigenvector_centrality'] = eigenvector_centralities\n",
    "    df_extended['closeness_centrality'] = closeness_centralities\n",
    "    df_extended['clustering_coefficient'] = clustering_coefficients\n",
    "    print(\"All done!\")\n",
    "\n",
    "    # Modify this to your \n",
    "    df_extended.to_csv('./dades_marti/labelled_extended.csv')\n",
    "\n",
    "else:\n",
    "    df_extended = pd.read_csv('./dades_marti/labelled_sextended.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0b353",
   "metadata": {},
   "source": [
    "### Logistic regression using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e8343f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88f91241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203769"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79108cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: 0.9984828244981349\n",
      "Data reduction, from shape (203769, 171) to (203769, 100)\n"
     ]
    }
   ],
   "source": [
    "# Standardize the features (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_pca = df_extended.drop(columns=['txId', 'Time step', 'class'])\n",
    "scaled_data = scaler.fit_transform(df_pca)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=100)  # You can choose the number of components you want to keep\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a DataFrame for the principal components\n",
    "columns = [f\"PC{i+1}\" for i in range(principal_components.shape[1])]\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=columns)\n",
    "\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance_ratio = explained_variance_ratio.sum()\n",
    "\n",
    "print(f\"Explained variance ratio: {cumulative_variance_ratio}\")\n",
    "print(f\"Data reduction, from shape {df_pca.shape} to {principal_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509fd44e",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1478a29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m principal_df\u001b[38;5;241m.\u001b[39mloc[principal_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m])][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mlist\u001b[39m(principal_df\u001b[38;5;241m.\u001b[39mloc[principal_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m])][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m])))\n\u001b[1;32m----> 8\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m      9\u001b[0m     X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Train the Logistic Regression model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n",
      "File \u001b[1;32mc:\\Users\\marti\\anaconda3\\envs\\graphanomaly\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\marti\\anaconda3\\envs\\graphanomaly\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2614\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2616\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2617\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2619\u001b[0m )\n\u001b[0;32m   2621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\marti\\anaconda3\\envs\\graphanomaly\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2273\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2270\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2274\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2277\u001b[0m     )\n\u001b[0;32m   2279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "#afegir dues columnes per poder aplicar logistic regression\n",
    "principal_df['txId'] = df_pca.index\n",
    "principal_df['class'] = list(df_extended['class'])\n",
    "\n",
    "X = principal_df.loc[principal_df['class'].isin(['1', '2'])].drop(columns=['txId', 'class'])\n",
    "y = principal_df.loc[principal_df['class'].isin(['1', '2'])]['class']\n",
    "print(set(list(principal_df.loc[principal_df['class'].isin(['1', '2'])]['class'])))\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75251e96",
   "metadata": {},
   "source": [
    "### Trying node embeddings with node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a671595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from node2vec import Node2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8382bdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
