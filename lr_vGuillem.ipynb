{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import sys\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/gsamp/OneDrive/Documents/AI-3/2n Semestre/Projecte de Síntesi 2/GraphAnomaly/dades_guillem/\"\n",
    "df_classes = pd.read_csv(path + \"elliptic_txs_classes.csv\") # Nodes' labels\n",
    "df_edges = pd.read_csv(path + \"elliptic_txs_edgelist.csv\") # Edges\n",
    "df_features = pd.read_csv(path + \"elliptic_txs_features.csv\", header=None) # Nodes' features\n",
    "\n",
    "# Change column names of df_features\n",
    "colNames1 = {'0': 'txId', 1: \"Time step\"}\n",
    "colNames2 = {str(ii+2): \"Local_feature_\" + str(ii+1) for ii in range(93)}\n",
    "colNames3 = {str(ii+95): \"Aggregate_feature_\" + str(ii+1) for ii in range(72)}\n",
    "\n",
    "colNames = dict(colNames1, **colNames2, **colNames3 )\n",
    "colNames = {int(jj): item_kk for jj,item_kk in colNames.items()}\n",
    "\n",
    "df_features = df_features.rename(columns=colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your pickle file\n",
    "pickle_file_path = 'C:\\\\Users\\\\gsamp\\\\OneDrive\\\\Documents\\\\AI-3\\\\2n Semestre\\\\Projecte de Síntesi 2\\\\GraphAnomaly\\\\elipticData_graph.pkl'\n",
    "\n",
    "# Open the pickle file in binary mode\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    # Load the data from the pickle file\n",
    "    G = pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(left=df_features, right=df_classes, on=\"txId\", how=\"left\")\n",
    "df_merged = df_merged.set_index('txId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPLY PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: 0.99178286404931\n",
      "Data reduction, from shape (203769, 166) to (203769, 75)\n"
     ]
    }
   ],
   "source": [
    "# Standardize the features (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_pca = df_merged.drop(columns=['class'])\n",
    "scaled_data = scaler.fit_transform(df_pca)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=75)  # You can choose the number of components you want to keep\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a DataFrame for the principal components\n",
    "columns = [f\"PC{i+1}\" for i in range(principal_components.shape[1])]\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=columns)\n",
    "\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance_ratio = explained_variance_ratio.sum()\n",
    "\n",
    "print(f\"Explained variance ratio: {cumulative_variance_ratio}\")\n",
    "print(f\"Data reduction, from shape {df_pca.shape} to {principal_df.shape}\")\n",
    "\n",
    "#afegir dues columnes per poder aplicar logistic regression\n",
    "principal_df['txId'] = df_pca.index\n",
    "principal_df['class'] = list(df_merged['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topological features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = pd.merge(df_features, df_classes, on='txId', how='left')\n",
    "df_anotated = df_complete.loc[df_complete['class'].isin(['1', '2'])] \n",
    "df_edges = df_edges.loc[(df_edges.txId1.isin(df_anotated.txId)) & (df_edges.txId2.isin(df_anotated.txId))]\n",
    "generate_graph = True\n",
    "if generate_graph:\n",
    "    # Create an empty graph\n",
    "    G2 = nx.Graph()\n",
    "\n",
    "    for _, row in df_anotated.iterrows():\n",
    "        # Extract node ID and attributes\n",
    "        node_id = row['txId']\n",
    "        node_attributes = row.drop('txId').to_dict()\n",
    "        \n",
    "        # Add node to the graph with its attributes\n",
    "        G2.add_node(node_id, **node_attributes)\n",
    "\n",
    "    # Add edges to the graph\n",
    "    for _, row in df_edges.iterrows():\n",
    "        G2.add_edge(row['txId1'], row['txId2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Additional topological features\n",
    "degree_centrality = nx.degree_centrality(G2)\n",
    "print(1)\n",
    "betweenness_centrality = nx.betweenness_centrality(G2)\n",
    "print(1)\n",
    "clustering_coefficient = nx.clustering(G2)\n",
    "print(1)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G2)\n",
    "print(1)\n",
    "pagerank_centrality = nx.pagerank(G2)\n",
    "print(1)\n",
    "closeness_centrality = nx.closeness_centrality(G2)\n",
    "print(1)\n",
    "# avg_shortest_path_length = nx.average_shortest_path_length(G2)\n",
    "print(1)\n",
    "assortativity = nx.degree_assortativity_coefficient(G2)\n",
    "print(1)\n",
    "node_degrees = dict(G2.degree())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0013315293258595881, 0.00021842204302680992, 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[degree_centrality[node], betweenness_centrality[node], clustering_coefficient[node]\n",
    "                            #  eigenvector_centrality[node], pagerank_centrality[node], closeness_centrality[node],\n",
    "                            #   assortativity[node]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa\n"
     ]
    }
   ],
   "source": [
    "for n1, n2  in  zip(G2.nodes(), degree_centrality.keys()):\n",
    "    if n1 == n2:\n",
    "        print(\"aaa\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m feature_vectors \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m G2\u001b[38;5;241m.\u001b[39mnodes():\n\u001b[0;32m      3\u001b[0m     feature_vectors[node] \u001b[38;5;241m=\u001b[39m [degree_centrality[node], betweenness_centrality[node], clustering_coefficient[node],\n\u001b[0;32m      4\u001b[0m                              eigenvector_centrality[node], pagerank_centrality[node], closeness_centrality[node],\n\u001b[1;32m----> 5\u001b[0m                               \u001b[43massortativity\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m]\u001b[49m]\u001b[38;5;66;03m#, node_degrees[node]]\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "feature_vectors = {}\n",
    "for node in G2.nodes():\n",
    "    feature_vectors[node] = [degree_centrality[node], betweenness_centrality[node], clustering_coefficient[node],\n",
    "                             eigenvector_centrality[node], pagerank_centrality[node], closeness_centrality[node],\n",
    "                              assortativity[node]]#, node_degrees[node]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"topol_ft_dict.pkl\"\n",
    "with open(file_name, 'wb') as f:\n",
    "    pkl.dump(feature_vectors, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr(X, y, message: str):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the Logistic Regression model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    with open('results.txt', 'a') as f:\n",
    "        sys.stdout = f\n",
    "        print(message)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        print(f\"Precision: {precision * 100}%\")\n",
    "\n",
    "        # precision2 = precision_score(y_test, y_pred, pos_label='1')\n",
    "        # print(\"Precision for illicit nodes: {:.2f}%\".format(precision2 * 100))\n",
    "        \n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        print(\"Recall: {:.2f}%\".format(recall * 100))\n",
    "        \n",
    "        f1score = f1_score(y_test, y_pred)\n",
    "        print(\"F1 Score: {:.2f}%\".format(f1score * 100))\n",
    "        \n",
    "        # Reset stdout back to the console\n",
    "        sys.stdout = sys.__stdout__\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gsamp\\anaconda3\\envs\\visionEnv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X = principal_df.loc[principal_df['class'].isin(['1', '2'])].drop(columns=['txId', 'class'])\n",
    "y = [1 if classe == '1' else 0 for classe in principal_df.loc[principal_df['class'].isin(['1', '2'])]['class']]\n",
    "\n",
    "lr(X, y, message = \"\\nLogistic regression applied with PCA: \\n\")\n",
    "\n",
    "X = df_merged.loc[df_merged['class'].isin(['1', '2'])].drop(columns=[1, 'class'])\n",
    "y = [1 if classe == '1' else 0 for classe in df_merged.loc[df_merged['class'].isin(['1', '2'])]['class']]\n",
    "lr(X, y, message = \"\\nLogistic regression applied to the raw data: \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sintesis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
